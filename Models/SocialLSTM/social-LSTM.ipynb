{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Copy of Copy of lstm-based-Ying-model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8MlMbtspnvX"
      },
      "source": [
        "As Motion Prediction has temporal data, I tried to used LSTM based seq 2 seq generation (Encoder-Decoder Model approach) atleast inspired.\n",
        "Some part of code (validation) is borrowed from @ Marco B notebook . Model itself is quite basic and this can get to a score of 400 on leaderboard.\n",
        "\n",
        "I hope this trigger some more work using LSTM and more sharing to come."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afqbuI0jp4bg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd27a95-e51e-4eca-a329-316ba766016f"
      },
      "source": [
        "!pip install l5kit\n",
        "!pip install -U PyYAML"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: l5kit in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from l5kit) (5.3.1)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.6/dist-packages (from l5kit) (3.12.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from l5kit) (7.5.1)\n",
            "Requirement already satisfied: pymap3d in /usr/local/lib/python3.6/dist-packages (from l5kit) (2.4.3)\n",
            "Requirement already satisfied: opencv-contrib-python-headless in /usr/local/lib/python3.6/dist-packages (from l5kit) (4.4.0.46)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from l5kit) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from l5kit) (1.4.1)\n",
            "Requirement already satisfied: transforms3d in /usr/local/lib/python3.6/dist-packages (from l5kit) (0.3.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from l5kit) (5.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from l5kit) (4.41.1)\n",
            "Requirement already satisfied: torchvision<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from l5kit) (0.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from l5kit) (1.18.5)\n",
            "Requirement already satisfied: torch<2.0.0,>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from l5kit) (1.7.0+cu101)\n",
            "Requirement already satisfied: ptable in /usr/local/lib/python3.6/dist-packages (from l5kit) (0.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from l5kit) (50.3.2)\n",
            "Requirement already satisfied: zarr in /usr/local/lib/python3.6/dist-packages (from l5kit) (2.5.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from l5kit) (2.4.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.2->l5kit) (1.15.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->l5kit) (5.0.8)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->l5kit) (3.5.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->l5kit) (4.10.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->l5kit) (4.3.3)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->l5kit) (5.5.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->l5kit) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->l5kit) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->l5kit) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->l5kit) (2.4.7)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->l5kit) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook->l5kit) (4.6.3)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook->l5kit) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->l5kit) (5.1.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->l5kit) (1.5.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->l5kit) (5.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->l5kit) (2.11.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->l5kit) (0.9.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision<1.0.0,>=0.6.0->l5kit) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.5.0->l5kit) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.5.0->l5kit) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.5.0->l5kit) (0.7)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.6/dist-packages (from zarr->l5kit) (0.15)\n",
            "Requirement already satisfied: numcodecs>=0.6.4 in /usr/local/lib/python3.6/dist-packages (from zarr->l5kit) (0.7.2)\n",
            "Requirement already satisfied: asciitree in /usr/local/lib/python3.6/dist-packages (from zarr->l5kit) (0.3.3)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->l5kit) (2.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.1->ipywidgets->l5kit) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->l5kit) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->l5kit) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->l5kit) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->l5kit) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->l5kit) (4.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.0->notebook->l5kit) (19.0.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->l5kit) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->l5kit) (1.4.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->l5kit) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->l5kit) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->l5kit) (3.2.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->l5kit) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->l5kit) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->l5kit) (0.6.0)\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.6/dist-packages (from fasteners->zarr->l5kit) (1.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->l5kit) (0.2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->l5kit) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->l5kit) (20.4)\n",
            "Requirement already up-to-date: PyYAML in /usr/local/lib/python3.6/dist-packages (5.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGVbpD5ZiDSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4f46e3-4dc6-4eb4-a99f-60ffc7929588"
      },
      "source": [
        "from google.colab import drive\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "op5PSXwKpnva"
      },
      "source": [
        " import numpy as np\n",
        "\n",
        "import os\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import resnet18,resnet50,resnet101\n",
        "from tqdm import tqdm\n",
        "from typing import Dict\n",
        "from torch import functional as F\n",
        "\n",
        "from l5kit.configs import load_config_data\n",
        "from l5kit.data import LocalDataManager, ChunkedDataset\n",
        "from l5kit.dataset import AgentDataset, EgoDataset\n",
        "from l5kit.rasterization import build_rasterizer\n",
        "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
        "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
        "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
        "from l5kit.geometry import transform_points\n",
        "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
        "from prettytable import PrettyTable\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": false,
        "id": "h7uTvVV6pnvj"
      },
      "source": [
        "# set env variable for data\n",
        "os.environ['L5KIT_DATA_FOLDER'] = '/content/drive/Shared drives/social-LSTM/l5kit/'\n",
        "dm = LocalDataManager(None)\n",
        "VALIDATION = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp-0YGgYtM1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42912b69-a495-4c9a-d0b3-c83ca774947f"
      },
      "source": [
        "#cfg = load_config_data(\"/content/drive/My Drive/l5kit/examples/agent_motion_prediction/agent_motion_config.yaml\")\n",
        "\n",
        "cfg = load_config_data('/content/drive/Shared drives/social-LSTM/l5kit/examples/agent_motion_prediction/agent_motion_config.yaml')\n",
        "cfg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'format_version': 4,\n",
              " 'model_params': {'future_delta_time': 0.1,\n",
              "  'future_num_frames': 50,\n",
              "  'future_step_size': 1,\n",
              "  'history_delta_time': 0.1,\n",
              "  'history_num_frames': 50,\n",
              "  'history_step_size': 1,\n",
              "  'model_architecture': 'resnet50'},\n",
              " 'raster_params': {'dataset_meta_key': 'meta.json',\n",
              "  'disable_traffic_light_faces': False,\n",
              "  'ego_center': [0.25, 0.5],\n",
              "  'filter_agents_threshold': 0.5,\n",
              "  'map_type': 'py_semantic',\n",
              "  'pixel_size': [0.5, 0.5],\n",
              "  'raster_size': [224, 224],\n",
              "  'satellite_map_key': 'aerial_map/aerial_map.png',\n",
              "  'semantic_map_key': 'semantic_map/semantic_map.pb'},\n",
              " 'train_data_loader': {'batch_size': 12,\n",
              "  'key': '/content/drive/Shared drives/social-LSTM/l5kit/scenes/sample.zarr',\n",
              "  'num_workers': 16,\n",
              "  'shuffle': True},\n",
              " 'train_params': {'checkpoint_every_n_steps': 10000,\n",
              "  'eval_every_n_steps': 10000,\n",
              "  'max_num_steps': 5},\n",
              " 'val_data_loader': {'batch_size': 12,\n",
              "  'key': '/content/drive/Shared drives/social-LSTM/l5kit/scenes/sample.zarr',\n",
              "  'num_workers': 16,\n",
              "  'shuffle': False}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "965wM1Ippnv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae137ea4-1f72-4b5b-e01b-fea8cc0e128a"
      },
      "source": [
        "# ===== INIT DATASET\n",
        "train_cfg = cfg[\"train_data_loader\"]\n",
        "\n",
        "# Rasterizer\n",
        "rasterizer = build_rasterizer(cfg, dm)\n",
        "\n",
        "# Train dataset/dataloader\n",
        "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
        "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              shuffle=train_cfg[\"shuffle\"],\n",
        "                              batch_size=train_cfg[\"batch_size\"],\n",
        "                              num_workers=4)#train_cfg[\"num_workers\"])\n",
        "\n",
        "\n",
        "tr_it = iter(train_dataloader)\n",
        "\n",
        "data = next(tr_it)\n",
        "\n",
        "batch_size, step_size, fea_size = data['history_positions'].shape\n",
        "# step_size = 20\n",
        "\n",
        "\n",
        "\n",
        "#Need to modify\n",
        "input_dim = fea_size\n",
        "hidden_dim = fea_size\n",
        "output_dim = fea_size\n",
        "predic_time_seq_len = 50\n",
        "encoder_h_dim = hidden_dim\n",
        "decoder_h_dim = hidden_dim\n",
        "print(train_dataset)\n",
        "print(step_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "|    100     |   24838    |  1893736   |     316008    |       0.69      |        248.38        |        76.24         |        24.83         |        10.00        |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF7Dg3XlNpL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69964917-0aeb-452b-afc2-ebb21137a690"
      },
      "source": [
        "print(fea_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPjLEUmgIaWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e371b23-9ff4-4773-f89e-a1c43331c973"
      },
      "source": [
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "20dBaG7Hpnv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c897622-6111-4462-f191-4305073046e5"
      },
      "source": [
        "# ===== INIT  VAL DATASET\n",
        "val_cfg = cfg[\"val_data_loader\"]\n",
        "\n",
        "# Rasterizer\n",
        "rasterizer = build_rasterizer(cfg, dm)\n",
        "\n",
        "# Train dataset/dataloader\n",
        "val_zarr = ChunkedDataset(dm.require(val_cfg[\"key\"])).open()\n",
        "val_dataset = AgentDataset(cfg, val_zarr, rasterizer)\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                              shuffle=val_cfg[\"shuffle\"],\n",
        "                              batch_size=val_cfg[\"batch_size\"],\n",
        "                              num_workers=train_cfg[\"num_workers\"])\n",
        "\n",
        "print(val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "|    100     |   24838    |  1893736   |     316008    |       0.69      |        248.38        |        76.24         |        24.83         |        10.00        |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEboTSYMysCS"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train = 5\n",
        "pred = 1\n",
        "\n",
        "def data_handle_for_lstm(dir, train_seq_len= train, pred_seq_len=pred, frame_lenth_cap=train + pred):\n",
        "#     '''\n",
        "#     This function returns train sequences and prediction sequences for LSTM and RNN\n",
        "#     :param dir: directory where the dataset is present\n",
        "#     :param train_seq_len: length of the train sequence\n",
        "#     :param pred_seq_len: length of the prediction sequence\n",
        "#     :param frame_lenth_cap: minimum length of time presence the agent must have\n",
        "#     :return: train sequences and prediction sequences\n",
        "#     '''\n",
        "\n",
        "     data = np.load(dir)\n",
        "     d_IDs = np.unique(data[:, 4]).astype(int)\n",
        "\n",
        "     train_sequence = []\n",
        "     pred_sequence = []\n",
        "     sz = len(d_IDs)\n",
        "\n",
        "     mx = 0\n",
        "\n",
        "     agent_IDs = np.unique(data[:,1]).astype(int)\n",
        "\n",
        "     total_frames = int(np.amax(data[:, 0]))\n",
        "\n",
        "     # traversing through each agent from all agents corresponding to that dataset\n",
        "     for agent_id in agent_IDs:\n",
        "\n",
        "         # obtaining the trajectory of that individual agent\n",
        "         one_agent_traj = data[np.where(data[:, 1] == agent_id)]  # each_agent_id\n",
        "\n",
        "         # selecting the agent only if it is visible in at least 'frame_length_cap' number of frames\n",
        "         mx = max(mx, len(one_agent_traj[:, 0]))\n",
        "\n",
        "         if len(one_agent_traj[:, 0]) >= frame_lenth_cap:\n",
        "\n",
        "             # initializing an index range to obtain the train sequences and pred sequences\n",
        "             index_range = len(one_agent_traj[:, 0]) - train_seq_len - pred_seq_len\n",
        "\n",
        "             # obtaining the sequences\n",
        "             for idx in range(1, index_range + 2):\n",
        "                 train_sequence_dict = {}\n",
        "                 pred_sequence_dict = {}\n",
        "\n",
        "                 train_sequence_dict['agent_ID'] = agent_id\n",
        "\n",
        "                 # train_sequence_dict['sequence'] = one_agent_traj[idx:idx+train_seq_len,2:4]\n",
        "                 #Get its (x, y) under the time range\n",
        "                 train_sequence_dict['sequence'] = one_agent_traj[idx - 1:idx - 1 + train_seq_len, 2:4]\n",
        "\n",
        "                 pred_sequence_dict['agent_ID'] = agent_id\n",
        "\n",
        "                 # pred_sequence_dict['sequence'] = one_agent_traj[idx+train_seq_len:idx+train_seq_len+pred_seq_len,2:4]\n",
        "                 #This is for test the accuracy\n",
        "                 pred_sequence_dict['sequence'] = one_agent_traj[\n",
        "                                                  idx - 1 + train_seq_len:idx - 1 + train_seq_len + pred_seq_len,\n",
        "                                                  2:4]\n",
        "\n",
        "                 train_sequence.append(train_sequence_dict)\n",
        "                 pred_sequence.append(pred_sequence_dict)\n",
        "\n",
        "     return train_sequence, pred_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWRlC4d1C-UM"
      },
      "source": [
        "# **Pre-Process data**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnhJQXAjC9o3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwoloqfwHcgA"
      },
      "source": [
        "# **Encoder LSTM**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "U4A1WEYGpnwF"
      },
      "source": [
        "class Encoder ( nn.Module ):\n",
        "    def __init__ ( self , input_size , cell_size , hidden_size ):\n",
        "        \"\"\"\n",
        "        cell_size is the size of cell_state.\n",
        "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
        "        \"\"\"\n",
        "        super ( Encoder , self ).__init__ ()\n",
        "\n",
        "        self.cell_size = cell_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fl = nn.Linear ( input_size + hidden_size , hidden_size )\n",
        "        self.il = nn.Linear ( input_size + hidden_size , hidden_size )\n",
        "        self.ol = nn.Linear ( input_size + hidden_size , hidden_size )\n",
        "        self.Cl = nn.Linear ( input_size + hidden_size , hidden_size )\n",
        "\n",
        "    #Here x1, y1 are the nodes with (x,y)\n",
        "    def computeDist ( self , x1 , y1 ):\n",
        "        return np.abs ( x1 - y1 )\n",
        "\n",
        "    #This is what we can use for social LSTM later (haven't defined)\n",
        "    def computeKNN ( self , curr_dict , ID , k ):\n",
        "        import heapq\n",
        "        from operator import itemgetter\n",
        "\n",
        "        ID_x = curr_dict[ ID ]\n",
        "        dists = {}\n",
        "        for j in range ( len ( curr_dict ) ):\n",
        "            if j != ID:\n",
        "                dists[ j ] = self.computeDist ( ID_x , curr_dict[ j ] )\n",
        "        KNN_IDs = dict ( heapq.nsmallest ( k , dists.items () , key=itemgetter ( 1 ) ) )\n",
        "        neighbors = list ( KNN_IDs.keys () )\n",
        "\n",
        "        return neighbors\n",
        "        # return [1,2,3]\n",
        "\n",
        "    def compute_A ( self , xt ):\n",
        "        # return Variable(torch.Tensor(np.ones([xt.shape[0],xt.shape[0]])).cuda())\n",
        "        xt = xt.cpu ().detach ().numpy ()\n",
        "        A = np.zeros ( [ xt.shape[ 0 ] , xt.shape[ 0 ] ] )\n",
        "        for i in range ( len ( xt ) ):\n",
        "            #if xt[ i ] is not None:\n",
        "            if xt[i][0] and xt[i][1] :\n",
        "                neighbors = self.computeKNN ( xt , i , 4 )\n",
        "                for neighbor in neighbors:\n",
        "                    # if neighbor in labels:\n",
        "                    # if idx < labels.index ( neighbor ):\n",
        "                    A[ i ][ neighbor ] = 1\n",
        "        return Variable ( torch.Tensor ( A ).cuda () )\n",
        "\n",
        "\n",
        "    def forward ( self , input , Hidden_State , Cell_State ):\n",
        "\n",
        "        combined = torch.cat ( (input , Hidden_State) , 1 )\n",
        "        f = torch.sigmoid ( self.fl ( combined ) )\n",
        "        i = torch.sigmoid ( self.il ( combined ) )\n",
        "        o = torch.sigmoid ( self.ol ( combined ) )\n",
        "        g = torch.tanh ( self.Cl ( combined ) )\n",
        "        Cell_State = f * Cell_State + i * g\n",
        "        Hidden_State = o * torch.tanh ( Cell_State )\n",
        "\n",
        "        return Hidden_State , Cell_State\n",
        "\n",
        "    def loop ( self , inputs ):\n",
        "        batch_size = inputs.size ( 0 )\n",
        "        time_step = inputs.size ( 1 )\n",
        "        Hidden_State , Cell_State = self.initHidden ( batch_size )\n",
        "        for i in range ( time_step ):\n",
        "            Hidden_State , Cell_State = self.forward(torch.squeeze(inputs[:, i:i+1,:]), Hidden_State, Cell_State)\n",
        "        return Hidden_State , Cell_State\n",
        "\n",
        "    def initHidden ( self , batch_size ):\n",
        "        Hidden_State = Variable ( torch.zeros ( batch_size , self.hidden_size ).to ( device ) )\n",
        "        Cell_State = Variable ( torch.zeros ( batch_size , self.hidden_size ).to ( device ) )\n",
        "        return Hidden_State , Cell_State\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM350fVMvzRy"
      },
      "source": [
        "# **Revised Encoder part(Social-Pooling Layer)**\n",
        "\n",
        "---\n",
        "\n",
        "Here I want my input will be 1) hidden_state output for encoder, 2) time_neighbor matrix (identity matrix) 3) coordination  to create the model\n",
        "\n",
        "Here input at following part is dataframe for whole agents: [agent 1, agent 2, ... agent_n]\n",
        "\n",
        "I assume that you each time we only have one agent to feed so that here it is not the matrix, it should be vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phb5dDdHv6BC"
      },
      "source": [
        "def make_mlp(dim_list, activation='relu', batch_norm=False, dropout=0):\n",
        "    layers = []\n",
        "    for dim_in, dim_out in zip(dim_list[:-1], dim_list[1:]):\n",
        "        layers.append(nn.Linear(dim_in, dim_out))\n",
        "        if batch_norm:\n",
        "            layers.append(nn.BatchNorm1d(dim_out))\n",
        "        if activation == 'relu':\n",
        "            layers.append(nn.ReLU())\n",
        "        elif activation == 'leakyrelu':\n",
        "            layers.append(nn.LeakyReLU())\n",
        "        if dropout > 0:\n",
        "            layers.append(nn.Dropout(p=dropout))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class SocialPooling(nn.Module):\n",
        "    def __init__(\n",
        "        self, h_dim, activation='relu', batch_norm=True, dropout=0.0,\n",
        "        neighborhood_size=2.0, grid_size=8, pool_dim=None\n",
        "    ):\n",
        "        super(SocialPooling, self).__init__()\n",
        "        self.h_dim = h_dim\n",
        "        self.grid_size = grid_size\n",
        "        self.neighborhood_size = neighborhood_size\n",
        "        if pool_dim:\n",
        "            mlp_pool_dims = [grid_size * grid_size * h_dim, pool_dim]\n",
        "        else:\n",
        "            mlp_pool_dims = [grid_size * grid_size * h_dim, h_dim]\n",
        "\n",
        "        self.mlp_pool = make_mlp(\n",
        "            mlp_pool_dims,\n",
        "            activation=activation,\n",
        "            batch_norm=batch_norm,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "    def get_bounds(self, ped_pos):\n",
        "        top_left_x = ped_pos[:, 0] - self.neighborhood_size / 2\n",
        "        top_left_y = ped_pos[:, 1] + self.neighborhood_size / 2\n",
        "        bottom_right_x = ped_pos[:, 0] + self.neighborhood_size / 2\n",
        "        bottom_right_y = ped_pos[:, 1] - self.neighborhood_size / 2\n",
        "        top_left = torch.stack([top_left_x, top_left_y], dim=1)\n",
        "        bottom_right = torch.stack([bottom_right_x, bottom_right_y], dim=1)\n",
        "        return top_left, bottom_right\n",
        "\n",
        "    def get_grid_locations(self, top_left, other_pos):\n",
        "        cell_x = torch.floor(\n",
        "            ((other_pos[:, 0] - top_left[:, 0]) / self.neighborhood_size) *\n",
        "            self.grid_size)\n",
        "        cell_y = torch.floor(\n",
        "            ((top_left[:, 1] - other_pos[:, 1]) / self.neighborhood_size) *\n",
        "            self.grid_size)\n",
        "        grid_pos = cell_x + cell_y * self.grid_size\n",
        "        return grid_pos\n",
        "\n",
        "    def repeat(self, tensor, num_reps):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        -tensor: 2D tensor of any shape\n",
        "        -num_reps: Number of times to repeat each row\n",
        "        Outpus:\n",
        "        -repeat_tensor: Repeat each row such that: R1, R1, R2, R2\n",
        "        \"\"\"\n",
        "        col_len = tensor.size(1)\n",
        "        #print(col_len)\n",
        "        \n",
        "        tensor = tensor.unsqueeze(dim=1).repeat(1, num_reps, 1)\n",
        "\n",
        "        #print('starrrrrrrrr')\n",
        "        \n",
        "        \n",
        "        tensor = tensor.view(-1, col_len)\n",
        "        \n",
        "        #print(tensor.shape)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def forward(self, h_states, seq_start_end, end_pos):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - h_states: Tesnsor of shape (batch, h_dim)\n",
        "        - seq_start_end: A list of tuples which delimit sequences within batch.\n",
        "        - end_pos: Absolute end position of obs_traj (batch, 2)\n",
        "        Output:\n",
        "        - pool_h: Tensor of shape (batch, h_dim)\n",
        "        \"\"\"\n",
        "        pool_h = []\n",
        "        for _, x in enumerate(seq_start_end):\n",
        "            \n",
        "            start = x[0][0].item()\n",
        "            end = x[0][1].item()\n",
        "            num_ped = end - start\n",
        "            grid_size = self.grid_size * self.grid_size\n",
        "\n",
        "            curr_hidden = h_states.view(-1, self.h_dim)[start:end]\n",
        "\n",
        "            #print(curr_hidden.shape)\n",
        "\n",
        "\n",
        "            curr_hidden_repeat = curr_hidden.repeat(num_ped, 1)\n",
        "\n",
        "            #print(\"this is the output for curr_hidden_repeat\")\n",
        "            #print(curr_hidden_repeat.shape)\n",
        "\n",
        "            curr_hidden_repeat = curr_hidden_repeat.to(device = device)\n",
        "\n",
        "\n",
        "            curr_end_pos = end_pos[start:end]\n",
        "\n",
        "            curr_pool_h_size = (num_ped * grid_size) + 1\n",
        "            curr_pool_h = curr_hidden.new_zeros((curr_pool_h_size, self.h_dim))\n",
        "            \n",
        "            curr_pool_h = curr_pool_h.to(device = device)\n",
        "            \n",
        "            # curr_end_pos = curr_end_pos.data\n",
        "            top_left, bottom_right = self.get_bounds(curr_end_pos)\n",
        "\n",
        "            # Repeat position -> P1, P2, P1, P2\n",
        "            \n",
        "            #print('start curr_end_pos')\n",
        "            \n",
        "            \n",
        "            curr_end_pos = curr_end_pos.repeat(num_ped, 1)\n",
        "\n",
        "            #print(curr_end_pos.shape)\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "            # Repeat bounds -> B1, B1, B2, B2\n",
        "            top_left = self.repeat(top_left, num_ped)\n",
        "            bottom_right = self.repeat(bottom_right, num_ped)\n",
        "\n",
        "            grid_pos = self.get_grid_locations(\n",
        "                    top_left, curr_end_pos).type_as(seq_start_end)\n",
        "            # Make all positions to exclude as non-zero\n",
        "            # Find which peds to exclude\n",
        "            x_bound = ((curr_end_pos[:, 0] >= bottom_right[:, 0]) +\n",
        "                       (curr_end_pos[:, 0] <= top_left[:, 0]))\n",
        "            y_bound = ((curr_end_pos[:, 1] >= top_left[:, 1]) +\n",
        "                       (curr_end_pos[:, 1] <= bottom_right[:, 1]))\n",
        "\n",
        "            within_bound = x_bound + y_bound\n",
        "            within_bound[0::num_ped + 1] = 1  # Don't include the ped itself\n",
        "            within_bound = within_bound.view(-1)\n",
        "\n",
        "            # This is a tricky way to get scatter add to work. Helps me avoid a\n",
        "            # for loop. Offset everything by 1. Use the initial 0 position to\n",
        "            # dump all uncessary adds.\n",
        "            grid_pos += 1\n",
        "            total_grid_size = self.grid_size * self.grid_size\n",
        "            offset = torch.arange(\n",
        "                0, total_grid_size * num_ped, total_grid_size\n",
        "            ).type_as(seq_start_end)\n",
        "\n",
        "            offset = self.repeat(offset.view(-1, 1), num_ped).view(-1)\n",
        "\n",
        "            #print(offset.shape)\n",
        "            #print(grid_pos.shape)\n",
        "\n",
        "\n",
        "            #grid_pos += offset\n",
        "            grid_pos[within_bound != 0] = 0\n",
        "\n",
        "            curr_hidden_repeat = curr_hidden_repeat.to(device)\n",
        "            grid_pos = grid_pos.to(device)\n",
        "            curr_pool_h = curr_pool_h.to(device)\n",
        "\n",
        "            grid_pos = grid_pos.view(-1, 1).expand_as(curr_hidden_repeat)\n",
        "\n",
        "            curr_pool_h = curr_pool_h.scatter_add(0, grid_pos,\n",
        "                                                  curr_hidden_repeat)\n",
        "            curr_pool_h = curr_pool_h[1:]\n",
        "            curr_pool_h = curr_pool_h.to(device = device)\n",
        "            pool_h.append(curr_pool_h.view(num_ped, -1))\n",
        "\n",
        "        pool_h = torch.cat(pool_h, dim=0)\n",
        "        pool_h = self.mlp_pool(pool_h)\n",
        "\n",
        "        \n",
        "        pool_h = pool_h.narrow(0, 0, h_states.shape[0])\n",
        "\n",
        "        #print('this is the final pool_h size')\n",
        "        #print(pool_h.shape)\n",
        "\n",
        "\n",
        "        pool_h = pool_h.to(device)\n",
        "        return pool_h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6QovcKAHX28"
      },
      "source": [
        "# **Decoder LSTM**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1GDCq3tHRZ4"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    # def __init__(self, cfg):\n",
        "    def __init__(self, input_size, cell_size, hidden_size, batchsize, timestep):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.cell_size = cell_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batchsize\n",
        "        self.time_step = timestep\n",
        "        self.num_mog_params = 5\n",
        "        self.sampled_point_size = 2\n",
        "        #self.stream = stream\n",
        "        self.stream_specific_param = self.num_mog_params\n",
        "        self.fl = nn.Linear(self.stream_specific_param + hidden_size, hidden_size)\n",
        "        self.il = nn.Linear(self.stream_specific_param + hidden_size, hidden_size)\n",
        "        self.ol = nn.Linear(self.stream_specific_param + hidden_size, hidden_size)\n",
        "        self.Cl = nn.Linear(self.stream_specific_param + hidden_size, hidden_size)\n",
        "        self.linear1 = nn.Linear(cell_size, self.stream_specific_param)\n",
        "        # self.one_lstm = nn.LSTMCell\n",
        "        # self.linear2 = nn.Linear ( self.sampled_point_size ,  hidden_size )\n",
        "\n",
        "        #print(\"Hidden_size\")\n",
        "        #print(self.hidden_size)\n",
        "        #print(\"stream_spefic_param\")\n",
        "        #print(self.stream_specific_param)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input, Hidden_State, Cell_State):\n",
        "\n",
        "        #print('input~')\n",
        "        #print(input.shape)\n",
        "\n",
        "        combined = torch.cat((input, Hidden_State), 1)\n",
        "\n",
        "        #print(\"Input shape\")\n",
        "        #print(input.shape)\n",
        "        #print(\"Hidden_State shape\")\n",
        "        #print(Hidden_State.shape)\n",
        "        #print(\"Combined shape\")\n",
        "        #print(combined.shape)\n",
        "\n",
        "        \n",
        "\n",
        "        f = torch.sigmoid(self.fl(combined))\n",
        "        i = torch.sigmoid(self.il(combined))\n",
        "        o = torch.sigmoid(self.ol(combined))\n",
        "        g = torch.tanh(self.Cl(combined))\n",
        "        Cell_State = f * Cell_State + i * g\n",
        "        Hidden_State = o * torch.tanh(Cell_State)\n",
        "\n",
        "        return Hidden_State, Cell_State\n",
        "\n",
        "    def loop(self, hidden_vec_from_encoder):\n",
        "        batch_size = self.batch_size\n",
        "        time_step = self.time_step\n",
        "\n",
        "        init_hidden_output = self.initHidden()\n",
        "\n",
        "        Cell_State, out = init_hidden_output[0], init_hidden_output[1]\n",
        "\n",
        "        mu1_all , mu2_all , sigma1_all , sigma2_all , rho_all = self.initMogParams()\n",
        "\n",
        "        for i in range(time_step):\n",
        "            if i == 0:\n",
        "                Hidden_State = hidden_vec_from_encoder\n",
        "            Hidden_State, Cell_State = self.forward(out, Hidden_State, Cell_State)\n",
        "\n",
        "            mog_params = self.linear1(Hidden_State)\n",
        "\n",
        "            out = mog_params\n",
        "\n",
        "            mu_1 , mu_2 , log_sigma_1 , log_sigma_2 , pre_rho = mog_params.chunk ( 6 , dim=-1 )\n",
        "            rho = torch.tanh ( pre_rho )\n",
        "            log_sigma_1 = torch.exp(log_sigma_1)\n",
        "            log_sigma_2 = torch.exp(log_sigma_2)\n",
        "            mu1_all[:,i,:] = mu_1\n",
        "            mu2_all[:,i,:] = mu_2\n",
        "            sigma1_all[:,i,:] = log_sigma_1\n",
        "            sigma2_all[:,i,:] = log_sigma_2\n",
        "            rho_all[:,i,:] = rho\n",
        "\n",
        "        Stream_output  = torch.cat((mu1_all,mu2_all), dim=2)\n",
        "\n",
        "        return Stream_output, Cell_State\n",
        "\n",
        "\n",
        "    def initHidden(self):\n",
        "        out = torch.randn(self.batch_size, self.num_mog_params, device=device)\n",
        "        \n",
        "        return torch.randn(self.batch_size, self.hidden_size, device=device), out\n",
        "    \n",
        "\n",
        "    #Here i want to store (x,y) for the future predictions\n",
        "    def initMogParams(self):\n",
        "        mu1_all = torch.randn(self.batch_size, self.time_step, 1, device=device)\n",
        "        mu2_all = torch.randn(self.batch_size, self.time_step, 1, device=device)\n",
        "        sigma1_all = torch.randn(self.batch_size, self.time_step, 1, device=device)\n",
        "        sigma2_all = torch.randn(self.batch_size, self.time_step, 1, device=device)\n",
        "        rho_all = torch.randn(self.batch_size, self.time_step, 1, device=device)\n",
        "\n",
        "        return mu1_all, mu2_all, sigma1_all, sigma2_all, rho_all    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7tj8MfeTI2p"
      },
      "source": [
        "# **Original LSTM**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "J_e7qWSnpnwM"
      },
      "source": [
        "# ==== INIT MODEL\n",
        "learning_rate = 1e-3\n",
        "\n",
        "encoder_stream = Encoder(input_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "decoder_stream = Decoder(input_dim, hidden_dim, output_dim, batch_size, predic_time_seq_len).to(device)\n",
        "\n",
        "\n",
        "encoder_stream_optimizer = optim.RMSprop(encoder_stream.parameters(), lr=learning_rate)\n",
        "decoder_stream_optimizer = optim.RMSprop(decoder_stream.parameters(), lr=learning_rate)\n",
        "\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=7000,gamma=0.1)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oMfu67hmRpK"
      },
      "source": [
        "def train_stream(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer):\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    Hidden_State, _ = encoder.loop(input_tensor)\n",
        "\n",
        "\n",
        "    #print(Hidden_State.shape)\n",
        "\n",
        "    stream_out, _ = decoder.loop(Hidden_State)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    #print(\"This is the beginning!!!!\")\n",
        "    #print(target_tensor.shape)\n",
        "\n",
        "    #print(\"This is the output for the stream we want to predict\")\n",
        "    #print(stream_out.shape)\n",
        "\n",
        "    loss = criterion(target_tensor, stream_out)\n",
        "\n",
        "    #loss = loss if loss > 0 else -1 * loss\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GVgInipCpnwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f48e63f-684a-4a95-c11e-519a09b1100b"
      },
      "source": [
        "import pdb\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# ==== TRAIN LOOP\n",
        "tr_it = iter(train_dataloader)\n",
        "vl_it = iter(val_dataloader)\n",
        "\n",
        "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
        "losses_train = []\n",
        "losses_mean_train = []\n",
        "losses_val = []\n",
        "losses_mean_val = []\n",
        "\n",
        "for itr in progress_bar:\n",
        "    try:\n",
        "        data = next(tr_it)\n",
        "    except StopIteration:\n",
        "        tr_it = iter(train_dataloader)\n",
        "        data = next(tr_it)\n",
        "    \n",
        "    #You need to train two parts\n",
        "    encoder_stream.train()\n",
        "    \n",
        "    decoder_stream.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    # Forward pass\n",
        "    history_positions = data['history_positions'].to(device)\n",
        "    history_availabilities = data['history_availabilities'].to(device)\n",
        "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
        "    targets_position = data[\"target_positions\"].to(device)\n",
        "    \n",
        "    # data_handle_for_lstm(dir, train_seq_len= train, pred_seq_len=pred, frame_lenth_cap=train + pred)\n",
        "\n",
        "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
        "\n",
        "    #print(history_positions.shape)\n",
        "    #print(targets_position.shape)\n",
        "\n",
        "    #print(history_positions)\n",
        "    #print(targets_position)\n",
        "\n",
        "\n",
        "    loss = train_stream(history_positions, targets_position, encoder_stream, decoder_stream, encoder_stream_optimizer, decoder_stream_optimizer)\n",
        "    # loss = train_stream(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "    loss = loss.mean()\n",
        "\n",
        "    print(loss.item())\n",
        "    \n",
        "    losses_train.append(loss.item())\n",
        "    losses_mean_train.append(np.mean(losses_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|        | 1/5 [00:25<01:40, 25.14s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60.50748825073242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 40%|      | 2/5 [00:28<00:55, 18.64s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "85.8955078125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 60%|    | 3/5 [00:31<00:27, 13.98s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "154.9336700439453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 80%|  | 4/5 [00:35<00:10, 10.93s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "107.52083587646484\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|| 5/5 [00:46<00:00,  9.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "123.9096908569336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAK6w_ZGvNjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d217ed-d46d-4c77-b1ec-c9bd89dde29f"
      },
      "source": [
        "print(losses_mean_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[60.50748825073242, 73.20149803161621, 100.44555536905925, 102.21437549591064, 106.55343856811524]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1ZW1PE4KNus"
      },
      "source": [
        "# **Create a Class about all social parts**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKr7ri2GKOOI"
      },
      "source": [
        "class SocialLSTM(nn.Module):\n",
        "  def __init__(self, input_size, output_size, predic_tim_seq_len, encoder_h_dim, decoder_h_dim, mlp_dim = 1024, \n",
        "                dropout = 0.0, activation = 'relu', batch_norm = True, neighborhood_size = 2.0, grid_size = 8, bottleneck_dim=1024):\n",
        "    super(SocialLSTM, self).__init__()\n",
        "\n",
        "    #self.input_size = input_size\n",
        "    #self.obs_len = obs_len\n",
        "    self.pred_len = predic_time_seq_len\n",
        "    #self.seq_len = obs_len + pred_len\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.seq_start_end = seq_start_end\n",
        "    \n",
        "    self.decoder_input = input_size\n",
        "\n",
        "    self.encoder = Encoder(input_size, encoder_h_dim, output_size)\n",
        "    self.decoder = Decoder(input_size, decoder_h_dim, output_size, batch_size, predic_time_seq_len)\n",
        "    self.pool_net = SocialPooling(h_dim = encoder_h_dim, activation  = activation, batch_norm = batch_norm, dropout = dropout, neighborhood_size=neighborhood_size,grid_size = grid_size)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    #Decoder hidden:\n",
        "    #input_dim = encoder_h_dim + bottleneck_dim\n",
        "\n",
        "    mlp_decoder_context_dims = [input_dim, mlp_dim, decoder_h_dim]\n",
        "\n",
        "    self.mlp_decoder_context = make_mlp(\n",
        "                mlp_decoder_context_dims,\n",
        "                activation=activation,\n",
        "                batch_norm=batch_norm,\n",
        "                dropout=dropout\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input_tensor, obs_traj, seq_start_end):\n",
        "    #Here I want to know the last one end_pos\n",
        "    #I assume that obs_traj: Tensor of shape (obs_len, batch, 2)\n",
        "\n",
        "\n",
        "    encoder_hidden_state, _  = self.encoder.loop(input_tensor)\n",
        "\n",
        "    end_pos = obs_traj[:,-1,:]\n",
        "\n",
        "    pool_h = self.pool_net(encoder_hidden_state, seq_start_end, end_pos)\n",
        "\n",
        "    #print(pool_h.shape)\n",
        "    #print(encoder_hidden_state.shape)\n",
        "    \n",
        "    x = pool_h + encoder_hidden_state\n",
        "    #print(x.shape)\n",
        "\n",
        "\n",
        "    mlp_decoder_context_input = torch.cat([encoder_hidden_state.view(-1, encoder_h_dim), pool_h], dim=1)\n",
        "\n",
        "    #decoder_hidden = torch.unsqueeze(mlp_decoder_context_input, 0)\n",
        "\n",
        "    #Original one\n",
        "    #decoder_hidden = mlp_decoder_context_input\n",
        "\n",
        "    decoder_hidden = x\n",
        "\n",
        "    #print('decoder_hidden')\n",
        "    #print(decoder_hidden.shape)\n",
        "    #print('mlp')\n",
        "    #print(mlp_decoder_context_input.shape)\n",
        "\n",
        "    stream_out, _ = self.decoder.loop(decoder_hidden)\n",
        "\n",
        "\n",
        "\n",
        "    return stream_out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLRck5Lu-YfN"
      },
      "source": [
        "\n",
        "\n",
        "# **Prepare the data for the start and end**\n",
        "\n",
        "---\n",
        "\n",
        "Here I want to return a dataframe \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mENo3Cc7-pTG"
      },
      "source": [
        "def parse_scene(scene):\n",
        "    scene_dict = {\n",
        "            \"frame_index_interval_start\": scene[0][0],\n",
        "            \"frame_index_interval_end\": scene[0][1],\n",
        "            \"host\":  scene[1],\n",
        "            \"start_time\": scene[2],\n",
        "            \"end_time\": scene[3]\n",
        "        }\n",
        "    return scene_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkR23sp6_GUn"
      },
      "source": [
        "DATA_ROOT = Path(\"/content/drive\")\n",
        "\n",
        "class BaseParser:\n",
        "    \"\"\"\n",
        "    A robust and fast interface to load l5kit data into  Pandas dataframes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    chunk_size: int, default=1000\n",
        "        How many items do you want in a single slice. The larger the better;\n",
        "        as long as you have enough memory. Nevertheless, chunk sizes above `10_000` won't lead to\n",
        "        significant speed gain as the original zarr files was chunked at 10_000.\n",
        "\n",
        "    max_chunks: int, default=10\n",
        "        How many chunks do you want to read from memory.\n",
        "\n",
        "    root:\n",
        "        Zarr data root path\n",
        "\n",
        "    zarr_path:\n",
        "        relative path or key to the data.\n",
        "    \"\"\"\n",
        "    \n",
        "    field = \"scenes\"\n",
        "    dtypes = {}\n",
        "    \n",
        "    def __init__(self, start=0, end=None, chunk_size=1000, max_chunks=10, root=DATA_ROOT, zarr_path=\"Shared drives/social-LSTM/l5kit/scenes/sample.zarr\"):\n",
        "        \n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.chunk_size = chunk_size\n",
        "        self.max_chunks = max_chunks\n",
        "        \n",
        "\n",
        "        self.root = Path(root)\n",
        "        assert self.root.exists(), \"There is nothing at {}!\".format(self.root)\n",
        "        self.zarr_path = Path(zarr_path)\n",
        "        \n",
        "     \n",
        "    def parse(self):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def to_pandas(self, start=0, end=None, chunk_size=None, max_chunks=None):\n",
        "        start = start or self.start\n",
        "        end = end or self.end\n",
        "        chunk_size = chunk_size or self.chunk_size\n",
        "        max_chunks = max_chunks or self.max_chunks\n",
        "        \n",
        "        if not chunk_size or  not max_chunks: # One shot load, suitable for small zarr files\n",
        "            df = zarr.load(self.root.joinpath(self.zarr_path).as_posix()).get(self.field)\n",
        "            df = df[start:end]\n",
        "            df = map(self.parse, df) \n",
        "        else: # Chunked load, suitable for large zarr files\n",
        "            df = []\n",
        "            with zarr.open(self.root.joinpath(self.zarr_path).as_posix(), \"r\") as zf:\n",
        "                end = start+max_chunks*chunk_size if end is None else min(end, start+max_chunks*chunk_size)\n",
        "                for i_start in range(start, end, chunk_size ):\n",
        "                    items = zf[self.field][i_start: min(i_start + chunk_size,end)]\n",
        "                    items = map(self.parse, items)\n",
        "                    df.append(items)\n",
        "            df = it.chain(*df)\n",
        "            \n",
        "        df = pd.DataFrame.from_records(df)\n",
        "        for col, col_dtype in self.dtypes.items():\n",
        "            df[col] = df[col].astype(col_dtype, copy=False)\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpHEaIK2_Q7q"
      },
      "source": [
        "class SceneParser(BaseParser):\n",
        "    field = \"scenes\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def parse(scene):\n",
        "        scene_dict = {\n",
        "            \"frame_index_interval_start\": scene[0][0],\n",
        "            \"frame_index_interval_end\": scene[0][1],\n",
        "            \"host\":  scene[1],\n",
        "            \"start_time\": scene[2],\n",
        "            \"end_time\": scene[3]\n",
        "        }\n",
        "        return scene_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY2eFXkE-ZIu"
      },
      "source": [
        "def parse_frame(frame):\n",
        "    frame_dict = {\n",
        "        'timestamp': frame[0],\n",
        "        'agent_index_interval_start': frame[1][0],\n",
        "        'agent_index_interval_start': frame[1][1],\n",
        "        'traffic_light_faces_index_interval_start': frame[2][0],\n",
        "        'traffic_light_faces_index_interval_end': frame[2][1],\n",
        "        'ego_translation_x': frame[3][0],\n",
        "        'ego_translation_y': frame[3][1],\n",
        "        'ego_translation_z': frame[3][2],\n",
        "        'ego_rotation_xx': frame[4][0][0],\n",
        "        'ego_rotation_xy': frame[4][0][1],\n",
        "        'ego_rotation_xz': frame[4][0][2],\n",
        "        'ego_rotation_yx': frame[4][1][0],\n",
        "        'ego_rotation_yy': frame[4][1][1],\n",
        "        'ego_rotation_yz': frame[4][1][2],\n",
        "        'ego_rotation_zx': frame[4][2][0],\n",
        "        'ego_rotation_zy': frame[4][2][1],\n",
        "        'ego_rotation_zz': frame[4][2][2],\n",
        "        \n",
        "    }\n",
        "    return frame_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hD5f7s6-1C-"
      },
      "source": [
        "class FrameParser(BaseParser):\n",
        "    field = \"frames\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def parse(frame):\n",
        "        frame_dict = {\n",
        "            'timestamp': frame[0],\n",
        "            'agent_index_interval_start': frame[1][0],\n",
        "            'agent_index_interval_end': frame[1][1],\n",
        "            'traffic_light_faces_index_interval_start': frame[2][0],\n",
        "            'traffic_light_faces_index_interval_end': frame[2][1],\n",
        "            'ego_translation_x': frame[3][0],\n",
        "            'ego_translation_y': frame[3][1],\n",
        "            'ego_translation_z': frame[3][2],\n",
        "            'ego_rotation_xx': frame[4][0][0],\n",
        "            'ego_rotation_xy': frame[4][0][1],\n",
        "            'ego_rotation_xz': frame[4][0][2],\n",
        "            'ego_rotation_yx': frame[4][1][0],\n",
        "            'ego_rotation_yy': frame[4][1][1],\n",
        "            'ego_rotation_yz': frame[4][1][2],\n",
        "            'ego_rotation_zx': frame[4][2][0],\n",
        "            'ego_rotation_zy': frame[4][2][1],\n",
        "            'ego_rotation_zz': frame[4][2][2],\n",
        "\n",
        "        }\n",
        "        return frame_dict\n",
        "\n",
        "    def to_pandas(self, start=0, end=None, chunk_size=None, max_chunks=None, scene=None):\n",
        "        if scene is not None:\n",
        "            start = scene.frame_index_interval_start\n",
        "            end = scene.frame_index_interval_end\n",
        "        \n",
        "        df = super().to_pandas(start=start, end=end, chunk_size=chunk_size, max_chunks=max_chunks)\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQz6rARp_zJc"
      },
      "source": [
        "# **Input the data to generate the data**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyY0J3wc_4hF"
      },
      "source": [
        "import zarr\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "import itertools as it # I will be using the `itertools.chain` function\n",
        "from pathlib import Path # for better file/path operations management\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive\")\n",
        "\n",
        "zl5 = zarr.open(DATA_ROOT.joinpath(\"Shared drives/social-LSTM/l5kit/scenes/sample.zarr\").as_posix(), mode=\"r\")\n",
        "\n",
        "sp = SceneParser(chunk_size=None, max_chunks=None, zarr_path=\"/content/drive/Shared drives/social-LSTM/l5kit/scenes/sample.zarr\")\n",
        "\n",
        "scenes = sp.to_pandas()\n",
        "\n",
        "row_scene, col_scene = scenes.shape\n",
        "\n",
        "all_information = pd.DataFrame()\n",
        "\n",
        "\n",
        "for i in range(row_scene):\n",
        "  scene = scenes.iloc[i]\n",
        "  scene_frames = zl5.frames[scene.frame_index_interval_start:scene.frame_index_interval_end]\n",
        "  \n",
        "  fp = FrameParser()\n",
        "  frames = fp.to_pandas(scene = scene)\n",
        "\n",
        "  frames_needed = frames.iloc[:,0:3]\n",
        "\n",
        "  if i == 0:\n",
        "    all_information = frames_needed\n",
        "  else:\n",
        "    all_information = all_information.append(frames_needed, ignore_index=True)\n",
        "\n",
        "all_information['num_agent'] = all_information['agent_index_interval_end'] - all_information['agent_index_interval_start']\n",
        "\n",
        "#xx = all_information.loc[all_information['timestamp'] ==1572643684801892606]\n",
        "\n",
        "#print(xx.iloc[0]['num_agent'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX7Q8-ToBHuM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "a199eb36-7f6f-4092-d9de-87b6e2d2beb9"
      },
      "source": [
        "zl5.scenes.info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"zarr-info\"><tbody><tr><th style=\"text-align: left\">Name</th><td style=\"text-align: left\">/scenes</td></tr><tr><th style=\"text-align: left\">Type</th><td style=\"text-align: left\">zarr.core.Array</td></tr><tr><th style=\"text-align: left\">Data type</th><td style=\"text-align: left\">[('frame_index_interval', '<i8', (2,)), ('host', '<U16'), ('start_time', '<i8'), ('end_time', '<i8')]</td></tr><tr><th style=\"text-align: left\">Shape</th><td style=\"text-align: left\">(100,)</td></tr><tr><th style=\"text-align: left\">Chunk shape</th><td style=\"text-align: left\">(10000,)</td></tr><tr><th style=\"text-align: left\">Order</th><td style=\"text-align: left\">C</td></tr><tr><th style=\"text-align: left\">Read-only</th><td style=\"text-align: left\">True</td></tr><tr><th style=\"text-align: left\">Compressor</th><td style=\"text-align: left\">Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)</td></tr><tr><th style=\"text-align: left\">Store type</th><td style=\"text-align: left\">zarr.storage.DirectoryStore</td></tr><tr><th style=\"text-align: left\">No. bytes</th><td style=\"text-align: left\">9600 (9.4K)</td></tr><tr><th style=\"text-align: left\">No. bytes stored</th><td style=\"text-align: left\">5846 (5.7K)</td></tr><tr><th style=\"text-align: left\">Storage ratio</th><td style=\"text-align: left\">1.6</td></tr><tr><th style=\"text-align: left\">Chunks initialized</th><td style=\"text-align: left\">1/1</td></tr></tbody></table>"
            ],
            "text/plain": [
              "Name               : /scenes\n",
              "Type               : zarr.core.Array\n",
              "Data type          : [('frame_index_interval', '<i8', (2,)), ('host', '<U16'),\n",
              "                   : ('start_time', '<i8'), ('end_time', '<i8')]\n",
              "Shape              : (100,)\n",
              "Chunk shape        : (10000,)\n",
              "Order              : C\n",
              "Read-only          : True\n",
              "Compressor         : Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)\n",
              "Store type         : zarr.storage.DirectoryStore\n",
              "No. bytes          : 9600 (9.4K)\n",
              "No. bytes stored   : 5846 (5.7K)\n",
              "Storage ratio      : 1.6\n",
              "Chunks initialized : 1/1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qdRamXCCTkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169aeea2-2811-4280-c6c0-f9c6f1710787"
      },
      "source": [
        "print(scenes.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   frame_index_interval_start  ...             end_time\n",
            "0                           0  ...  1572643709617362176\n",
            "1                         248  ...  1572643774559148288\n",
            "2                         497  ...  1572643799559148288\n",
            "3                         746  ...  1572643824559148288\n",
            "4                         995  ...  1572643849559148288\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ9MAMtyKNTi"
      },
      "source": [
        "# **Overall revised code**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24IMPfDAUs05"
      },
      "source": [
        "# ==== INIT MODEL\n",
        "learning_rate = 1e-3\n",
        "\n",
        "seq_start_end = torch.LongTensor()\n",
        "\n",
        "slstm = SocialLSTM( input_size = input_dim, output_size= output_dim, predic_tim_seq_len= predic_time_seq_len, encoder_h_dim= hidden_dim, decoder_h_dim=decoder_h_dim).to(device)\n",
        "\n",
        "slstm_optimizer = optim.RMSprop(slstm.parameters(), lr = 0.0001)\n",
        "\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=7000,gamma=0.1)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMQIGrufNKCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78070ced-72b8-4e0f-c551-99a16632fba3"
      },
      "source": [
        "f = open(\"Social_LSTM_avg_res.csv\", \"r\")\n",
        "print(f.read()) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "249,94.20064425755696\n",
            "249,95.74866327857971\n",
            "499,90.06498186779022\n",
            "749,83.89890209293365\n",
            "999,79.85873646354675\n",
            "1249,83.32652612876892\n",
            "1499,80.92732544136048\n",
            "1749,83.49873586463929\n",
            "1999,88.57160601615905\n",
            "2249,79.78978960323333\n",
            "2499,83.80027301979065\n",
            "2749,77.24845838356018\n",
            "2999,82.62793109512329\n",
            "3249,78.21627745819092\n",
            "3499,81.6329727935791\n",
            "3749,81.42543850708007\n",
            "3999,91.67240799331665\n",
            "4249,83.56668976593018\n",
            "4499,81.01260525131225\n",
            "4749,84.35424163436889\n",
            "4999,80.2096270980835\n",
            "5249,83.1737455663681\n",
            "5499,79.60831385231018\n",
            "5749,79.98835241699219\n",
            "5999,77.40652156639099\n",
            "6249,76.53080376052857\n",
            "6499,81.5120063009262\n",
            "6749,78.79078069877625\n",
            "6999,79.65862364053726\n",
            "7249,84.00502889251709\n",
            "7499,84.55420792007446\n",
            "7749,77.42695995140076\n",
            "7999,79.78290935707092\n",
            "8249,73.33869337463379\n",
            "8499,79.7258823003769\n",
            "8749,77.49884818458557\n",
            "8999,73.97952580070496\n",
            "9249,79.75945672607422\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1b-z-w1Uh-w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "d2f611ca-feb5-4333-f8ca-a83ded0426ec"
      },
      "source": [
        "import pdb\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# ==== TRAIN LOOP\n",
        "tr_it = iter(train_dataloader)\n",
        "vl_it = iter(val_dataloader)\n",
        "\n",
        "progress_bar = tqdm(range(10000), position=0)#cfg[\"train_params\"][\"max_num_steps\"]))\n",
        "losses_train = []\n",
        "losses_mean_train = []\n",
        "losses_val = []\n",
        "losses_mean_val = []\n",
        "\n",
        "for i in progress_bar:\n",
        "    try:\n",
        "        data = next(tr_it)\n",
        "    except StopIteration:\n",
        "        tr_it = iter(train_dataloader)\n",
        "        data = next(tr_it)\n",
        "    \n",
        "    #You need to train two parts\n",
        "    slstm.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "\n",
        "    #print(data)\n",
        "\n",
        "    # Forward pass\n",
        "    history_positions = data['history_positions'].to(device)\n",
        "    history_availabilities = data['history_availabilities'].to(device)\n",
        "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
        "    targets_position = data[\"target_positions\"].to(device)\n",
        "    centroid = data['centroid'].to(device)\n",
        "\n",
        "    seq_start_end = list()\n",
        "\n",
        "    \n",
        "    for xx in (data['timestamp']):\n",
        "      #print(xx.item())\n",
        "      specific_row_dat = all_information.loc[all_information['timestamp'] ==xx.item()]\n",
        "      cum_start_idx = [0] + np.cumsum(specific_row_dat.iloc[0]['num_agent']).tolist()\n",
        "      \n",
        "      seq_start_end.append([(start, end) for start, end in zip(cum_start_idx, cum_start_idx[1:])])\n",
        "    \n",
        "    \n",
        "    seq_start_end = torch.LongTensor(seq_start_end)\n",
        "      \n",
        "    #print(seq_start_end)\n",
        "\n",
        " #   for _, x in enumerate(seq_start_end):\n",
        "#      print('sssss')\n",
        "#      print(x)\n",
        "      #print(x[0])\n",
        "      #print(x[0][0])\n",
        "#      start = x[0][0].item()\n",
        "#      print(start)\n",
        "#      end = x[0][1].item()\n",
        "#      print(end)\n",
        "      #start = x.item()\n",
        "      #end = end.item()\n",
        "      #print(start)\n",
        "      #print(end)\n",
        "      #start = x[0]\n",
        "      #end = x[1]\n",
        "      #print(start)\n",
        "      #print(end)\n",
        " #     print(x[0])\n",
        " #     print(x[0][0])\n",
        " #     print(x[0][1])\n",
        " #     print(type(x))\n",
        "\n",
        "\n",
        "\n",
        "    outputs = slstm(history_positions, history_positions, seq_start_end)\n",
        "\n",
        "    loss = criterion(outputs, targets_position)\n",
        "\n",
        "    loss = loss * target_availabilities\n",
        "    loss = loss.mean()\n",
        "\n",
        "    #Backward pass\n",
        "    slstm_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    slstm_optimizer.step()\n",
        "\n",
        "    losses_train.append(loss.item())\n",
        "    losses_mean_train.append(np.mean(losses_train))\n",
        "\n",
        "    if (i % 250) == 250 - 1:\n",
        "      with open('Social_LSTM_res2.csv','a') as fd:\n",
        "          for loss in losses_train:\n",
        "              fd.write(f\"{i},{loss}\\n\")\n",
        "      avg_loss = np.mean(losses_train)\n",
        "      losses_train = []\n",
        "      with open('Social_LSTM_avg_res2.csv','a') as fd:\n",
        "          fd.write(f\"{i},{avg_loss}\\n\")\n",
        "      progress_bar.set_description(f\"loss: {loss} loss(avg): {avg_loss}\")\n",
        "\n",
        "    # print(loss.item())\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-97ef6de5b6bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtr_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PnF95YMUhkD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ns2Gv41ApnwW"
      },
      "source": [
        "nn# ===== INIT DATASET\n",
        "test_cfg = cfg[\"test_data_loader\"]\n",
        "\n",
        "# Rasterizer\n",
        "rasterizer = build_rasterizer(cfg, dm)\n",
        "\n",
        "# Test dataset/dataloader\n",
        "test_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\n",
        "test_mask = np.load(f\"/content/drive/Shared drives/social-LSTM/l5kit/scenes/mask.npz\")[\"arr_0\"]\n",
        "test_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                             shuffle=test_cfg[\"shuffle\"],\n",
        "                             batch_size=test_cfg[\"batch_size\"],\n",
        "                             num_workers=test_cfg[\"num_workers\"])\n",
        "\n",
        "\n",
        "print(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BH0mR540pnwc"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "future_coords_offsets_pd = []\n",
        "timestamps = []\n",
        "agent_ids = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    dataiter = tqdm(test_dataloader)\n",
        "    \n",
        "    for data in dataiter:\n",
        "\n",
        "        history_positions = data['history_positions'].to(device)\n",
        "\n",
        "        outputs = model(history_positions)\n",
        "        \n",
        "        future_coords_offsets_pd.append(outputs.cpu().numpy().copy())\n",
        "        timestamps.append(data[\"timestamp\"].numpy().copy())\n",
        "        agent_ids.append(data[\"track_id\"].numpy().copy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Jf-AphnVpnwi"
      },
      "source": [
        "write_pred_csv('submission.csv',\n",
        "               timestamps=np.concatenate(timestamps),\n",
        "               track_ids=np.concatenate(agent_ids),\n",
        "               coords=np.concatenate(future_coords_offsets_pd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ7umXYiChe_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}