{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Copy of Copy of lstm-based-Ying-model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8MlMbtspnvX"
      },
      "source": [
        "As Motion Prediction has temporal data, I tried to used LSTM based seq 2 seq generation (Encoder-Decoder Model approach) atleast inspired.\n",
        "Some part of code (validation) is borrowed from @ Marco B notebook . Model itself is quite basic and this can get to a score of 400 on leaderboard.\n",
        "\n",
        "I hope this trigger some more work using LSTM and more sharing to come."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afqbuI0jp4bg"
      },
      "source": [
        "!pip install l5kit\n",
        "!pip install -U PyYAML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGVbpD5ZiDSZ"
      },
      "source": [
        "from google.colab import drive\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "op5PSXwKpnva"
      },
      "source": [
        " import numpy as np\n",
        "\n",
        "import os\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import resnet18,resnet50,resnet101\n",
        "from tqdm import tqdm\n",
        "from typing import Dict\n",
        "from torch import functional as F\n",
        "\n",
        "from l5kit.configs import load_config_data\n",
        "from l5kit.data import LocalDataManager, ChunkedDataset\n",
        "from l5kit.dataset import AgentDataset, EgoDataset\n",
        "from l5kit.rasterization import build_rasterizer\n",
        "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
        "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
        "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
        "from l5kit.geometry import transform_points\n",
        "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
        "from prettytable import PrettyTable\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": false,
        "id": "h7uTvVV6pnvj"
      },
      "source": [
        "# set env variable for data\n",
        "os.environ['L5KIT_DATA_FOLDER'] = '/content/drive/Shared drives/social-LSTM/l5kit/'\n",
        "dm = LocalDataManager(None)\n",
        "VALIDATION = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp-0YGgYtM1f"
      },
      "source": [
        "#cfg = load_config_data(\"/content/drive/My Drive/l5kit/examples/agent_motion_prediction/agent_motion_config.yaml\")\n",
        "\n",
        "cfg = load_config_data('/content/drive/Shared drives/social-LSTM/l5kit/examples/agent_motion_prediction/agent_motion_config.yaml')\n",
        "cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "965wM1Ippnv4"
      },
      "source": [
        "# ===== INIT DATASET\n",
        "train_cfg = cfg[\"train_data_loader\"]\n",
        "\n",
        "# Rasterizer\n",
        "rasterizer = build_rasterizer(cfg, dm)\n",
        "\n",
        "# Train dataset/dataloader\n",
        "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
        "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              shuffle=train_cfg[\"shuffle\"],\n",
        "                              batch_size=train_cfg[\"batch_size\"],\n",
        "                              num_workers=4)#train_cfg[\"num_workers\"])\n",
        "\n",
        "\n",
        "tr_it = iter(train_dataloader)\n",
        "\n",
        "data = next(tr_it)\n",
        "\n",
        "batch_size, step_size, fea_size = data['history_positions'].shape\n",
        "# step_size = 20\n",
        "\n",
        "\n",
        "\n",
        "#Need to modify\n",
        "input_dim = fea_size\n",
        "hidden_dim = fea_size\n",
        "output_dim = fea_size\n",
        "predic_time_seq_len = 50\n",
        "encoder_h_dim = hidden_dim\n",
        "decoder_h_dim = hidden_dim\n",
        "print(train_dataset)\n",
        "print(step_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF7Dg3XlNpL4"
      },
      "source": [
        "print(fea_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPjLEUmgIaWU"
      },
      "source": [
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "20dBaG7Hpnv_"
      },
      "source": [
        "# ===== INIT  VAL DATASET\n",
        "val_cfg = cfg[\"val_data_loader\"]\n",
        "\n",
        "# Rasterizer\n",
        "rasterizer = build_rasterizer(cfg, dm)\n",
        "\n",
        "# Train dataset/dataloader\n",
        "val_zarr = ChunkedDataset(dm.require(val_cfg[\"key\"])).open()\n",
        "val_dataset = AgentDataset(cfg, val_zarr, rasterizer)\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                              shuffle=val_cfg[\"shuffle\"],\n",
        "                              batch_size=val_cfg[\"batch_size\"],\n",
        "                              num_workers=train_cfg[\"num_workers\"])\n",
        "\n",
        "print(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEboTSYMysCS"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train = 5\n",
        "pred = 1\n",
        "\n",
        "def data_handle_for_lstm(dir, train_seq_len= train, pred_seq_len=pred, frame_lenth_cap=train + pred):\n",
        "#     '''\n",
        "#     This function returns train sequences and prediction sequences for LSTM and RNN\n",
        "#     :param dir: directory where the dataset is present\n",
        "#     :param train_seq_len: length of the train sequence\n",
        "#     :param pred_seq_len: length of the prediction sequence\n",
        "#     :param frame_lenth_cap: minimum length of time presence the agent must have\n",
        "#     :return: train sequences and prediction sequences\n",
        "#     '''\n",
        "\n",
        "     data = np.load(dir)\n",
        "     d_IDs = np.unique(data[:, 4]).astype(int)\n",
        "\n",
        "     train_sequence = []\n",
        "     pred_sequence = []\n",
        "     sz = len(d_IDs)\n",
        "\n",
        "     mx = 0\n",
        "\n",
        "     agent_IDs = np.unique(data[:,1]).astype(int)\n",
        "\n",
        "     total_frames = int(np.amax(data[:, 0]))\n",
        "\n",
        "     # traversing through each agent from all agents corresponding to that dataset\n",
        "     for agent_id in agent_IDs:\n",
        "\n",
        "         # obtaining the trajectory of that individual agent\n",
        "         one_agent_traj = data[np.where(data[:, 1] == agent_id)]  # each_agent_id\n",
        "\n",
        "         # selecting the agent only if it is visible in at least 'frame_length_cap' number of frames\n",
        "         mx = max(mx, len(one_agent_traj[:, 0]))\n",
        "\n",
        "         if len(one_agent_traj[:, 0]) >= frame_lenth_cap:\n",
        "\n",
        "             # initializing an index range to obtain the train sequences and pred sequences\n",
        "             index_range = len(one_agent_traj[:, 0]) - train_seq_len - pred_seq_len\n",
        "\n",
        "             # obtaining the sequences\n",
        "             for idx in range(1, index_range + 2):\n",
        "                 train_sequence_dict = {}\n",
        "                 pred_sequence_dict = {}\n",
        "\n",
        "                 train_sequence_dict['agent_ID'] = agent_id\n",
        "\n",
        "                 # train_sequence_dict['sequence'] = one_agent_traj[idx:idx+train_seq_len,2:4]\n",
        "                 #Get its (x, y) under the time range\n",
        "                 train_sequence_dict['sequence'] = one_agent_traj[idx - 1:idx - 1 + train_seq_len, 2:4]\n",
        "\n",
        "                 pred_sequence_dict['agent_ID'] = agent_id\n",
        "\n",
        "                 # pred_sequence_dict['sequence'] = one_agent_traj[idx+train_seq_len:idx+train_seq_len+pred_seq_len,2:4]\n",
        "                 #This is for test the accuracy\n",
        "                 pred_sequence_dict['sequence'] = one_agent_traj[\n",
        "                                                  idx - 1 + train_seq_len:idx - 1 + train_seq_len + pred_seq_len,\n",
        "                                                  2:4]\n",
        "\n",
        "                 train_sequence.append(train_sequence_dict)\n",
        "                 pred_sequence.append(pred_sequence_dict)\n",
        "\n",
        "     return train_sequence, pred_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWRlC4d1C-UM"
      },
      "source": [
        "# **Pre-Process data**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnhJQXAjC9o3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwoloqfwHcgA"
      },
      "source": [
        "# **Encoder LSTM**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "U4A1WEYGpnwF"
      },
      "source": [
        "class Encoder ( nn.Module ):\n",
        "    def __init__ ( self , input_size , cell_size , hidden_size ):\n",
        "        \"\"\"\n",
        "        cell_size is the size of cell_state.\n",
        "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
        "        \"\"\"\n",
        "        super ( Encoder , self ).__init__ ()\n",
        "\n",
        "        self.cell_size = cell_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fl = nn.Linear ( input_size + hidden_size , hidden_size )\n",
        "        self.il = nn.Linear ( input_size + hidden_size , hidden_size )\n",
        "        self.ol = nn.Linear ( input_size + hidden_size , hidden_size )\n",
        "        self.Cl = nn.Linear ( input_size + hidden_size , hidden_size )\n",
        "\n",
        "    #Here x1, y1 are the nodes with (x,y)\n",
        "    def computeDist ( self , x1 , y1 ):\n",
        "        return np.abs ( x1 - y1 )\n",
        "\n",
        "    #This is what we can use for social LSTM later (haven't defined)\n",
        "    def computeKNN ( self , curr_dict , ID , k ):\n",
        "        import heapq\n",
        "        from operator import itemgetter\n",
        "\n",
        "        ID_x = curr_dict[ ID ]\n",
        "        dists = {}\n",
        "        for j in range ( len ( curr_dict ) ):\n",
        "            if j != ID:\n",
        "                dists[ j ] = self.computeDist ( ID_x , curr_dict[ j ] )\n",
        "        KNN_IDs = dict ( heapq.nsmallest ( k , dists.items () , key=itemgetter ( 1 ) ) )\n",
        "        neighbors = list ( KNN_IDs.keys () )\n",
        "\n",
        "        return neighbors\n",
        "        # return [1,2,3]\n",
        "\n",
        "    def compute_A ( self , xt ):\n",
        "        # return Variable(torch.Tensor(np.ones([xt.shape[0],xt.shape[0]])).cuda())\n",
        "        xt = xt.cpu ().detach ().numpy ()\n",
        "        A = np.zeros ( [ xt.shape[ 0 ] , xt.shape[ 0 ] ] )\n",
        "        for i in range ( len ( xt ) ):\n",
        "            #if xt[ i ] is not None:\n",
        "            if xt[i][0] and xt[i][1] :\n",
        "                neighbors = self.computeKNN ( xt , i , 4 )\n",
        "                for neighbor in neighbors:\n",
        "                    # if neighbor in labels:\n",
        "                    # if idx < labels.index ( neighbor ):\n",
        "                    A[ i ][ neighbor ] = 1\n",
        "        return Variable ( torch.Tensor ( A ).cuda () )\n",
        "\n",
        "\n",
        "    def forward ( self , input , Hidden_State , Cell_State ):\n",
        "\n",
        "        combined = torch.cat ( (input , Hidden_State) , 1 )\n",
        "        f = torch.sigmoid ( self.fl ( combined ) )\n",
        "        i = torch.sigmoid ( self.il ( combined ) )\n",
        "        o = torch.sigmoid ( self.ol ( combined ) )\n",
        "        g = torch.tanh ( self.Cl ( combined ) )\n",
        "        Cell_State = f * Cell_State + i * g\n",
        "        Hidden_State = o * torch.tanh ( Cell_State )\n",
        "\n",
        "        return Hidden_State , Cell_State\n",
        "\n",
        "    def loop ( self , inputs ):\n",
        "        batch_size = inputs.size ( 0 )\n",
        "        time_step = inputs.size ( 1 )\n",
        "        Hidden_State , Cell_State = self.initHidden ( batch_size )\n",
        "        for i in range ( time_step ):\n",
        "            Hidden_State , Cell_State = self.forward(torch.squeeze(inputs[:, i:i+1,:]), Hidden_State, Cell_State)\n",
        "        return Hidden_State , Cell_State\n",
        "\n",
        "    def initHidden ( self , batch_size ):\n",
        "        Hidden_State = Variable ( torch.zeros ( batch_size , self.hidden_size ).to ( device ) )\n",
        "        Cell_State = Variable ( torch.zeros ( batch_size , self.hidden_size ).to ( device ) )\n",
        "        return Hidden_State , Cell_State\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM350fVMvzRy"
      },
      "source": [
        "# **Revised Encoder part(Social-Pooling Layer)**\n",
        "\n",
        "---\n",
        "\n",
        "Here I want my input will be 1) hidden_state output for encoder, 2) time_neighbor matrix (identity matrix) 3) coordination  to create the model\n",
        "\n",
        "Here input at following part is dataframe for whole agents: [agent 1, agent 2, ... agent_n]\n",
        "\n",
        "I assume that you each time we only have one agent to feed so that here it is not the matrix, it should be vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phb5dDdHv6BC"
      },
      "source": [
        "def make_mlp(dim_list, activation='relu', batch_norm=False, dropout=0):\n",
        "    layers = []\n",
        "    for dim_in, dim_out in zip(dim_list[:-1], dim_list[1:]):\n",
        "        layers.append(nn.Linear(dim_in, dim_out))\n",
        "        if batch_norm:\n",
        "            layers.append(nn.BatchNorm1d(dim_out))\n",
        "        if activation == 'relu':\n",
        "            layers.append(nn.ReLU())\n",
        "        elif activation == 'leakyrelu':\n",
        "            layers.append(nn.LeakyReLU())\n",
        "        if dropout > 0:\n",
        "            layers.append(nn.Dropout(p=dropout))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class SocialPooling(nn.Module):\n",
        "    def __init__(\n",
        "        self, h_dim, activation='relu', batch_norm=True, dropout=0.0,\n",
        "        neighborhood_size=2.0, grid_size=8, pool_dim=None\n",
        "    ):\n",
        "        super(SocialPooling, self).__init__()\n",
        "        self.h_dim = h_dim\n",
        "        self.grid_size = grid_size\n",
        "        self.neighborhood_size = neighborhood_size\n",
        "        if pool_dim:\n",
        "            mlp_pool_dims = [grid_size * grid_size * h_dim, pool_dim]\n",
        "        else:\n",
        "            mlp_pool_dims = [grid_size * grid_size * h_dim, h_dim]\n",
        "\n",
        "        self.mlp_pool = make_mlp(\n",
        "            mlp_pool_dims,\n",
        "            activation=activation,\n",
        "            batch_norm=batch_norm,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "    def get_bounds(self, ped_pos):\n",
        "        top_left_x = ped_pos[:, 0] - self.neighborhood_size / 2\n",
        "        top_left_y = ped_pos[:, 1] + self.neighborhood_size / 2\n",
        "        bottom_right_x = ped_pos[:, 0] + self.neighborhood_size / 2\n",
        "        bottom_right_y = ped_pos[:, 1] - self.neighborhood_size / 2\n",
        "        top_left = torch.stack([top_left_x, top_left_y], dim=1)\n",
        "        bottom_right = torch.stack([bottom_right_x, bottom_right_y], dim=1)\n",
        "        return top_left, bottom_right\n",
        "\n",
        "    def get_grid_locations(self, top_left, other_pos):\n",
        "        cell_x = torch.floor(\n",
        "            ((other_pos[:, 0] - top_left[:, 0]) / self.neighborhood_size) *\n",
        "            self.grid_size)\n",
        "        cell_y = torch.floor(\n",
        "            ((top_left[:, 1] - other_pos[:, 1]) / self.neighborhood_size) *\n",
        "            self.grid_size)\n",
        "        grid_pos = cell_x + cell_y * self.grid_size\n",
        "        return grid_pos\n",
        "\n",
        "    def repeat(self, tensor, num_reps):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        -tensor: 2D tensor of any shape\n",
        "        -num_reps: Number of times to repeat each row\n",
        "        Outpus:\n",
        "        -repeat_tensor: Repeat each row such that: R1, R1, R2, R2\n",
        "        \"\"\"\n",
        "        col_len = tensor.size(1)\n",
        "        #print(col_len)\n",
        "        \n",
        "        tensor = tensor.unsqueeze(dim=1).repeat(1, num_reps, 1)\n",
        "\n",
        "        #print('starrrrrrrrr')\n",
        "        \n",
        "        \n",
        "        tensor = tensor.view(-1, col_len)\n",
        "        \n",
        "        #print(tensor.shape)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def forward(self, h_states, seq_start_end, end_pos):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - h_states: Tesnsor of shape (batch, h_dim)\n",
        "        - seq_start_end: A list of tuples which delimit sequences within batch.\n",
        "        - end_pos: Absolute end position of obs_traj (batch, 2)\n",
        "        Output:\n",
        "        - pool_h: Tensor of shape (batch, h_dim)\n",
        "        \"\"\"\n",
        "        pool_h = []\n",
        "        for _, x in enumerate(seq_start_end):\n",
        "            \n",
        "            start = x[0][0].item()\n",
        "            end = x[0][1].item()\n",
        "            num_ped = end - start\n",
        "            grid_size = self.grid_size * self.grid_size\n",
        "\n",
        "            curr_hidden = h_states.view(-1, self.h_dim)[start:end]\n",
        "\n",
        "            #print(curr_hidden.shape)\n",
        "\n",
        "\n",
        "            curr_hidden_repeat = curr_hidden.repeat(num_ped, 1)\n",
        "\n",
        "            #print(\"this is the output for curr_hidden_repeat\")\n",
        "            #print(curr_hidden_repeat.shape)\n",
        "\n",
        "            curr_hidden_repeat = curr_hidden_repeat.to(device = device)\n",
        "\n",
        "\n",
        "            curr_end_pos = end_pos[start:end]\n",
        "\n",
        "            curr_pool_h_size = (num_ped * grid_size) + 1\n",
        "            curr_pool_h = curr_hidden.new_zeros((curr_pool_h_size, self.h_dim))\n",
        "            \n",
        "            curr_pool_h = curr_pool_h.to(device = device)\n",
        "            \n",
        "            # curr_end_pos = curr_end_pos.data\n",
        "            top_left, bottom_right = self.get_bounds(curr_end_pos)\n",
        "\n",
        "            # Repeat position -> P1, P2, P1, P2\n",
        "            \n",
        "            #print('start curr_end_pos')\n",
        "            \n",
        "            \n",
        "            curr_end_pos = curr_end_pos.repeat(num_ped, 1)\n",
        "\n",
        "            #print(curr_end_pos.shape)\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "            # Repeat bounds -> B1, B1, B2, B2\n",
        "            top_left = self.repeat(top_left, num_ped)\n",
        "            bottom_right = self.repeat(bottom_right, num_ped)\n",
        "\n",
        "            grid_pos = self.get_grid_locations(\n",
        "                    top_left, curr_end_pos).type_as(seq_start_end)\n",
        "            # Make all positions to exclude as non-zero\n",
        "            # Find which peds to exclude\n",
        "            x_bound = ((curr_end_pos[:, 0] >= bottom_right[:, 0]) +\n",
        "                       (curr_end_pos[:, 0] <= top_left[:, 0]))\n",
        "            y_bound = ((curr_end_pos[:, 1] >= top_left[:, 1]) +\n",
        "                       (curr_end_pos[:, 1] <= bottom_right[:, 1]))\n",
        "\n",
        "            within_bound = x_bound + y_bound\n",
        "            within_bound[0::num_ped + 1] = 1  # Don't include the ped itself\n",
        "            within_bound = within_bound.view(-1)\n",
        "\n",
        "            # This is a tricky way to get scatter add to work. Helps me avoid a\n",
        "            # for loop. Offset everything by 1. Use the initial 0 position to\n",
        "            # dump all uncessary adds.\n",
        "            grid_pos += 1\n",
        "            total_grid_size = self.grid_size * self.grid_size\n",
        "            offset = torch.arange(\n",
        "                0, total_grid_size * num_ped, total_grid_size\n",
        "            ).type_as(seq_start_end)\n",
        "\n",
        "            offset = self.repeat(offset.view(-1, 1), num_ped).view(-1)\n",
        "\n",
        "            #print(offset.shape)\n",
        "            #print(grid_pos.shape)\n",
        "\n",
        "\n",
        "            #grid_pos += offset\n",
        "            grid_pos[within_bound != 0] = 0\n",
        "\n",
        "            curr_hidden_repeat = curr_hidden_repeat.to(device)\n",
        "            grid_pos = grid_pos.to(device)\n",
        "            curr_pool_h = curr_pool_h.to(device)\n",
        "\n",
        "            grid_pos = grid_pos.view(-1, 1).expand_as(curr_hidden_repeat)\n",
        "\n",
        "            curr_pool_h = curr_pool_h.scatter_add(0, grid_pos,\n",
        "                                                  curr_hidden_repeat)\n",
        "            curr_pool_h = curr_pool_h[1:]\n",
        "            curr_pool_h = curr_pool_h.to(device = device)\n",
        "            pool_h.append(curr_pool_h.view(num_ped, -1))\n",
        "\n",
        "        pool_h = torch.cat(pool_h, dim=0)\n",
        "        pool_h = self.mlp_pool(pool_h)\n",
        "\n",
        "        \n",
        "        pool_h = pool_h.narrow(0, 0, h_states.shape[0])\n",
        "\n",
        "        #print('this is the final pool_h size')\n",
        "        #print(pool_h.shape)\n",
        "\n",
        "\n",
        "        pool_h = pool_h.to(device)\n",
        "        return pool_h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6QovcKAHX28"
      },
      "source": [
        "# **Decoder LSTM**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1GDCq3tHRZ4"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    # def __init__(self, cfg):\n",
        "    def __init__(self, input_size, cell_size, hidden_size, batchsize, timestep):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.cell_size = cell_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batchsize\n",
        "        self.time_step = timestep\n",
        "        self.num_mog_params = 5\n",
        "        self.sampled_point_size = 2\n",
        "        #self.stream = stream\n",
        "        self.stream_specific_param = self.num_mog_params\n",
        "        self.fl = nn.Linear(self.stream_specific_param + hidden_size, hidden_size)\n",
        "        self.il = nn.Linear(self.stream_specific_param + hidden_size, hidden_size)\n",
        "        self.ol = nn.Linear(self.stream_specific_param + hidden_size, hidden_size)\n",
        "        self.Cl = nn.Linear(self.stream_specific_param + hidden_size, hidden_size)\n",
        "        self.linear1 = nn.Linear(cell_size, self.stream_specific_param)\n",
        "        # self.one_lstm = nn.LSTMCell\n",
        "        # self.linear2 = nn.Linear ( self.sampled_point_size ,  hidden_size )\n",
        "\n",
        "        #print(\"Hidden_size\")\n",
        "        #print(self.hidden_size)\n",
        "        #print(\"stream_spefic_param\")\n",
        "        #print(self.stream_specific_param)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input, Hidden_State, Cell_State):\n",
        "\n",
        "        #print('input~')\n",
        "        #print(input.shape)\n",
        "\n",
        "        combined = torch.cat((input, Hidden_State), 1)\n",
        "\n",
        "        #print(\"Input shape\")\n",
        "        #print(input.shape)\n",
        "        #print(\"Hidden_State shape\")\n",
        "        #print(Hidden_State.shape)\n",
        "        #print(\"Combined shape\")\n",
        "        #print(combined.shape)\n",
        "\n",
        "        \n",
        "\n",
        "        f = torch.sigmoid(self.fl(combined))\n",
        "        i = torch.sigmoid(self.il(combined))\n",
        "        o = torch.sigmoid(self.ol(combined))\n",
        "        g = torch.tanh(self.Cl(combined))\n",
        "        Cell_State = f * Cell_State + i * g\n",
        "        Hidden_State = o * torch.tanh(Cell_State)\n",
        "\n",
        "        return Hidden_State, Cell_State\n",
        "\n",
        "    def loop(self, hidden_vec_from_encoder):\n",
        "        batch_size = self.batch_size\n",
        "        time_step = self.time_step\n",
        "\n",
        "        init_hidden_output = self.initHidden()\n",
        "\n",
        "        Cell_State, out = init_hidden_output[0], init_hidden_output[1]\n",
        "\n",
        "        mu1_all , mu2_all , sigma1_all , sigma2_all , rho_all = self.initMogParams()\n",
        "\n",
        "        for i in range(time_step):\n",
        "            if i == 0:\n",
        "                Hidden_State = hidden_vec_from_encoder\n",
        "            Hidden_State, Cell_State = self.forward(out, Hidden_State, Cell_State)\n",
        "\n",
        "            mog_params = self.linear1(Hidden_State)\n",
        "\n",
        "            out = mog_params\n",
        "\n",
        "            mu_1 , mu_2 , log_sigma_1 , log_sigma_2 , pre_rho = mog_params.chunk ( 6 , dim=-1 )\n",
        "            rho = torch.tanh ( pre_rho )\n",
        "            log_sigma_1 = torch.exp(log_sigma_1)\n",
        "            log_sigma_2 = torch.exp(log_sigma_2)\n",
        "            mu1_all[:,i,:] = mu_1\n",
        "            mu2_all[:,i,:] = mu_2\n",
        "            sigma1_all[:,i,:] = log_sigma_1\n",
        "            sigma2_all[:,i,:] = log_sigma_2\n",
        "            rho_all[:,i,:] = rho\n",
        "\n",
        "        Stream_output  = torch.cat((mu1_all,mu2_all), dim=2)\n",
        "\n",
        "        return Stream_output, Cell_State\n",
        "\n",
        "\n",
        "    def initHidden(self):\n",
        "        out = torch.randn(self.batch_size, self.num_mog_params, device=device)\n",
        "        \n",
        "        return torch.randn(self.batch_size, self.hidden_size, device=device), out\n",
        "    \n",
        "\n",
        "    #Here i want to store (x,y) for the future predictions\n",
        "    def initMogParams(self):\n",
        "        mu1_all = torch.randn(self.batch_size, self.time_step, 1, device=device)\n",
        "        mu2_all = torch.randn(self.batch_size, self.time_step, 1, device=device)\n",
        "        sigma1_all = torch.randn(self.batch_size, self.time_step, 1, device=device)\n",
        "        sigma2_all = torch.randn(self.batch_size, self.time_step, 1, device=device)\n",
        "        rho_all = torch.randn(self.batch_size, self.time_step, 1, device=device)\n",
        "\n",
        "        return mu1_all, mu2_all, sigma1_all, sigma2_all, rho_all    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7tj8MfeTI2p"
      },
      "source": [
        "# **Original LSTM**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "J_e7qWSnpnwM"
      },
      "source": [
        "# ==== INIT MODEL\n",
        "learning_rate = 1e-3\n",
        "\n",
        "encoder_stream = Encoder(input_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "decoder_stream = Decoder(input_dim, hidden_dim, output_dim, batch_size, predic_time_seq_len).to(device)\n",
        "\n",
        "\n",
        "encoder_stream_optimizer = optim.RMSprop(encoder_stream.parameters(), lr=learning_rate)\n",
        "decoder_stream_optimizer = optim.RMSprop(decoder_stream.parameters(), lr=learning_rate)\n",
        "\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=7000,gamma=0.1)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oMfu67hmRpK"
      },
      "source": [
        "def train_stream(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer):\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    Hidden_State, _ = encoder.loop(input_tensor)\n",
        "\n",
        "\n",
        "    #print(Hidden_State.shape)\n",
        "\n",
        "    stream_out, _ = decoder.loop(Hidden_State)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    #print(\"This is the beginning!!!!\")\n",
        "    #print(target_tensor.shape)\n",
        "\n",
        "    #print(\"This is the output for the stream we want to predict\")\n",
        "    #print(stream_out.shape)\n",
        "\n",
        "    loss = criterion(target_tensor, stream_out)\n",
        "\n",
        "    #loss = loss if loss > 0 else -1 * loss\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GVgInipCpnwR"
      },
      "source": [
        "import pdb\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# ==== TRAIN LOOP\n",
        "tr_it = iter(train_dataloader)\n",
        "vl_it = iter(val_dataloader)\n",
        "\n",
        "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
        "losses_train = []\n",
        "losses_mean_train = []\n",
        "losses_val = []\n",
        "losses_mean_val = []\n",
        "\n",
        "for itr in progress_bar:\n",
        "    try:\n",
        "        data = next(tr_it)\n",
        "    except StopIteration:\n",
        "        tr_it = iter(train_dataloader)\n",
        "        data = next(tr_it)\n",
        "    \n",
        "    #You need to train two parts\n",
        "    encoder_stream.train()\n",
        "    \n",
        "    decoder_stream.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    # Forward pass\n",
        "    history_positions = data['history_positions'].to(device)\n",
        "    history_availabilities = data['history_availabilities'].to(device)\n",
        "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
        "    targets_position = data[\"target_positions\"].to(device)\n",
        "    \n",
        "    # data_handle_for_lstm(dir, train_seq_len= train, pred_seq_len=pred, frame_lenth_cap=train + pred)\n",
        "\n",
        "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
        "\n",
        "    #print(history_positions.shape)\n",
        "    #print(targets_position.shape)\n",
        "\n",
        "    #print(history_positions)\n",
        "    #print(targets_position)\n",
        "\n",
        "\n",
        "    loss = train_stream(history_positions, targets_position, encoder_stream, decoder_stream, encoder_stream_optimizer, decoder_stream_optimizer)\n",
        "    # loss = train_stream(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer)\n",
        "    loss = loss.mean()\n",
        "\n",
        "    print(loss.item())\n",
        "    \n",
        "    losses_train.append(loss.item())\n",
        "    losses_mean_train.append(np.mean(losses_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAK6w_ZGvNjh"
      },
      "source": [
        "print(losses_mean_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1ZW1PE4KNus"
      },
      "source": [
        "# **Create a Class about all social parts**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKr7ri2GKOOI"
      },
      "source": [
        "class SocialLSTM(nn.Module):\n",
        "  def __init__(self, input_size, output_size, predic_tim_seq_len, encoder_h_dim, decoder_h_dim, mlp_dim = 1024, \n",
        "                dropout = 0.0, activation = 'relu', batch_norm = True, neighborhood_size = 2.0, grid_size = 8, bottleneck_dim=1024):\n",
        "    super(SocialLSTM, self).__init__()\n",
        "\n",
        "    #self.input_size = input_size\n",
        "    #self.obs_len = obs_len\n",
        "    self.pred_len = predic_time_seq_len\n",
        "    #self.seq_len = obs_len + pred_len\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.seq_start_end = seq_start_end\n",
        "    \n",
        "    self.decoder_input = input_size\n",
        "\n",
        "    self.encoder = Encoder(input_size, encoder_h_dim, output_size)\n",
        "    self.decoder = Decoder(input_size, decoder_h_dim, output_size, batch_size, predic_time_seq_len)\n",
        "    self.pool_net = SocialPooling(h_dim = encoder_h_dim, activation  = activation, batch_norm = batch_norm, dropout = dropout, neighborhood_size=neighborhood_size,grid_size = grid_size)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    #Decoder hidden:\n",
        "    #input_dim = encoder_h_dim + bottleneck_dim\n",
        "\n",
        "    mlp_decoder_context_dims = [input_dim, mlp_dim, decoder_h_dim]\n",
        "\n",
        "    self.mlp_decoder_context = make_mlp(\n",
        "                mlp_decoder_context_dims,\n",
        "                activation=activation,\n",
        "                batch_norm=batch_norm,\n",
        "                dropout=dropout\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input_tensor, obs_traj, seq_start_end):\n",
        "    #Here I want to know the last one end_pos\n",
        "    #I assume that obs_traj: Tensor of shape (obs_len, batch, 2)\n",
        "\n",
        "\n",
        "    encoder_hidden_state, _  = self.encoder.loop(input_tensor)\n",
        "\n",
        "    end_pos = obs_traj[:,-1,:]\n",
        "\n",
        "    pool_h = self.pool_net(encoder_hidden_state, seq_start_end, end_pos)\n",
        "\n",
        "    #print(pool_h.shape)\n",
        "    #print(encoder_hidden_state.shape)\n",
        "    \n",
        "    x = pool_h + encoder_hidden_state\n",
        "    #print(x.shape)\n",
        "\n",
        "\n",
        "    mlp_decoder_context_input = torch.cat([encoder_hidden_state.view(-1, encoder_h_dim), pool_h], dim=1)\n",
        "\n",
        "    #decoder_hidden = torch.unsqueeze(mlp_decoder_context_input, 0)\n",
        "\n",
        "    #Original one\n",
        "    #decoder_hidden = mlp_decoder_context_input\n",
        "\n",
        "    decoder_hidden = x\n",
        "\n",
        "    #print('decoder_hidden')\n",
        "    #print(decoder_hidden.shape)\n",
        "    #print('mlp')\n",
        "    #print(mlp_decoder_context_input.shape)\n",
        "\n",
        "    stream_out, _ = self.decoder.loop(decoder_hidden)\n",
        "\n",
        "\n",
        "\n",
        "    return stream_out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLRck5Lu-YfN"
      },
      "source": [
        "\n",
        "\n",
        "# **Prepare the data for the start and end**\n",
        "\n",
        "---\n",
        "\n",
        "Here I want to return a dataframe \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mENo3Cc7-pTG"
      },
      "source": [
        "def parse_scene(scene):\n",
        "    scene_dict = {\n",
        "            \"frame_index_interval_start\": scene[0][0],\n",
        "            \"frame_index_interval_end\": scene[0][1],\n",
        "            \"host\":  scene[1],\n",
        "            \"start_time\": scene[2],\n",
        "            \"end_time\": scene[3]\n",
        "        }\n",
        "    return scene_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkR23sp6_GUn"
      },
      "source": [
        "DATA_ROOT = Path(\"/content/drive\")\n",
        "\n",
        "class BaseParser:\n",
        "    \"\"\"\n",
        "    A robust and fast interface to load l5kit data into  Pandas dataframes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    chunk_size: int, default=1000\n",
        "        How many items do you want in a single slice. The larger the better;\n",
        "        as long as you have enough memory. Nevertheless, chunk sizes above `10_000` won't lead to\n",
        "        significant speed gain as the original zarr files was chunked at 10_000.\n",
        "\n",
        "    max_chunks: int, default=10\n",
        "        How many chunks do you want to read from memory.\n",
        "\n",
        "    root:\n",
        "        Zarr data root path\n",
        "\n",
        "    zarr_path:\n",
        "        relative path or key to the data.\n",
        "    \"\"\"\n",
        "    \n",
        "    field = \"scenes\"\n",
        "    dtypes = {}\n",
        "    \n",
        "    def __init__(self, start=0, end=None, chunk_size=1000, max_chunks=10, root=DATA_ROOT, zarr_path=\"Shared drives/social-LSTM/l5kit/scenes/sample.zarr\"):\n",
        "        \n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.chunk_size = chunk_size\n",
        "        self.max_chunks = max_chunks\n",
        "        \n",
        "\n",
        "        self.root = Path(root)\n",
        "        assert self.root.exists(), \"There is nothing at {}!\".format(self.root)\n",
        "        self.zarr_path = Path(zarr_path)\n",
        "        \n",
        "     \n",
        "    def parse(self):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def to_pandas(self, start=0, end=None, chunk_size=None, max_chunks=None):\n",
        "        start = start or self.start\n",
        "        end = end or self.end\n",
        "        chunk_size = chunk_size or self.chunk_size\n",
        "        max_chunks = max_chunks or self.max_chunks\n",
        "        \n",
        "        if not chunk_size or  not max_chunks: # One shot load, suitable for small zarr files\n",
        "            df = zarr.load(self.root.joinpath(self.zarr_path).as_posix()).get(self.field)\n",
        "            df = df[start:end]\n",
        "            df = map(self.parse, df) \n",
        "        else: # Chunked load, suitable for large zarr files\n",
        "            df = []\n",
        "            with zarr.open(self.root.joinpath(self.zarr_path).as_posix(), \"r\") as zf:\n",
        "                end = start+max_chunks*chunk_size if end is None else min(end, start+max_chunks*chunk_size)\n",
        "                for i_start in range(start, end, chunk_size ):\n",
        "                    items = zf[self.field][i_start: min(i_start + chunk_size,end)]\n",
        "                    items = map(self.parse, items)\n",
        "                    df.append(items)\n",
        "            df = it.chain(*df)\n",
        "            \n",
        "        df = pd.DataFrame.from_records(df)\n",
        "        for col, col_dtype in self.dtypes.items():\n",
        "            df[col] = df[col].astype(col_dtype, copy=False)\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpHEaIK2_Q7q"
      },
      "source": [
        "class SceneParser(BaseParser):\n",
        "    field = \"scenes\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def parse(scene):\n",
        "        scene_dict = {\n",
        "            \"frame_index_interval_start\": scene[0][0],\n",
        "            \"frame_index_interval_end\": scene[0][1],\n",
        "            \"host\":  scene[1],\n",
        "            \"start_time\": scene[2],\n",
        "            \"end_time\": scene[3]\n",
        "        }\n",
        "        return scene_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY2eFXkE-ZIu"
      },
      "source": [
        "def parse_frame(frame):\n",
        "    frame_dict = {\n",
        "        'timestamp': frame[0],\n",
        "        'agent_index_interval_start': frame[1][0],\n",
        "        'agent_index_interval_start': frame[1][1],\n",
        "        'traffic_light_faces_index_interval_start': frame[2][0],\n",
        "        'traffic_light_faces_index_interval_end': frame[2][1],\n",
        "        'ego_translation_x': frame[3][0],\n",
        "        'ego_translation_y': frame[3][1],\n",
        "        'ego_translation_z': frame[3][2],\n",
        "        'ego_rotation_xx': frame[4][0][0],\n",
        "        'ego_rotation_xy': frame[4][0][1],\n",
        "        'ego_rotation_xz': frame[4][0][2],\n",
        "        'ego_rotation_yx': frame[4][1][0],\n",
        "        'ego_rotation_yy': frame[4][1][1],\n",
        "        'ego_rotation_yz': frame[4][1][2],\n",
        "        'ego_rotation_zx': frame[4][2][0],\n",
        "        'ego_rotation_zy': frame[4][2][1],\n",
        "        'ego_rotation_zz': frame[4][2][2],\n",
        "        \n",
        "    }\n",
        "    return frame_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hD5f7s6-1C-"
      },
      "source": [
        "class FrameParser(BaseParser):\n",
        "    field = \"frames\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def parse(frame):\n",
        "        frame_dict = {\n",
        "            'timestamp': frame[0],\n",
        "            'agent_index_interval_start': frame[1][0],\n",
        "            'agent_index_interval_end': frame[1][1],\n",
        "            'traffic_light_faces_index_interval_start': frame[2][0],\n",
        "            'traffic_light_faces_index_interval_end': frame[2][1],\n",
        "            'ego_translation_x': frame[3][0],\n",
        "            'ego_translation_y': frame[3][1],\n",
        "            'ego_translation_z': frame[3][2],\n",
        "            'ego_rotation_xx': frame[4][0][0],\n",
        "            'ego_rotation_xy': frame[4][0][1],\n",
        "            'ego_rotation_xz': frame[4][0][2],\n",
        "            'ego_rotation_yx': frame[4][1][0],\n",
        "            'ego_rotation_yy': frame[4][1][1],\n",
        "            'ego_rotation_yz': frame[4][1][2],\n",
        "            'ego_rotation_zx': frame[4][2][0],\n",
        "            'ego_rotation_zy': frame[4][2][1],\n",
        "            'ego_rotation_zz': frame[4][2][2],\n",
        "\n",
        "        }\n",
        "        return frame_dict\n",
        "\n",
        "    def to_pandas(self, start=0, end=None, chunk_size=None, max_chunks=None, scene=None):\n",
        "        if scene is not None:\n",
        "            start = scene.frame_index_interval_start\n",
        "            end = scene.frame_index_interval_end\n",
        "        \n",
        "        df = super().to_pandas(start=start, end=end, chunk_size=chunk_size, max_chunks=max_chunks)\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQz6rARp_zJc"
      },
      "source": [
        "# **Input the data to generate the data**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyY0J3wc_4hF"
      },
      "source": [
        "import zarr\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "import itertools as it # I will be using the `itertools.chain` function\n",
        "from pathlib import Path # for better file/path operations management\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive\")\n",
        "\n",
        "zl5 = zarr.open(DATA_ROOT.joinpath(\"Shared drives/social-LSTM/l5kit/scenes/sample.zarr\").as_posix(), mode=\"r\")\n",
        "\n",
        "sp = SceneParser(chunk_size=None, max_chunks=None, zarr_path=\"/content/drive/Shared drives/social-LSTM/l5kit/scenes/sample.zarr\")\n",
        "\n",
        "scenes = sp.to_pandas()\n",
        "\n",
        "row_scene, col_scene = scenes.shape\n",
        "\n",
        "all_information = pd.DataFrame()\n",
        "\n",
        "\n",
        "for i in range(row_scene):\n",
        "  scene = scenes.iloc[i]\n",
        "  scene_frames = zl5.frames[scene.frame_index_interval_start:scene.frame_index_interval_end]\n",
        "  \n",
        "  fp = FrameParser()\n",
        "  frames = fp.to_pandas(scene = scene)\n",
        "\n",
        "  frames_needed = frames.iloc[:,0:3]\n",
        "\n",
        "  if i == 0:\n",
        "    all_information = frames_needed\n",
        "  else:\n",
        "    all_information = all_information.append(frames_needed, ignore_index=True)\n",
        "\n",
        "all_information['num_agent'] = all_information['agent_index_interval_end'] - all_information['agent_index_interval_start']\n",
        "\n",
        "#xx = all_information.loc[all_information['timestamp'] ==1572643684801892606]\n",
        "\n",
        "#print(xx.iloc[0]['num_agent'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX7Q8-ToBHuM"
      },
      "source": [
        "zl5.scenes.info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qdRamXCCTkx"
      },
      "source": [
        "print(scenes.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ9MAMtyKNTi"
      },
      "source": [
        "# **Overall revised code**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24IMPfDAUs05"
      },
      "source": [
        "# ==== INIT MODEL\n",
        "learning_rate = 1e-3\n",
        "\n",
        "seq_start_end = torch.LongTensor()\n",
        "\n",
        "slstm = SocialLSTM( input_size = input_dim, output_size= output_dim, predic_tim_seq_len= predic_time_seq_len, encoder_h_dim= hidden_dim, decoder_h_dim=decoder_h_dim).to(device)\n",
        "\n",
        "slstm_optimizer = optim.RMSprop(slstm.parameters(), lr = 0.0001)\n",
        "\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=7000,gamma=0.1)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMQIGrufNKCn"
      },
      "source": [
        "f = open(\"Social_LSTM_avg_res.csv\", \"r\")\n",
        "print(f.read()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1b-z-w1Uh-w"
      },
      "source": [
        "import pdb\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# ==== TRAIN LOOP\n",
        "tr_it = iter(train_dataloader)\n",
        "vl_it = iter(val_dataloader)\n",
        "\n",
        "progress_bar = tqdm(range(10000), position=0)#cfg[\"train_params\"][\"max_num_steps\"]))\n",
        "losses_train = []\n",
        "losses_mean_train = []\n",
        "losses_val = []\n",
        "losses_mean_val = []\n",
        "\n",
        "for i in progress_bar:\n",
        "    try:\n",
        "        data = next(tr_it)\n",
        "    except StopIteration:\n",
        "        tr_it = iter(train_dataloader)\n",
        "        data = next(tr_it)\n",
        "    \n",
        "    #You need to train two parts\n",
        "    slstm.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "\n",
        "    #print(data)\n",
        "\n",
        "    # Forward pass\n",
        "    history_positions = data['history_positions'].to(device)\n",
        "    history_availabilities = data['history_availabilities'].to(device)\n",
        "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
        "    targets_position = data[\"target_positions\"].to(device)\n",
        "    centroid = data['centroid'].to(device)\n",
        "\n",
        "    seq_start_end = list()\n",
        "\n",
        "    \n",
        "    for xx in (data['timestamp']):\n",
        "      #print(xx.item())\n",
        "      specific_row_dat = all_information.loc[all_information['timestamp'] ==xx.item()]\n",
        "      cum_start_idx = [0] + np.cumsum(specific_row_dat.iloc[0]['num_agent']).tolist()\n",
        "      \n",
        "      seq_start_end.append([(start, end) for start, end in zip(cum_start_idx, cum_start_idx[1:])])\n",
        "    \n",
        "    \n",
        "    seq_start_end = torch.LongTensor(seq_start_end)\n",
        "      \n",
        "    #print(seq_start_end)\n",
        "\n",
        " #   for _, x in enumerate(seq_start_end):\n",
        "#      print('sssss')\n",
        "#      print(x)\n",
        "      #print(x[0])\n",
        "      #print(x[0][0])\n",
        "#      start = x[0][0].item()\n",
        "#      print(start)\n",
        "#      end = x[0][1].item()\n",
        "#      print(end)\n",
        "      #start = x.item()\n",
        "      #end = end.item()\n",
        "      #print(start)\n",
        "      #print(end)\n",
        "      #start = x[0]\n",
        "      #end = x[1]\n",
        "      #print(start)\n",
        "      #print(end)\n",
        " #     print(x[0])\n",
        " #     print(x[0][0])\n",
        " #     print(x[0][1])\n",
        " #     print(type(x))\n",
        "\n",
        "\n",
        "\n",
        "    outputs = slstm(history_positions, history_positions, seq_start_end)\n",
        "\n",
        "    loss = criterion(outputs, targets_position)\n",
        "\n",
        "    loss = loss * target_availabilities\n",
        "    loss = loss.mean()\n",
        "\n",
        "    #Backward pass\n",
        "    slstm_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    slstm_optimizer.step()\n",
        "\n",
        "    losses_train.append(loss.item())\n",
        "    losses_mean_train.append(np.mean(losses_train))\n",
        "\n",
        "    if (i % 250) == 250 - 1:\n",
        "      with open('Social_LSTM_res2.csv','a') as fd:\n",
        "          for loss in losses_train:\n",
        "              fd.write(f\"{i},{loss}\\n\")\n",
        "      avg_loss = np.mean(losses_train)\n",
        "      losses_train = []\n",
        "      with open('Social_LSTM_avg_res2.csv','a') as fd:\n",
        "          fd.write(f\"{i},{avg_loss}\\n\")\n",
        "      progress_bar.set_description(f\"loss: {loss} loss(avg): {avg_loss}\")\n",
        "\n",
        "    # print(loss.item())\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PnF95YMUhkD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ns2Gv41ApnwW"
      },
      "source": [
        "nn# ===== INIT DATASET\n",
        "test_cfg = cfg[\"test_data_loader\"]\n",
        "\n",
        "# Rasterizer\n",
        "rasterizer = build_rasterizer(cfg, dm)\n",
        "\n",
        "# Test dataset/dataloader\n",
        "test_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\n",
        "test_mask = np.load(f\"/content/drive/Shared drives/social-LSTM/l5kit/scenes/mask.npz\")[\"arr_0\"]\n",
        "test_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                             shuffle=test_cfg[\"shuffle\"],\n",
        "                             batch_size=test_cfg[\"batch_size\"],\n",
        "                             num_workers=test_cfg[\"num_workers\"])\n",
        "\n",
        "\n",
        "print(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BH0mR540pnwc"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "future_coords_offsets_pd = []\n",
        "timestamps = []\n",
        "agent_ids = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    dataiter = tqdm(test_dataloader)\n",
        "    \n",
        "    for data in dataiter:\n",
        "\n",
        "        history_positions = data['history_positions'].to(device)\n",
        "\n",
        "        outputs = model(history_positions)\n",
        "        \n",
        "        future_coords_offsets_pd.append(outputs.cpu().numpy().copy())\n",
        "        timestamps.append(data[\"timestamp\"].numpy().copy())\n",
        "        agent_ids.append(data[\"track_id\"].numpy().copy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Jf-AphnVpnwi"
      },
      "source": [
        "write_pred_csv('submission.csv',\n",
        "               timestamps=np.concatenate(timestamps),\n",
        "               track_ids=np.concatenate(agent_ids),\n",
        "               coords=np.concatenate(future_coords_offsets_pd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ7umXYiChe_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}