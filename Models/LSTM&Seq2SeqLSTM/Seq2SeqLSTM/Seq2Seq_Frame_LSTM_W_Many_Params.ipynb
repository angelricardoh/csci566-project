{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.resnet import resnet18\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()        \n",
    "        self.lstm = nn.LSTM(1000+2+1, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out, hidden = self.lstm(data)\n",
    "\n",
    "        return out, hidden\n",
    "        \n",
    "def build_cnn(in_channels):\n",
    "    # load pre-trained Conv2D model\n",
    "    model = resnet18(pretrained=True)\n",
    "\n",
    "    # change input channels number to match the rasterizer's output\n",
    "    if input != 3:\n",
    "        model.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            model.conv1.out_channels,\n",
    "            kernel_size=model.conv1.kernel_size,\n",
    "            stride=model.conv1.stride,\n",
    "            padding=model.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    return model\n",
    "    \n",
    "class Seq2Seq(nn.Module):    \n",
    "    def __init__(self, hidden_size1, hidden_size2, hidden_size3, future_num_frames, seq_len, decoder_seq_len, num_layers=1, input_size=256):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.future_num_frames = future_num_frames\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = seq_len\n",
    "        self.decoder_seq_len = decoder_seq_len\n",
    "\n",
    "        \n",
    "        self.encoder_lstm = Encoder(hidden_size1, num_layers)\n",
    "        self.agent_cnn = build_cnn(2)\n",
    "        self.road_cnn = build_cnn(3)\n",
    "        \n",
    "        self.decoder = nn.LSTM(input_size, hidden_size1, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size1+1000+2+1+3, hidden_size2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size3, 2)\n",
    "        \n",
    "    def forward(self, data, device):\n",
    "        history_positions = torch.flip(data['history_positions'], [1]).to(device)\n",
    "        history_yaws = torch.flip(data['history_yaws'], [1]).to(device)\n",
    "        encoded_images, road_image = self.images_to_embeddings(data['image'].to(device))\n",
    "        \n",
    "        encoder_in = torch.cat([history_positions, history_yaws, encoded_images], dim=2)\n",
    "        \n",
    "        _, hidden = self.encoder_lstm(encoder_in)\n",
    "        \n",
    "        batch_size = history_positions.shape[0]\n",
    "        \n",
    "        out, _ = self.decoder(torch.ones(batch_size, self.decoder_seq_len, self.input_size).to(device), hidden)\n",
    "        \n",
    "        centroid = data['centroid'].type(torch.float32).to(device)\n",
    "        yaw = data['yaw'].reshape(-1, 1).type(torch.float32).to(device)\n",
    "        extent = data['extent'].type(torch.float32).to(device)\n",
    "        \n",
    "        extra_data = torch.cat([centroid, yaw, extent, road_image], dim=1)\n",
    "        extra_data = extra_data.reshape(batch_size, 1, -1).repeat(1, self.decoder_seq_len, 1)  \n",
    "\n",
    "        out = torch.cat([out, extra_data],dim=2)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out.view(batch_size, self.future_num_frames, 2)\n",
    "    \n",
    "    def images_to_embeddings(self,images):\n",
    "        seq_len = self.seq_len\n",
    "        batch_size = images.shape[0]\n",
    "        encoded_images = []\n",
    "        for i in range(seq_len):\n",
    "            ego_idx = (seq_len) + i\n",
    "            im = torch.cat([images[:,i:i+1,:,:], images[:,ego_idx:ego_idx+1,:,:]], dim=1)\n",
    "\n",
    "            out = self.agent_cnn(im).reshape(batch_size,1,-1)\n",
    "            encoded_images.append(out)\n",
    "        encoded_images.reverse()\n",
    "        return torch.cat(encoded_images, dim=1), self.road_cnn(images[:,-3:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"A:/CSCI 566 Project/\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = {'format_version': 4,\n",
    " 'model_params': {'history_num_frames': 10,\n",
    "  'history_step_size': 1,\n",
    "  'history_delta_time': 0.1,\n",
    "  'future_num_frames': 50,\n",
    "  'future_step_size': 1,\n",
    "  'future_delta_time': 0.1},\n",
    " 'raster_params': {'raster_size': [224, 224],\n",
    "  'pixel_size': [0.5, 0.5],\n",
    "  'ego_center': [0.5, 0.5],\n",
    "  'map_type': 'py_semantic',\n",
    "  'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "  'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "  'dataset_meta_key': 'meta.json',\n",
    "  'filter_agents_threshold': 0.5,\n",
    "  'disable_traffic_light_faces': False},\n",
    " 'train_data_loader': {'key': 'scenes/sample.zarr',\n",
    "  'batch_size': 4,\n",
    "  'shuffle': True,\n",
    "  'num_workers': 1},\n",
    " 'val_data_loader': {'key': 'scenes/sample.zarr',\n",
    "  'batch_size': 4,\n",
    "  'shuffle': False,\n",
    "  'num_workers': 1},\n",
    " 'test_data_loader': {'key': 'scenes/test.zarr',\n",
    "  'batch_size': 32,\n",
    "  'shuffle': False,\n",
    "  'num_workers': 1},\n",
    " 'train_params': {'checkpoint_every_n_steps': 500,\n",
    "  'max_num_steps': 100000,\n",
    "  'eval_every_n_steps': 250}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT DATASET\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle=train_cfg[\"shuffle\"],\n",
    "                              batch_size=train_cfg[\"batch_size\"])\n",
    "\n",
    "FUTURE_NUM_FRAMES = cfg['model_params']['future_num_frames']\n",
    "SEQ_LEN = cfg['model_params']['history_num_frames'] + 1\n",
    "\n",
    "\n",
    "\n",
    "# ===== INIT  VAL DATASET\n",
    "val_cfg = cfg[\"val_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "# Train dataset/dataloader\n",
    "val_zarr = ChunkedDataset(dm.require(val_cfg[\"key\"])).open()\n",
    "val_dataset = AgentDataset(cfg, val_zarr, rasterizer)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                              shuffle=val_cfg[\"shuffle\"],\n",
    "                              batch_size=val_cfg[\"batch_size\"])\n",
    "                              #num_workers=train_cfg[\"num_workers\"])\n",
    "    \n",
    "# print(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2Seq(1024, 1024, 256, FUTURE_NUM_FRAMES, SEQ_LEN, FUTURE_NUM_FRAMES).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 13.488561630249023 loss(avg): 51.60660457611084 loss_val(avg): 27.114134788513184:   0%| | 9/100000 [00:08<24:51:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-998e6c471d0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvl_it\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mvl_it\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\me\\anaconda3\\envs\\sdcar\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\me\\anaconda3\\envs\\sdcar\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\me\\anaconda3\\envs\\sdcar\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\me\\anaconda3\\envs\\sdcar\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\me\\anaconda3\\envs\\sdcar\\lib\\site-packages\\l5kit\\dataset\\agent.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mstate_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe_index\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscene_index\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscene_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_scene_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscene_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"AgentDataset\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\me\\anaconda3\\envs\\sdcar\\lib\\site-packages\\l5kit\\dataset\\ego.py\u001b[0m in \u001b[0;36mget_frame\u001b[1;34m(self, scene_index, state_index, track_id)\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             )\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtl_faces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;31m# 0,1,C -> C,0,1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\me\\anaconda3\\envs\\sdcar\\lib\\site-packages\\l5kit\\sampling\\agent_sampling.py\u001b[0m in \u001b[0;36mgenerate_agent_sample\u001b[1;34m(state_index, frames, agents, tl_faces, selected_track_id, render_context, history_num_frames, history_step_size, future_num_frames, future_step_size, filter_agents_threshold, rasterizer, perturbation)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrasterizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[1;32melse\u001b[0m \u001b[0mrasterizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrasterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_tl_faces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\me\\anaconda3\\envs\\sdcar\\lib\\site-packages\\l5kit\\rasterization\\sem_box_rasterizer.py\u001b[0m in \u001b[0;36mrasterize\u001b[1;34m(self, history_frames, history_agents, history_tl_faces, agent)\u001b[0m\n\u001b[0;32m     40\u001b[0m     ) -> np.ndarray:\n\u001b[0;32m     41\u001b[0m         \u001b[0mim_out_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbox_rast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrasterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_tl_faces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mim_out_sat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msat_rast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrasterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_tl_faces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mim_out_box\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim_out_sat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\me\\anaconda3\\envs\\sdcar\\lib\\site-packages\\l5kit\\rasterization\\semantic_rasterizer.py\u001b[0m in \u001b[0;36mrasterize\u001b[1;34m(self, history_frames, history_agents, history_tl_faces, agent)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mcenter_in_world_m\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter_in_raster_px\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworld_from_raster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0msem_im\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_semantic_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter_in_world_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraster_from_world\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_tl_faces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msem_im\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\me\\anaconda3\\envs\\sdcar\\lib\\site-packages\\l5kit\\rasterization\\semantic_rasterizer.py\u001b[0m in \u001b[0;36mrender_semantic_map\u001b[1;34m(self, center_in_world, raster_from_world, tl_faces)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mlanes_lines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlane_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxy_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxy_right\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolylines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanes_lines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"default\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m217\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m82\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCV2_SHIFT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolylines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanes_lines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"green\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCV2_SHIFT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolylines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanes_lines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"yellow\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCV2_SHIFT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==== TRAIN LOOP\n",
    "tr_it = iter(train_dataloader)\n",
    "vl_it = iter(val_dataloader)\n",
    "\n",
    "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]), position=0)\n",
    "# progress_bar = tqdm(range(1), position=0)\n",
    "checkpoint_n = cfg[\"train_params\"][\"checkpoint_every_n_steps\"]\n",
    "losses_train = []\n",
    "curr_losses = []\n",
    "losses_avg = []\n",
    "hidden = None\n",
    "\n",
    "losses_val = []\n",
    "val_avg = []\n",
    "\n",
    "num_frames = 20\n",
    "for i in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    pred = model.forward(data, device)\n",
    "    \n",
    "    targets = data['target_positions'].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss = criterion(pred, targets)\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses_train.append(loss.item())\n",
    "    curr_losses.append(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            val_data = next(vl_it)\n",
    "        except StopIteration:\n",
    "            vl_it = iter(val_dataloader)\n",
    "            val_data = next(vl_it)\n",
    "\n",
    "        model.eval()\n",
    "        pred = model.forward(val_data, device)\n",
    "    \n",
    "        targets = val_data['target_positions'].to(device)\n",
    "        target_availabilities = val_data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "\n",
    "        # Backward pass\n",
    "        v_loss = criterion(pred, targets)\n",
    "        v_loss = v_loss * target_availabilities\n",
    "        v_loss = v_loss.mean()\n",
    "        losses_val.append(v_loss.item())\n",
    "\n",
    "#     if (i % checkpoint_n) == checkpoint_n - 1:\n",
    "    if i % 2 == 1:\n",
    "        with open('Seq2Seq_Many_LSTM_val.csv','a') as fd:\n",
    "            for loss in losses_val:\n",
    "                fd.write(f\"{i},{loss}\\n\")\n",
    "                \n",
    "        with open('Seq2Seq_Many_LSTM_train.csv','a') as fd:\n",
    "            for loss in losses_train:\n",
    "                fd.write(f\"{i},{loss}\\n\")\n",
    "\n",
    "        train_avg_loss = np.mean(losses_train)\n",
    "        val_avg_loss =  np.mean(losses_val)\n",
    "        \n",
    "        losses_train = []\n",
    "        losses_val = []\n",
    "        \n",
    "        \n",
    "        with open('Seq2Seq_Many_LSTM_train_avg.csv','a') as fd:\n",
    "            fd.write(f\"{i},{train_avg_loss}\\n\")\n",
    "            \n",
    "        with open('Seq2Seq_Many_LSTM_val_avg.csv','a') as fd:\n",
    "            fd.write(f\"{i},{val_avg_loss}\\n\")\n",
    "        \n",
    "        progress_bar.set_description(f\"loss: {loss} loss(avg): {train_avg_loss} loss_val(avg): {val_avg_loss}\")\n",
    "        if (i+1) % 1000 == 0:\n",
    "            torch.save(model.state_dict(), f'Seq2Seq_Many_LSTM_{i}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT DATASET\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle=train_cfg[\"shuffle\"],\n",
    "                              batch_size=train_cfg[\"batch_size\"])\n",
    "\n",
    "FUTURE_NUM_FRAMES = cfg['model_params']['future_num_frames']\n",
    "SEQ_LEN = cfg['model_params']['history_num_frames'] + 1\n",
    "\n",
    "\n",
    "\n",
    "# ===== INIT  VAL DATASET\n",
    "val_cfg = cfg[\"val_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "# Train dataset/dataloader\n",
    "val_zarr = ChunkedDataset(dm.require(val_cfg[\"key\"])).open()\n",
    "val_dataset = AgentDataset(cfg, val_zarr, rasterizer)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                              shuffle=val_cfg[\"shuffle\"],\n",
    "                              batch_size=val_cfg[\"batch_size\"])\n",
    "                              #num_workers=train_cfg[\"num_workers\"])\n",
    "    \n",
    "# print(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "FUTURE_NUM_FRAMES = cfg['model_params']['future_num_frames']\n",
    "SEQ_LEN = cfg['model_params']['history_num_frames'] + 1\n",
    "\n",
    "model = Seq2Seq(1024, 1024, 256, FUTURE_NUM_FRAMES, SEQ_LEN, FUTURE_NUM_FRAMES).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "model.load_state_dict(torch.load('saved_models/Seq2Seq_Many_LSTM_29999.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:11<00:00,  5.21it/s]\n"
     ]
    }
   ],
   "source": [
    "tr_it = iter(train_dataloader)\n",
    "progress_bar = tqdm(range(1000), position=0)\n",
    "checkpoint_n = cfg[\"train_params\"][\"checkpoint_every_n_steps\"]\n",
    "losses_train = []\n",
    "curr_losses = []\n",
    "losses_avg = []\n",
    "\n",
    "losses_val = []\n",
    "val_avg = []\n",
    "\n",
    "hidden = None\n",
    "\n",
    "num_frames = 20\n",
    "for i in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    \n",
    "    # Train\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model.forward(data, device)\n",
    "    \n",
    "        targets = data['target_positions'].to(device)\n",
    "        target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "\n",
    "        # Backward pass\n",
    "        loss = criterion(pred, targets)\n",
    "        loss = loss * target_availabilities\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        losses_train.append(loss.item())\n",
    "    \n",
    "with open('Seq2Seq_LSTM_Many_CNN_test.csv','a') as fd:\n",
    "    for loss in losses_train:\n",
    "        fd.write(f\"{i},{loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss: 4.966149000187724\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Test Loss: {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
