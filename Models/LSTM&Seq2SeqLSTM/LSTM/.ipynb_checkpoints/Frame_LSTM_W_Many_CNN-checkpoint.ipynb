{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.resnet import resnet50\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(in_channels):\n",
    "    # load pre-trained Conv2D model\n",
    "    model = resnet50(pretrained=True)\n",
    "\n",
    "    # change input channels number to match the rasterizer's output\n",
    "    if in_channels != 3:\n",
    "        model.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            model.conv1.out_channels,\n",
    "            kernel_size=model.conv1.kernel_size,\n",
    "            stride=model.conv1.stride,\n",
    "            padding=model.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "class Frame_LSTM(nn.Module):\n",
    "    def __init__(self, hidden_size1, hidden_size2, hidden_size3, future_num_frames, seq_len, num_layers=1):\n",
    "        super(Frame_LSTM, self).__init__()\n",
    "        self.future_num_frames = future_num_frames\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.lstm = nn.LSTM(1000+2+1, hidden_size1, num_layers, batch_first=True)\n",
    "        self.agent_cnn = build_cnn(2)\n",
    "        self.road_cnn = build_cnn(3)\n",
    "        \n",
    "        self.fc1 = nn.Linear((hidden_size1)+1000+2+1+3, hidden_size2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size3, 2*future_num_frames)\n",
    "        \n",
    "        \n",
    "    def forward(self, data, device):\n",
    "        history_positions = torch.flip(data['history_positions'], [1]).to(device)\n",
    "        history_yaws = torch.flip(data['history_yaws'], [1]).to(device)\n",
    "        encoded_images, road_image = self.images_to_embeddings(data['image'].to(device))\n",
    "        \n",
    "        encoder_in = torch.cat([history_positions, history_yaws, encoded_images], dim=2)\n",
    "        \n",
    "        _, (out, _) = self.lstm(encoder_in)\n",
    "        \n",
    "        batch_size = history_positions.shape[0]\n",
    "        \n",
    "        out = out.squeeze(0)\n",
    "        \n",
    "        centroid = data['centroid'].type(torch.float32).to(device)\n",
    "        yaw = data['yaw'].reshape(-1, 1).type(torch.float32).to(device)\n",
    "        extent = data['extent'].type(torch.float32).to(device)\n",
    "        \n",
    "        out = torch.cat([out, road_image, centroid, yaw, extent],dim=1)\n",
    "        \n",
    "        out = self.fc1(out.squeeze(0))\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)    \n",
    "        \n",
    "        return out.view(batch_size, self.future_num_frames, 2)\n",
    "    \n",
    "    def images_to_embeddings(self,images):\n",
    "        seq_len = self.seq_len\n",
    "        batch_size = images.shape[0]\n",
    "        encoded_images = []\n",
    "        for i in range(seq_len):\n",
    "            ego_idx = (seq_len) + i\n",
    "            im = torch.cat([images[:,i:i+1,:,:], images[:,ego_idx:ego_idx+1,:,:]], dim=1)\n",
    "\n",
    "            out = self.agent_cnn(im).reshape(batch_size,1,-1)\n",
    "            encoded_images.append(out)\n",
    "        encoded_images.reverse()\n",
    "        return torch.cat(encoded_images, dim=1), self.road_cnn(images[:,-3:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"A:/CSCI 566 Project/\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "# cfg = load_config_data(\"./configs/Frame_LSTM_Config.yaml\")\n",
    "cfg = {'format_version': 4,\n",
    " 'model_params': {'history_num_frames': 10,\n",
    "  'history_step_size': 1,\n",
    "  'history_delta_time': 0.1,\n",
    "  'future_num_frames': 50,\n",
    "  'future_step_size': 1,\n",
    "  'future_delta_time': 0.1},\n",
    " 'raster_params': {'raster_size': [224, 224],\n",
    "  'pixel_size': [0.5, 0.5],\n",
    "  'ego_center': [0.5, 0.5],\n",
    "  'map_type': 'py_semantic',\n",
    "  'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "  'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "  'dataset_meta_key': 'meta.json',\n",
    "  'filter_agents_threshold': 0.5,\n",
    "  'disable_traffic_light_faces': False},\n",
    " 'train_data_loader': {'key': 'scenes/train.zarr',\n",
    "  'batch_size': 4,\n",
    "  'shuffle': True,\n",
    "  'num_workers': 1},\n",
    " 'val_data_loader': {'key': 'scenes/validate.zarr',\n",
    "  'batch_size': 4,\n",
    "  'shuffle': False,\n",
    "  'num_workers': 1},\n",
    " 'test_data_loader': {'key': 'scenes/test.zarr',\n",
    "  'batch_size': 32,\n",
    "  'shuffle': False,\n",
    "  'num_workers': 1},\n",
    " 'train_params': {'checkpoint_every_n_steps': 100,\n",
    "  'max_num_steps': 100000,\n",
    "  'eval_every_n_steps': 100}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT DATASET\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle=train_cfg[\"shuffle\"],\n",
    "                              batch_size=train_cfg[\"batch_size\"])\n",
    "\n",
    "FUTURE_NUM_FRAMES = cfg['model_params']['future_num_frames']\n",
    "SEQ_LEN = cfg['model_params']['history_num_frames'] + 1\n",
    "\n",
    "\n",
    "\n",
    "# ===== INIT  VAL DATASET\n",
    "val_cfg = cfg[\"val_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "# Train dataset/dataloader\n",
    "val_zarr = ChunkedDataset(dm.require(val_cfg[\"key\"])).open()\n",
    "val_dataset = AgentDataset(cfg, val_zarr, rasterizer)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                              shuffle=val_cfg[\"shuffle\"],\n",
    "                              batch_size=val_cfg[\"batch_size\"])\n",
    "                              #num_workers=train_cfg[\"num_workers\"])\n",
    "    \n",
    "# print(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Frame_LSTM(1024, 1024, 512, FUTURE_NUM_FRAMES, SEQ_LEN).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TRAIN LOOP\n",
    "tr_it = iter(train_dataloader)\n",
    "vl_it = iter(val_dataloader)\n",
    "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]), position=0)\n",
    "# progress_bar = tqdm(range(1), position=0)\n",
    "checkpoint_n = cfg[\"train_params\"][\"checkpoint_every_n_steps\"]\n",
    "losses_train = []\n",
    "curr_losses = []\n",
    "losses_avg = []\n",
    "hidden = None\n",
    "\n",
    "losses_val = []\n",
    "val_avg = []\n",
    "\n",
    "num_frames = 20\n",
    "for i in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    pred = model.forward(data, device)\n",
    "    \n",
    "    targets = data['target_positions'].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Backward pass\n",
    "    loss = criterion(pred, targets)\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses_train.append(loss.item())\n",
    "    curr_losses.append(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            val_data = next(vl_it)\n",
    "        except StopIteration:\n",
    "            vl_it = iter(val_dataloader)\n",
    "            val_data = next(vl_it)\n",
    "\n",
    "        model.eval()\n",
    "        # Forward pass\n",
    "        pred = model.forward(val_data, device)\n",
    "    \n",
    "    targets = val_data['target_positions'].to(device)\n",
    "    target_availabilities = val_data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "\n",
    "    # Backward pass\n",
    "    v_loss = criterion(pred, targets)\n",
    "    v_loss = v_loss * target_availabilities\n",
    "    v_loss = v_loss.mean()\n",
    "    losses_val.append(v_loss.item())\n",
    "\n",
    "#     if (i % checkpoint_n) == checkpoint_n - 1:\n",
    "    if i % 2 == 1:\n",
    "        with open('Frame_CNN_Many_LSTM_val.csv','a') as fd:\n",
    "            for loss in losses_val:\n",
    "                fd.write(f\"{i},{loss}\\n\")\n",
    "                \n",
    "        with open('Frame_CNN_Many_LSTM_train.csv','a') as fd:\n",
    "            for loss in losses_train:\n",
    "                fd.write(f\"{i},{loss}\\n\")\n",
    "\n",
    "        train_avg_loss = np.mean(losses_train)\n",
    "        val_avg_loss =  np.mean(losses_val)\n",
    "        \n",
    "        losses_train = []\n",
    "        losses_val = []\n",
    "        \n",
    "        \n",
    "        with open('Frame_CNN_Many_LSTM_train_avg.csv','a') as fd:\n",
    "            fd.write(f\"{i},{train_avg_loss}\\n\")\n",
    "            \n",
    "        with open('Frame_CNN_Many_LSTM_val_avg.csv','a') as fd:\n",
    "            fd.write(f\"{i},{val_avg_loss}\\n\")\n",
    "            \n",
    "        progress_bar.set_description(f\"loss: {loss} loss(avg): {train_avg_loss} loss_val(avg): {val_avg_loss}\")\n",
    "        if (i+1) % 1000 == 0:\n",
    "            torch.save(model.state_dict(), f'saved_models/Frame_Many_LSTM{i}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame_Partial_LSTM9999\n",
    "train_cfg = cfg[\"test_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle=train_cfg[\"shuffle\"],\n",
    "                              batch_size=train_cfg[\"batch_size\"])\n",
    "\n",
    "FUTURE_NUM_FRAMES = cfg['model_params']['future_num_frames']\n",
    "SEQ_LEN = cfg['model_params']['history_num_frames'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Frame_LSTM(1024, 1024, 512, FUTURE_NUM_FRAMES, SEQ_LEN).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "model.load_state_dict(torch.load('saved_models/Frame_Many_LSTM27999.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_it = iter(train_dataloader)\n",
    "progress_bar = tqdm(range(1000), position=0)\n",
    "checkpoint_n = cfg[\"train_params\"][\"checkpoint_every_n_steps\"]\n",
    "losses_train = []\n",
    "curr_losses = []\n",
    "losses_avg = []\n",
    "\n",
    "losses_val = []\n",
    "val_avg = []\n",
    "\n",
    "hidden = None\n",
    "\n",
    "num_frames = 20\n",
    "for i in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    \n",
    "    # Train\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model.forward(data, device)\n",
    "    \n",
    "        targets = data['target_positions'].to(device)\n",
    "        target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "\n",
    "        # Backward pass\n",
    "        loss = criterion(pred, targets)\n",
    "        loss = loss * target_availabilities\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        losses_train.append(loss.item())\n",
    "    \n",
    "with open('Frame_LSTM_Partial_test.csv','a') as fd:\n",
    "    for loss in losses_train:\n",
    "        fd.write(f\"{i},{loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Test Loss: {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
