{"version":3,"sources":["Header.js","images/main/Motivation1.png","images/main/SDCAR.gif","images/main/SDCAR2.gif","images/main/SLSTM1.png","images/main/SLSTM2.png","images/main/SLSTM3.png","images/main/GAN1.png","images/main/GAN2.png","images/main/GAN3.png","images/main/GAN4.png","images/main/GAN5.png","MainContainer.js","images/main/SLSTM0.png","BaseComponent.js","images/resnetgru/resnetgru1.png","images/resnetgru/resnetgru2.png","images/resnetgru/resnetgru3.png","images/resnetgru/resnetgru4.png","images/resnetgru/resnetgru5.png","images/resnetgru/resnetgru6.png","images/resnetgru/resnetgru7.png","images/resnetgru/resnetgru8.png","images/resnetgru/resnetgrutable1.png","images/resnetgru/resnetgrutable2.png","ResnetGNU.js","Seq2Seq.js","LSTM.js","images/vaelstm/vae1.png","images/vaelstm/vaetable1.png","images/vaelstm/vae2.png","images/vaelstm/vae3.png","images/vaelstm/vae4.png","images/vaelstm/vae5.png","images/vaelstm/vae6.png","images/vaelstm/vae7.png","images/vaelstm/vae8.png","images/vaelstm/vae9.png","images/vaelstm/vaea.png","images/vaelstm/vaeb.png","VAELSTM.js","Seq2SeqGAN.js","SocialLSTM.js","App.js","reportWebVitals.js","index.js"],"names":["Header","props","Navbar","collapseOnSelect","bg","variant","expand","Brand","Toggle","aria-controls","Collapse","id","Nav","className","activeKey","Link","eventKey","as","RouterNavLink","to","Component","withRouter","MainContainer","alt","src","Motivation1","class","SDCAR","SDCAR2","style","textAlign","GAN1","GAN2","GAN3","width","GAN4","GAN5","SLSTM1","SLSTM2","SLSTM3","BaseComponent","window","scrollTo","ResnetGNU","resnetgru1","resnetgru2","resnetgrutable1","resnetgru3","resnetgru4","resnetgru5","resnetgru6","resnetgrutable2","resnetgru7","resnetgru8","Seq2Seq","LSTM","VAELSTM","vae1","scr","vaetable1","vae2","vae3","vae4","vae5","vae6","vae7","vae8","vae9","vaea","vaeb","Seq2SeqGAN","SocialLSTM","App","exact","path","component","page","from","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"wdAQMA,E,kDAEF,WAAYC,GAAQ,uCACVA,G,qDAIN,OACI,iCACK,eAACC,EAAA,EAAD,CACGC,kBAAgB,EAChBC,GAAG,OACHC,QAAQ,OACRC,OAAO,KAJV,UAKG,cAACJ,EAAA,EAAOK,MAAR,8DACA,sBACA,cAACL,EAAA,EAAOM,OAAR,CACIC,gBAAc,qBAClB,eAACP,EAAA,EAAOQ,SAAR,CAAiBC,GAAG,mBAApB,UACI,eAACC,EAAA,EAAD,CAAKC,UAAU,sBAAsBC,UAAU,IAA/C,UACI,cAACF,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,QAHP,kBAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,cAHP,wBAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,QAHP,kBAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,WAHP,qBAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,YAHP,sBAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,cAHP,wBAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,UAHP,4BAOJ,cAACjB,EAAA,EAAOK,MAAR,yC,GA/DHa,aAuENC,cAAWrB,GC/EX,MAA0B,wCCA1B,MAA0B,kCCA1B,MAA0B,mCCA1B,MAA0B,mCCA1B,MAA0B,mCCA1B,MAA0B,mCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iC,MCmM1BsB,MAnLf,SAAuBrB,GACnB,OACI,qBAAKU,GAAG,kBAAR,SACI,sBAAKA,GAAG,YAAR,UACI,mDACA,q6BAEA,qBAAKA,GAAG,iBAAiBY,IAAI,cAAcC,IAAKC,EAAaC,MAAM,WACnE,sBAAKf,GAAG,2BAAR,UACI,qBAAKY,IAAI,WAAWC,IAAKG,IACzB,qBAAKJ,IAAI,WAAWC,IAAKI,OAE7B,4CACA,4uCACA,yCACA,0LACK,cAAC,IAAD,CAAMT,GAAG,cAAT,SAAuB,6CAD5B,QACyD,cAAC,IAAD,CAAMA,GAAG,QAAT,SAAiB,oDAD1E,gFACsL,cAAC,IAAD,CAAMA,GAAG,YAAT,SAAqB,2CAD3M,QACsO,cAAC,IAAD,CAAMA,GAAG,cAAT,SAAuB,8CAD7P,iCACoT,cAAC,IAAD,CAAMA,GAAG,UAAT,SAAmB,8CADvU,8DAEA,uMACA,4BAAG,4BAAG,8BAAG,cAAC,IAAD,CAAMA,GAAG,cAAcN,UAAU,OAAjC,yBAAH,0BACN,sBAAKgB,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKN,IAAKO,EAAML,MAAM,UAAc,8EACvE,sBAAKG,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKN,IAAKQ,IAAO,gFACpD,uHAAyF,4CAAzF,yQACA,iJACA,sBAAKH,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKN,IAAKS,IAAO,qGACpD,uLACA,uBACA,wBAAOJ,MAAO,CAACK,MAAO,OAAtB,UACI,2FACA,+BACI,wCACA,wCACA,mDAEJ,+BACI,qDACA,mCACA,2CAEJ,+BACI,qDACA,mCACA,8CAGR,sBAAKL,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKN,IAAKW,IAAO,yFACpD,gWACA,sBAAKN,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKN,IAAKY,IAAO,+FACpD,kKACA,uBAEA,4BAAG,4BAAG,8BAAG,cAAC,IAAD,CAAMjB,GAAG,cAAcN,UAAU,OAAjC,yBAAH,0BACN,sBAAKgB,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKN,IAAKa,EAAQX,MAAM,UACxB,6DAEJ,2PACA,qBAAKG,MAAO,CAACC,UAAW,UAAxB,SAAmC,qBAAKN,ICxEzC,izRDyEC,uBACA,mFACI,+BACI,6MACA,4NACA,2SAJR,+FAOI,+BACI,+IACA,sWACA,oHAVR,2FAcA,sBAAKK,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKN,IAAKc,EAAQZ,MAAM,UACxB,8DAEJ,uPACA,sBAAKG,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKN,IAAKe,EAAQb,MAAM,YACxB,kGAEJ,6CACA,uBACA,8BAAG,4BAAG,6EAAqD,uBAA3D,yaAEA,8BAAG,4BAAG,6EAAqD,uBAA3D,uqBAEA,yCACA,+eACI,kCACI,+BACI,6BAAI,yCACJ,6BAAI,+CAER,+BACI,kDACA,gDAEJ,+BACI,8DACA,gDAEJ,+BACI,6EACA,+CAEJ,+BACI,2DACA,iDAEJ,+BACI,oDACA,gDAEJ,+BACI,oDACA,+CAEJ,+BACI,0DACA,2CAEJ,+BACI,0DACA,2CAEJ,+BACI,qFACA,gDAEJ,+BACI,6DACA,6BAAI,4CAER,+BACI,+DACA,2CAEJ,+BACI,+DACA,2CAEJ,+BACI,gDACA,kDAGR,m3BAEA,6CACA,mQACA,0EACI,+BACI,mHACA,sHAIR,8JACI,+BACI,kGACA,oGACA,uHAIR,gFACI,6BAAI,kMAER,0MACA,oWAEA,4CACA,g7BE3LCc,E,kLAEbC,OAAOC,SAAS,EAAG,O,GAFgBtB,aCF5B,MAA0B,uCCA1B,MAA0B,uCCA1B,MAA0B,uCCA1B,MAA0B,uCCA1B,MAA0B,uCCA1B,MAA0B,uCCA1B,MAA0B,uCCA1B,MAA0B,uCCA1B,MAA0B,4CCA1B,MAA0B,4CCcpBuB,G,6KAEb,OAEI,qBAAKhC,GAAG,kBAAR,SACI,sBAAKA,GAAG,YAAR,UACA,kDACA,60FASJ,sBAAKkB,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKN,IAAKoB,IACV,sDACA,qBAAKpB,IAAKqB,IACV,4DAEA,olBAEJ,sBAAKhB,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKN,IAAKsB,EAAiBpB,MAAM,eACjC,uBACA,qBAAKF,IAAKuB,IACV,2EACA,qBAAKvB,IAAKwB,IACV,2EACA,qBAAKxB,IAAKyB,IACV,2EACA,qFACA,qBAAKzB,IAAK0B,IACV,+EACA,yIACA,qBAAK1B,IAAK2B,EAAiBzB,MAAM,eACjC,6FACA,qBAAKF,IAAK4B,IACV,4EACA,qBAAK5B,IAAK6B,IACV,iEAEJ,qEACA,s2B,GA7C2Bb,ICTlBc,G,6KAEb,OACI,qBAAK3C,GAAG,kBAAR,SACI,qBAAKA,GAAG,YAAR,mD,GAJqB6B,ICDhBe,G,6KAEb,OACI,qBAAK5C,GAAG,kBAAR,SACI,qBAAKA,GAAG,YAAR,SACK,uC,GALa6B,ICJnB,G,MAAA,IAA0B,kCCA1B,MAA0B,sCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,OAA0B,iCCA1B,OAA0B,iCCgBpBgB,G,uKAEb,OACI,qBAAK7C,GAAG,kBAAR,SACI,sBAAKA,GAAG,YAAR,UACI,0CACA,wPACA,wSACA,yMACA,qBAAKkB,MAAO,CAACC,UAAW,UAAxB,SAAmC,qBAAKN,IAAKiC,MAC7C,gNACA,qBAAK5B,MAAO,CAACC,UAAW,UAAxB,SAAmC,qBAAK4B,IAAKC,MAC7C,sBAAK9B,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKJ,MAAM,OAAOF,IAAKoC,IAAO,6DACjE,sBAAK/B,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKJ,MAAM,OAAOF,IAAKqC,IAAO,6DACjE,sBAAKhC,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKJ,MAAM,OAAOF,IAAKsC,IAAO,8EACjE,+IACA,inBACA,2RACA,sBAAKjC,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKN,IAAKuC,IAAO,iFACpD,4aACA,sBAAKlC,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKJ,MAAM,OAAOF,IAAKwC,IACvB,qBAAKtC,MAAM,OAAOF,IAAKyC,IACvB,qBAAKvC,MAAM,OAAOF,IAAK0C,IACvB,uBACA,qBAAKxC,MAAM,OAAOF,IAAK2C,IACvB,qBAAKzC,MAAM,OAAOF,IAAK4C,KACvB,qBAAK1C,MAAM,OAAOF,IAAK6C,iB,GA3BV7B,GCZhB8B,I,6KAEb,OACI,qBAAK3D,GAAG,kBAAR,SACI,qBAAKA,GAAG,YAAR,sD,GAJwB6B,ICAnB+B,I,6KAEb,OACI,qBAAK5D,GAAG,kBAAR,SACI,qBAAKA,GAAG,YAAR,sD,GAJwB6B,ICiDzBgC,OAxCf,WACI,OACI,oCACI,cAAC,EAAD,IACA,eAAC,IAAD,WACA,cAAC,IAAD,CAAOC,OAAK,EACJC,KAAK,QACLC,UAAW,kBAAM,cAAC,EAAD,CAA0BC,KAAK,QAAZ,WAE5C,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,cACLC,UAAW,kBAAM,cAAC,EAAD,CAA4BC,KAAK,cAAlB,iBAExC,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,QACLC,UAAW,kBAAM,cAAC,EAAD,CAAiBC,KAAK,QAAZ,WAEnC,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,WACLC,UAAW,kBAAM,cAAC,EAAD,CAAuBC,KAAK,WAAf,cAEtC,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,YACLC,UAAW,kBAAM,cAAC,GAAD,CAAwBC,KAAK,YAAhB,eAEtC,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,cACLC,UAAW,kBAAM,cAAC,GAAD,CAA6BC,KAAK,cAAlB,iBAEzC,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,UACLC,UAAW,kBAAM,cAAC,GAAD,CAAYpB,MAAI,EAAcqB,KAAK,UAAd,aAE9C,cAAC,IAAD,CAAUC,KAAK,IAAI1D,GAAG,eA/Bb,QCHN2D,GAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,6BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCAdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,IAAD,UAAQ,cAAC,GAAD,QAEVC,SAASC,eAAe,SAM1Bb,O","file":"static/js/main.b725b9f1.chunk.js","sourcesContent":["import React, {Component} from \"react\"\nimport 'bootstrap/dist/css/bootstrap.min.css'\nimport Navbar from 'react-bootstrap/Navbar'\nimport Nav from 'react-bootstrap/Nav'\nimport './Header.css'\nimport { withRouter } from 'react-router-dom'\nimport { NavLink as RouterNavLink } from 'react-router-dom';\n\nclass Header extends Component {\n\n    constructor(props) {\n        super(props)\n    }\n\n    render() {\n        return (\n            <header>\n                 <Navbar\n                    collapseOnSelect\n                    bg=\"dark\"\n                    variant=\"dark\"\n                    expand=\"lg\">\n                    <Navbar.Brand>Lyft5 Motion Prediction for Autonomous Vehicles</Navbar.Brand>\n                    <t/>\n                    <Navbar.Toggle\n                        aria-controls=\"basic-navbar-nav\" />\n                    <Navbar.Collapse id=\"basic-navbar-nav\">\n                        <Nav className='mr-auto nav-section' activeKey='7'>\n                            <Nav.Link\n                                eventKey='0'\n                                as={RouterNavLink}\n                                to=\"/home\">\n                                Home\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='1'\n                                as={RouterNavLink}\n                                to=\"/resnet-gru\">\n                                Resnet-GRU\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='2'\n                                as={RouterNavLink}\n                                to=\"/lstm\">\n                                LSTM\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='3'\n                                as={RouterNavLink}\n                                to=\"/seq2seq\">\n                                Seq2Seq\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='4'\n                                as={RouterNavLink}\n                                to=\"/vae-lstm\">\n                                VAE+LSTM\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='5'\n                                as={RouterNavLink}\n                                to=\"/seq2seqGAN\">\n                                Seq2SeqGAN\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='6'\n                                as={RouterNavLink}\n                                to=\"/s-lstm\">\n                                Social LSTM\n                            </Nav.Link>\n                        </Nav>\n                        <Navbar.Brand>Deep New World</Navbar.Brand>\n                    </Navbar.Collapse>\n                </Navbar>\n            </header>\n        )\n    }\n}\n\nexport default withRouter(Header)","export default __webpack_public_path__ + \"static/media/Motivation1.7da28e5e.png\";","export default __webpack_public_path__ + \"static/media/SDCAR.f1dc928a.gif\";","export default __webpack_public_path__ + \"static/media/SDCAR2.462f0ff0.gif\";","export default __webpack_public_path__ + \"static/media/SLSTM1.53c7ec37.png\";","export default __webpack_public_path__ + \"static/media/SLSTM2.87ffcdf1.png\";","export default __webpack_public_path__ + \"static/media/SLSTM3.0e772885.png\";","export default __webpack_public_path__ + \"static/media/GAN1.a5d75e69.png\";","export default __webpack_public_path__ + \"static/media/GAN2.f236eade.png\";","export default __webpack_public_path__ + \"static/media/GAN3.b1c2e4d2.png\";","export default __webpack_public_path__ + \"static/media/GAN4.2d86396c.png\";","export default __webpack_public_path__ + \"static/media/GAN5.bc8a36cc.png\";","import React from \"react\";\nimport Motivation1 from './images/main/Motivation1.png'\nimport SDCAR from './images/main/SDCAR.gif'\nimport SDCAR2 from './images/main/SDCAR2.gif'\nimport SLSTM0 from './images/main/SLSTM0.png'\nimport SLSTM1 from './images/main/SLSTM1.png'\nimport SLSTM2 from './images/main/SLSTM2.png'\nimport SLSTM3 from './images/main/SLSTM3.png'\nimport GAN1 from './images/main/GAN1.png'\nimport GAN2 from './images/main/GAN2.png'\nimport GAN3 from './images/main/GAN3.png'\nimport GAN4 from './images/main/GAN4.png'\nimport GAN5 from './images/main/GAN5.png'\nimport { Link } from \"react-router-dom\";\nimport \"./MainContainer.css\";\n\nfunction MainContainer(props) {\n    return (\n        <div id='outer_container'>\n            <div id='container'>\n                <h2>Problem Statement</h2>\n                <p>One of the most primitive tasks of autonomous vehicles is to predict the future positions of other vehicles and the vehicle itself amidst traffic. In this project, we aim to predict the future positions of the other vehicles/agents (cyclists/cars/pedestrians). \nWe are using the dataset provided by Lyft known as the l5kit. This dataset contains over 1000 hours of scenes with labeled positional information of objects in the scene such as vehicles, cyclists, and pedestrians. The scenes in the dataset are from a 6.8 mile route chosen by the Lyft team. The input data will be the sequence of spatial coordination about each vehicle (both for train and test) which can also include road information of the given agents environment.  The output will be the future spatial coordination. Given the uncertainty of live traffic, we have also aimed to predict the future positions for multiple trajectories.</p>\n                <img id='motivation_pic' alt=\"Motivation1\" src={Motivation1} class='center'/>\n                <div id='more_motivation_pics_div'>\n                    <img alt=\"Picture2\" src={SDCAR}/>\n                    <img alt=\"Picture3\" src={SDCAR2}/>\n                </div>\n                <h2>Motivation</h2>\n                <p>Autonomous vehicles are the future of on land mobility for mankind. But, we are still far fetched from seamless autonomous transportation and we need to resolve many significant challenges before we can truly travel autonomous. One of the most important aspects of any autonomous system is the prediction of the next move, let it be an automatic game, or even an automatic vehicle, the challenge to predict the next move is significant for its success. It's of utmost importance to correctly predict the next position of the vehicle as it will not only determine the direction in which the vehicle should move but will also impact the safety of the passenger and also others on the road. What makes this challenge even more critical is the fact that the next position of the vehicle should be determined keeping in mind not only the surroundings but also the destination and traffic agents. The impact of this solution is enormous in autonomous vehicles being successful and making transportation seamless and safe which has been a driving force behind us choosing to work on this problem statement. It is an exciting opportunity to work on a problem that will solve some pressing real world use cases and make a global impact.</p>\n                <h2>Methods</h2>\n                <p>To resolve this issue, we have implemented various methods and compared their performance by test loss in the Result section below. \nFirstly, we explored <Link to='/resnet-gru'><b>Resnet-Gru</b></Link> and <Link to='/lstm'><b>LSTM/Seq2Seq LSTM</b></Link> models. Based on the LSTM model, we added VAE and GAN structure and created <Link to='/vae-lstm'><b>VAE+LSTM</b></Link> and <Link to='/seq2seqGAN'><b>Seq2Seq GAN</b></Link>. And finally, we implemented <Link to='/s-lstm'><b>Social LSTM</b></Link> which incorporates the neighbors' effect into the model</p>\n                <p>In this page, we mainly talk about the two most interesting models among these. For the detailed information about each model we created, please check the other tabs.</p>\n                <p><i><b><Link to='/seq2seqGAN' className=\"link\">Seq2Seq GAN</Link> and its variant</b></i></p>\n                <div style={{textAlign: 'center'}}><img src={GAN1} class='model'></img><p>Figure 1. Generator of Seq2Seq GAN Model-1</p></div>\n                <div style={{textAlign: 'center'}}><img src={GAN2}/><p>Figure 2. Discriminator of Seq2Seq GAN Model</p></div>\n                <p>To the baseline LSTM model, we added the GAN and created a new model. We call this as <b>Seq2Seq GAN</b> here. In this model, the generator generates the next 50 moves based on the past 11 positions and the discriminator determines whether it’s real or fake. We expected that this structure can avoid the blurry prediction and make more accurate predictions.</p>\n                <p>To improve Seq2Seq GAN Model-1, we incorporated the past yaw information as well and made a Seq2Seq GAN Model-2.</p>\n                <div style={{textAlign: 'center'}}><img src={GAN3}/><p>Figure 3. Generator of Seq2Seq GAN Model-2 (with yaw information)</p></div>\n                <p>As you see the Table 1, compared to the Seq2Seq GAN Model-1, the Seq2Seq GAN Model-2 was improved in the sense of test loss under the same conditions.</p>\n                <br></br>\n                <table style={{width: '30%'}}>\n                    <caption>Table 1. Test Losses of different Seq2Seq GAN Models</caption>\n                    <tr>\n                        <td>Models</td>\n                        <td>Epochs</td>\n                        <td>Test Loss(MSE)</td>\n                    </tr>\n                    <tr>\n                        <td>Seq2Seq GAN Model-1</td>\n                        <td>1</td>\n                        <td>2.6551</td>\n                    </tr>\n                    <tr>\n                        <td>Seq2Seq GAN Model-2</td>\n                        <td>1</td>\n                        <td>2.2576</td>\n                    </tr>\n                </table>\n                <div style={{textAlign: 'center'}}><img src={GAN4}/><p>Figure 4. Loss graphs during training for Seq2Seq GAN</p></div>\n                <p>After the training, we tested our Seq2Seq model to check if it doesn’t fall into the mode failure and it can generate diverse predictions for the inputs. We generated one hundred sample predictions for the same batch(32) input and plotted all the generated future trajectories for the randomly-picked four inputs. </p>\n                <div style={{textAlign: 'center'}}><img src={GAN5}/><p>Figure 5. Predicted Trajectories from the Seq2Seq GAN Model</p></div>\n                <p>As you see in Figure 5, the Seq2Seq GAN model does not suffer from mode collapse and successfully generates diverse trajectories.</p>\n                <br></br>\n\n                <p><i><b><Link to='/seq2seqGAN' className=\"link\">Social LSTM</Link> and its variant</b></i></p>\n                <div style={{textAlign: 'center'}}>\n                    <img src={SLSTM1} class='model'/>\n                    <p>Fig 6. Social LSTM method</p>\n                </div>\n                <p>-Based on Alexandre etc’s research, we tried to take the neighbor’s effect into consideration combining with the LSTM model. For this, we added a max-pooling layer between each time stamp, its formulation is:</p>\n                <div style={{textAlign: 'center'}}><img src={SLSTM0}/></div>\n                <br></br>\n                <p>However, this method has the following limitation:\n                    <ul>\n                        <li>Simply adding all the neighbor’s information does not make sense. People care more about the neighborhood closer to them than those people who are far away from them.</li>\n                        <li>This social LSTM architecture is a generalization method. However, if we can give a more specific classification, it is highly likely that we can improve the prediction’s accuracy. </li>\n                        <li>For the social LSTM architecture, when the agent falls into a grid, it will be considered as a neighborhood. For the architecture, the grid we considered is a square. It seems unreasonable because agents move faster in the speed’s direction than the other. </li>\n                    </ul>\n                    Based on this two observations, we can modify the current social-LSTM architecture as below:\n                    <ul>\n                        <li>Introducing the spatial information max-pooling part, which can be realized by using the convolutional layer.</li>\n                        <li>In the real world, when a car moves, it only has two classes: longitude and latitude. For the longitude classes, we splitted into 3 situations: speed up, normal and slow down; turning left, staying the same and turning right are these 3 situations for the latitude classes. More details are available on the Social LSTM tab.</li>\n                        <li>For the grid, instead of considering the square, tried to consider rectangular.</li>\n                    </ul>\n                    As a result of the modifications, we created a Social LSTM variant model as Figure 6. \n                </p>\n                <div style={{textAlign: 'center'}}>\n                    <img src={SLSTM2} class='model'/>\n                    <p>Fig 7. Social-LSTM variant</p>\n                </div>\n                <p>For this model, we slightly changed our goal from directly predicting the future trajectory to maximizing the probability of prediction and returning the one with the highest probability. The objective function is:</p>\n                <div style={{textAlign: 'center'}}>\n                    <img src={SLSTM3} class='formula'/>\n                    <p>Figure 8. Training Losses of Social LSTM and its variant model</p>\n                </div>\n                <h2>Fine Tuning</h2>\n                <br></br>\n                <p><i><b>Seq2Seq GAN and Seq2Seq GAN variant models</b></i><br></br>\n                Based on the two Seq2Seq models, we tried parameter-tuning by changing learning rates(1e-3, 5e-3, 1e-4), loss functions(BCE, MSE), and layers on the discriminators. So far, the best Seq2Seq GAN Model among these is the model-1 with a learning rate of 0.001, batch size of 32, MSE loss function on the generator, and epochs of 2 and the test loss of this model is 1.9143. More detailed information is on the Seq2Seq tab.</p>\n                <p><i><b>Social-LSTM and social-LSTM variant Models</b></i><br></br>\n                For social-LSTM and its variants, there exist multiple parameters that we can tune.  For both models, we tried six different learning rates(1e-3, 3e-3, 5e-3, 8e-3, 1e-2, 2e-2) and found that 1e-03 is the best one. For social-LSTM’s neighborhood grid, we tried the grid’s size from (1,1) to (10, 10), it seems that (7,7) gives the minimum loss for the training part. For the variant method, we tried one direction from 1 to 5 and another one is from 7 to 15. Among all these trials, (13, 3) returns the best loss for the training datasets. In addition, for the convolutional layer in the variant method, we tried 3 by 3, 4 by 4 and 5 by 5. At the end, 3 by 3 wins.</p>\n                <h2>Results</h2>\n                <p>After training all of our models and comparing their performance on the test set, it was clear that Seq2Seq GAN had the best performance, though our more advanced models all achieved similar results. We are happy to report that all of our models perform significantly better than the provided Lyft baseline, with all presented models achieving over 3 times better performance. The following chart shows the average MSE test loss from our most interesting models:</p>\n                    <table>\n                        <tr>\n                            <th><b>Method</b></th>\n                            <th><b>Test loss</b></th>\n                        </tr>\n                        <tr>\n                            <td>LSTM - Positions</td>\n                            <td>2.257230417</td>\n                        </tr>\n                        <tr>\n                            <td>LSTM - Positions + BEV Image</td>\n                            <td>2.591220331</td>\n                        </tr>\n                        <tr>\n                            <td>LSTM - Positions + Agent View + Road Images</td>\n                            <td>3.11691956</td>\n                        </tr>\n                        <tr>\n                            <td>LSTM - All Available Data</td>\n                            <td>2.6480581797</td>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq LSTM - POS</td>\n                            <td>2.998081285</td>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq LSTM - CNN</td>\n                            <td>3.61865188</td>\n                        </tr>\n                        <tr>\n                            <td>VAE-LSTM Configuration A</td>\n                            <td>2.3270</td>\n                        </tr>\n                        <tr>\n                            <td>VAE-LSTM Configuration B</td>\n                            <td>2.2085</td>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq LSTM - Positions + Agent View + Road Images</td>\n                            <td>2.290573328</td>\n                        </tr>\n                        <tr>\n                            <td>Seq2SeqGAN Configuration V2</td>\n                            <td><b>1.9143</b></td>\n                        </tr>\n                        <tr>\n                            <td>Seq2SeqGAN Configuration V4-1</td>\n                            <td>2.1783</td>\n                        </tr>\n                        <tr>\n                            <td>Seq2SeqGAN Configuration V4-2</td>\n                            <td>2.2576</td>\n                        </tr>\n                        <tr>\n                            <td>Social LSTM-v2</td>\n                            <td>3.55784371</td>\n                        </tr>\n                    </table>\n                    <p>While Social LSTM was not the best performer, we believe in its potential and that our current architecture or even a slightly modified version can achieve a much lower loss, especially with more training. Moreover, due to computational constraints, our social LSTM does not differentiate different types of objects (i.e. Car, Cyclist, Pedestrian) when attempting to compute their trajectories. This loss of information could prevent the model from achieving a better loss.\nOur Seq2Seq Gan architecture achieved the best performance, performing more than 6x better than the baseline model. While the Seq2Seq architecture is often used for text translations, we were interested to see its performance in the trajectory prediction domain. To our surprise, Seq2Seq performed extremely well, allowing our Seq2Seq GAN model to achieve the best test loss. </p>\n                    <h2>Future Work</h2>\n                    <p>Although we implemented various models and tried many different approaches with those models, we found some limitations on our models from our experiments and they still give us chances to enhance our models with future works.</p>\n                    <p>For Seq2Seq GAN,  it didn’t consider\n                        <ul>\n                            <li>the road information therefore, the generated predictions can fall off the roads.</li>\n                            <li>the interaction with neighbors including other cars, pedestrians, or cyclists.</li>\n                        </ul>\n                    </p>\n             \n                    <p>To complement these limitations and improve our Seq2Seq GAN Model, we can introduce the following methods in the future work.\n                        <ul>\n                            <li>Training the model with optimal parameters and with more epochs.</li>\n                            <li>Combining with a CNN model including road information from images.</li>\n                            <li>Applying the concept of Social GAN which embraces interactions among neighbors.</li>\n                        </ul>\n                    </p>\n  \n                    <p>For our Social LSTM, the current model does not \n                        <ul><li>differentiate different types of objects(i.e. Car, Cyclist, Pedestrian) when attempting to compute their trajectories because of computational constraints.</li></ul>\n                    </p>\n                    <p>However, we believe that our Social LSTM model can provide more precise predictions with much lower loss if we apply the former approach and train it with more epochs.  </p>\n                    <p>One potential approach we had hoped to try was utilizing a transformer based network to compare its performance with other models. This dataset contains multiple facets of vehicle information like historical availability, various views like semantic, satellite, etc.  which can be utilized to get more precise predictions. </p>\n\n                    <h2>Conclusion</h2>\n                    <p>All in all, we are excited about our results and our model’s ability to surpass the baseline model. Our approach to this problem was to perform an in-depth analysis of several deep learning approaches, testing many architectures like CNN-GRU, LSTM, and GAN. Through our testing, we were able to iterate on our architectures through both parameter tuning and changes to the structure of each model. With these trials, we were able to get a holistic view of the problem and gain an understanding of how changes to our architectures can impact its performance. Our iterative approach sparked our interest in utilizing the Seq2Seq model on top of our existing architectures or even prompted us to apply advanced techniques like Social GAN and tweak its architecture to best match our dataset. For more information on other architectures we tested, please see the other tabs listed at the top of the page.</p>\n            </div>\n        </div>\n    )\n}\n\nexport default MainContainer;","export default \"data:image/png;base64,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\"","import { Component } from \"react\";\n\nexport default class BaseComponent extends Component {\n    componentDidMount() {\n        window.scrollTo(0, 0);\n    }\n}","export default __webpack_public_path__ + \"static/media/resnetgru1.8940d720.png\";","export default __webpack_public_path__ + \"static/media/resnetgru2.57e9661d.png\";","export default __webpack_public_path__ + \"static/media/resnetgru3.2cd47887.png\";","export default __webpack_public_path__ + \"static/media/resnetgru4.49f870f3.png\";","export default __webpack_public_path__ + \"static/media/resnetgru5.44b320fd.png\";","export default __webpack_public_path__ + \"static/media/resnetgru6.652db087.png\";","export default __webpack_public_path__ + \"static/media/resnetgru7.4cab73d5.png\";","export default __webpack_public_path__ + \"static/media/resnetgru8.e78d3c08.png\";","export default __webpack_public_path__ + \"static/media/resnetgrutable1.95a707de.png\";","export default __webpack_public_path__ + \"static/media/resnetgrutable2.1dd9b81d.png\";","import React from \"react\";\nimport BaseComponent from './BaseComponent'\nimport resnetgru1 from './images/resnetgru/resnetgru1.png'\nimport resnetgru2 from './images/resnetgru/resnetgru2.png'\nimport resnetgru3 from './images/resnetgru/resnetgru3.png'\nimport resnetgru4 from './images/resnetgru/resnetgru4.png'\nimport resnetgru5 from './images/resnetgru/resnetgru5.png'\nimport resnetgru6 from './images/resnetgru/resnetgru6.png'\nimport resnetgru7 from './images/resnetgru/resnetgru7.png'\nimport resnetgru8 from './images/resnetgru/resnetgru8.png'\nimport resnetgrutable1 from './images/resnetgru/resnetgrutable1.png'\nimport resnetgrutable2 from './images/resnetgru/resnetgrutable2.png'\nimport \"./ResnetGNU.css\";\n\nexport default class ResnetGNU extends BaseComponent {\n    render() {\n        return (\n\n            <div id='outer_container'>\n                <div id='container'>\n                <h2>Resnet-GRU model</h2>\n                <p>Our first model was an attempt to create a baseline for the problem at hand. The model was based on a pre-trained Resnet_34 network which was extended using a bi-directional GRU.\n    The input dataset consisted of input images, target positions of the agent vehicle as well as the ego vehicle. It also included target availability for the agent and ego vehicle. The input also provided us with historical positions of both sets of vehicles. For our model we used the input image of the agent as input and trained it on its given target positions. We optimized the model by calculating losses on target availability. The input number of channels was given by the number of historical positions/frames we wanted to consider. \n    For our scenario we considered 10 historical frames which gave us the number of input channels as 22 because we considered 10 for each agent and ego vehicle and also included the current frame for both of them.\n    The Resnet network was fed input images of the agent vehicle frame and the number of input channels were 22 as described above. The output from the resnet was of shape (bathc_size, 512). For our experiments we considered batch_size to be 16.\n    The output from the Resnet was further fed to the bi-directional GRU. The hidden size for the network was chosen to be 2048. The gru layer gave output with shape ((batch_size, 1, 512), 2*hidden_size). The output features from this layer of GRU was then further fed to a sequential network consisting of a linear layer followed by a RElu and another linear layer and generated output with 1024 features. The hidden state from the previous GRU layer and the output features from the sequential network was fed to another bi-directional GRU layer and a similar cycle was followed.\n    The GRU layer returned an output with 1024 features which was fed to a linear network to finally get the desired target output features.\n    The total number of targets considered of the number of future positions we wanted to predict * the number of trajectories we wished to predict. In our scenario we decided to predict 50 future positions for 3 trajectories as kaggle was evaluating the results with the same. We also chose to predict confidences for 3 trajectories given the high uncertainty of real time traffic. Each future position had X and Y coordinate values, hence total number of target became number_of_trajectories * num_of_future_positons * 2 + confidences_for_3_trajectories i.e (3*50*2 + 3 = 303).\n    Once we had the desired output, we split it into future positions (300) and confidences (3). The confidences were further transformed to probabilities using softmax. The final predicted outputs had shape (batch_size,num_of_trajectories,50,2) and confidences had shape (batch_size, num_of_trajectories). The model and output generation process described above is represented diagrammatically below.\n    </p>\n            <div style={{textAlign: 'center'}}>\n                <img src={resnetgru1}></img>\n                <p>Fig 1. Model journey </p>\n                <img src={resnetgru2}></img>\n                <p>Fig 2. Output generation</p>\n            </div>\n                <p>For our experiments we executed our model multiple times on the training data to improve its performance. One of the critical things we ensured in this methodology was to use the trained model and latest optimizer state for the next cycle of training. Each cycle of training consisted of 12000 iterations. We generated our predictions from test data after each such cycle to observe the models performance. The predictions were evaluated by Kaggle using a negative log likelihood function. Below are the details of the three rounds of experiments conducted by us.\n                </p>\n            <div style={{textAlign: 'center'}}>\n                <img src={resnetgrutable1} class='imagetable'></img>\n                <br></br>\n                <img src={resnetgru3}></img>\n                <p>Fig 3. Cycle 1 average training loss curve</p>\n                <img src={resnetgru4}></img>\n                <p>Fig 4. Cycle 2 average training loss curve</p>\n                <img src={resnetgru5}></img>\n                <p>Fig 5. Cycle 3 average training loss curve</p>\n                <p>Prediction loss on test dataset evaluated by Kaggle:</p>\n                <img src={resnetgru6}></img>\n                <p>Fig 6. Best test result from kaggle submission</p>\n                <p>We also tried to train our model using a different loss function to compare the model’s performance</p>\n                <img src={resnetgrutable2} class='imagetable'></img>\n                <p>Table 2. Experiment details using MSE and rMSE loss function</p>\n                <img src={resnetgru7}></img>\n                <p>Fig 7. Training and Validation loss for MSE</p>\n                <img src={resnetgru8}></img>\n                <p>Fig 8. Training loss for rMSE</p>\n            </div>\n            <h4>Prediction trajectory visualization</h4>\n            <p>Every agent is identified by its track id and timestamp. In our submission file we have every record with timestamp and track id as index. For comparing the results generated by each of our models, we have compared trajectories predicted for the same agent. For my prediction I have used 3 modes or 3 prediction trajectories. Each trajectory holds 50 two dimensional (X,Y) predictions and each trajectory will have its own confidence. The sum of all three confidences will sum up to 1.\n    In the given visualization, we compared predicted trajectories for an agent with track id “18431”. We can observe that trajectory 3 represented by color green is dominant and as the model improves it reaffirms trajectory 3 with the highest confidence of 0.943. The visualizations are represented in both semantic and satellite view.</p>\n                </div>\n            </div>\n        )\n    }\n}","import React from \"react\";\nimport BaseComponent from './BaseComponent'\n\nimport \"./Seq2Seq.css\";\n\nexport default class Seq2Seq extends BaseComponent {\n    render() {\n        return (\n            <div id='outer_container'>\n                <div id='container'>\n                    Detailed information of Seq2Seq\n                </div>\n            </div>\n        )\n    }\n}","import React from \"react\";\nimport BaseComponent from './BaseComponent'\nimport \"./LSTM.css\";\n\nexport default class LSTM extends BaseComponent {\n    render() {\n        return (\n            <div id='outer_container'>\n                <div id='container'>\n                    {\"Detailed information of LSTM\"}\n                </div>\n            </div>\n        )\n    }\n}","export default __webpack_public_path__ + \"static/media/vae1.510ea3d0.png\";","export default __webpack_public_path__ + \"static/media/vaetable1.2d8c7ebf.png\";","export default __webpack_public_path__ + \"static/media/vae2.5e26c15a.png\";","export default __webpack_public_path__ + \"static/media/vae3.729b6eb4.png\";","export default __webpack_public_path__ + \"static/media/vae4.b7fa9d7f.png\";","export default __webpack_public_path__ + \"static/media/vae5.dd1d9795.png\";","export default __webpack_public_path__ + \"static/media/vae6.794464c8.png\";","export default __webpack_public_path__ + \"static/media/vae7.9b7e52c9.png\";","export default __webpack_public_path__ + \"static/media/vae8.750d2809.png\";","export default __webpack_public_path__ + \"static/media/vae9.02e156ca.png\";","export default __webpack_public_path__ + \"static/media/vaea.667bf8ac.png\";","export default __webpack_public_path__ + \"static/media/vaeb.04767882.png\";","import React from \"react\";\nimport BaseComponent from './BaseComponent'\nimport \"./VAELSTM.css\";\nimport vae1 from './images/vaelstm/vae1.png'\nimport vaetable1 from './images/vaelstm/vaetable1.png'\nimport vae2 from './images/vaelstm/vae2.png'\nimport vae3 from './images/vaelstm/vae3.png'\nimport vae4 from './images/vaelstm/vae4.png'\nimport vae5 from './images/vaelstm/vae5.png'\nimport vae6 from './images/vaelstm/vae6.png'\nimport vae7 from './images/vaelstm/vae7.png'\nimport vae8 from './images/vaelstm/vae8.png'\nimport vae9 from './images/vaelstm/vae9.png'\nimport vaea from './images/vaelstm/vaea.png'\nimport vaeb from './images/vaelstm/vaeb.png'\n\nexport default class VAELSTM extends BaseComponent {\n    render() {\n        return (\n            <div id='outer_container'>\n                <div id='container'>\n                    <h2>VAE+LSTM</h2>\n                    <p>This model follows the variational model architecture seen in class using LSTM layers to learn the probability distribution of the trajectories and as a result output similar trajectory within the same distribution.</p> \n                    <p>We present two configurations, Configuration A, uses a hidden layer and splits the hidden layer into two tensors mean and standard deviation. The output of parametrization trick is concatenated with the cell state and feeds the decoder to output next n positions.</p>\n                    <p>Configuration B uses both hidden layer and cell state in two parametrization phases and the output is concatenated and is fed to the decoder to output next n positions.</p>\n                    <div style={{textAlign: 'center'}}><img src={vae1}/></div>\n                    <p>The following table shows the losses using MSE as criterion achieved after 10000 iterations, number of history positions was 10, predicted positions 50 and beta = 0.2 for VAE.</p>\n                    <div style={{textAlign: 'center'}}><img scr={vaetable1}/></div>\n                    <div style={{textAlign: 'center'}}><img class='loss' src={vae2}/><p>Configuration A test loss</p></div>\n                    <div style={{textAlign: 'center'}}><img class='loss' src={vae3}/><p>Configuration B test loss</p></div>\n                    <div style={{textAlign: 'center'}}><img class='loss' src={vae4}/><p>Configuration B Reconstruction and KL loss</p></div>\n                    <p>Both configurations produce similar results, total loss curve, reconstruction and kl loss are similar as well.</p>\n                    <p>As it can be appreciated, VAE+LSTM average losses seem to be slightly better than other LSTM models, but not as good as other models like Seq2Seq GAN. Our assumption is that this is because the model is creating in some cases sharper trajectories since the model is not tuned or trained enough to output smooth trajectories for such scenarios and while most trajectories are good approximations when compared to ground truth trajectories in general seem to be a little bit unnatural where Seq2SeqGAN model with the help of the generator seems to output more natural (real) future predictions.</p>\n                    <p>In the following example we can see the predicted positions for an agent. Here we can see that the first 5 positions have sharp changes with respect to the last history positions, but after those positions it seems to project a reasonable trajectory.</p>\n                    <div style={{textAlign: 'center'}}><img src={vae5}/><p>Next 50 calculated positions for random agent</p></div>\n                    <p>Finally, we present random sampled trajectories generated from the agent's test set during evaluation time, as we can see the model generates different predicted trajectories based on the history positions which shows that while some trajectories behavior seem similar the coordinate positions are not the same and as a result there is some diversity on the trajectories generated by this model.</p>\n                    <div style={{textAlign: 'center'}}>\n                        <img class='grid' src={vae6}/>\n                        <img class='grid' src={vae7}/>\n                        <img class='grid' src={vae8}/>\n                        <br></br>\n                        <img class='grid' src={vae9}/>\n                        <img class='grid' src={vaea}/>\n                        <img class='grid' src={vaeb}/>\n                    </div>\n                </div>\n            </div>\n        )\n    }\n}","import React from \"react\";\nimport BaseComponent from './BaseComponent'\nimport \"./Seq2SeqGAN.css\";\n\nexport default class Seq2SeqGAN extends BaseComponent {\n    render() {\n        return (\n            <div id='outer_container'>\n                <div id='container'>\n                    Detailed information of Seq2SeqGAN\n                </div>\n            </div>\n        )\n    }\n}","import React from \"react\";\nimport BaseComponent from './BaseComponent'\nimport \"./SocialLSTM.css\";\n\nexport default class SocialLSTM extends BaseComponent {\n    render() {\n        return (\n            <div id='outer_container'>\n                <div id='container'>\n                    Detailed information of SocialLSTM\n                </div>\n            </div>\n        )\n    }\n}","import React, { Component } from \"react\";\nimport \"./App.css\";\nimport Header from \"./Header\";\nimport { Route, Switch, Redirect } from \"react-router-dom\";\nimport \"./App.css\";\nimport MainContainer from \"./MainContainer\";\nimport ResnetGNU from './ResnetGNU';\nimport Seq2Seq from './Seq2Seq';\nimport LSTM from './LSTM';\nimport VAELSTM from './VAELSTM';\nimport Seq2SeqGAN from './Seq2SeqGAN';\nimport SocialLSTM from './SocialLSTM';\n\nfunction App() {\n    return (\n        <section key='app'>\n            <Header/>\n            <Switch>\n            <Route exact\n                    path=\"/home\"\n                    component={() => <MainContainer key='home' page='home'/>}\n            />\n            <Route exact\n                    path=\"/resnet-gru\"\n                    component={() => <ResnetGNU key='resnet-gru' page='resnet-gru'/>}\n            />\n            <Route exact\n                    path=\"/lstm\"\n                    component={() => <LSTM key='lstm' page='lstm'/>}\n            />\n            <Route exact\n                    path=\"/seq2seq\"\n                    component={() => <Seq2Seq key='seq2seq' page='seq2seq'/>}\n            />\n            <Route exact\n                    path=\"/vae-lstm\"\n                    component={() => <VAELSTM key='vae-lstm' page='vae-lstm'/>}\n            />\n            <Route exact\n                    path=\"/seq2seqGAN\"\n                    component={() => <Seq2SeqGAN key='seq2seqGAN' page='seq2seqGAN'/>}\n            />\n            <Route exact\n                    path=\"/s-lstm\"\n                    component={() => <SocialLSTM LSTM key='s-lstm' page='s-lstm'/>}\n            />\n            <Redirect from=\"/\" to=\"/home\" />\n            </Switch>\n        </section>\n    );\n  \n};\n\nexport default App;","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\nimport {HashRouter as Router} from \"react-router-dom\";\n\nReactDOM.render(\n  <React.StrictMode>\n    <Router><App /></Router>\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();"],"sourceRoot":""}