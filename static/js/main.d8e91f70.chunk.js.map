{"version":3,"sources":["Header.js","images/main/Motivation1.png","images/main/SDCAR.gif","images/main/SDCAR2.gif","ProblemStatement.js","Motivation.js","images/main/SLSTM0.png","images/main/SLSTM1.png","images/main/SLSTM2.png","images/main/SLSTM3.png","images/main/GAN1.png","images/main/GAN2.png","images/main/GAN3.png","images/main/GAN5.png","Methods.js","FineTuning.js","ResultsTable.js","images/main/results1.png","images/main/results2.png","images/main/results3.png","images/main/results4.png","images/main/results5.png","images/main/results6.png","Results.js","Conclusion.js","FutureWork.js","PageWithComments.js","MainContainer.js","BaseComponent.js","images/resnetgru/resnetgru1.png","images/resnetgru/resnetgru2.png","images/resnetgru/resnetgru3.png","images/resnetgru/resnetgru4.png","images/resnetgru/resnetgru5.png","images/resnetgru/resnetgru6.png","images/resnetgru/resnetgru7.png","images/resnetgru/resnetgru8.png","images/resnetgru/resnetgrutable1.png","images/resnetgru/resnetgrutable2.png","images/resnetgru/img6.png","ResnetGNU.js","images/main/GAN4.png","images/lstm/LSTM-POS.png","images/lstm/LSTM-CNN.png","images/lstm/LSTM-2CNN.png","images/lstm/LSTM-All.png","images/lstm/LSTM-Loss.png","images/lstm/SeqLSTM-Models.png","images/lstm/SeqLSTM-Bar.png","LSTM+Seq2Seq.js","images/lstm/LSTM-Bar.png","images/vaelstm/vae1.png","images/vaelstm/vae2.png","images/vaelstm/vae3.png","images/vaelstm/vae4.png","images/vaelstm/vae5.png","images/vaelstm/vae6.png","images/vaelstm/vae7.png","images/vaelstm/vae8.png","images/vaelstm/vae9.png","images/vaelstm/vaea.png","images/vaelstm/vaeb.png","images/vaelstm/vaec.png","VAELSTM.js","images/main/GAN6.png","images/main/GAN7.png","Seq2SeqGAN.js","images/main/slstmcode.png","images/main/slstmtrainloss.png","images/main/slstmtestloss.png","SocialLSTM.js","App.js","reportWebVitals.js","index.js"],"names":["Header","props","Navbar","collapseOnSelect","bg","variant","expand","Brand","Toggle","aria-controls","Collapse","id","Nav","className","activeKey","Link","eventKey","as","RouterNavLink","to","Component","withRouter","ProblemStatement","alt","src","Motivation1","class","SDCAR","SDCAR2","Motivation","Methods","style","textAlign","GAN1","GAN2","GAN3","width","tableLayout","maxWidth","GAN5","SLSTM1","SLSTM0","SLSTM2","SLSTM3","FineTuning","ResultsTable","Results","Container","Figure","Row","Image","results1","Caption","results2","results3","results4","results5","results6","Conclusion","href","FutureWork","PageWithComments","this","removeCommentBox","commentBox","defaultBoxId","articleId","React","MainContainer","BaseComponent","window","scrollTo","ResnetGNU","resnetgru1","resnetgru2","resnetgrutable1","resnetgru3","resnetgru4","resnetgru5","resnetgru6","resnetgrutable2","resnetgru7","resnetgru8","resnetgru10","m_w","LSTM","l1","l2","l3","l4","l5","l7","l8","VAELSTM","vae1","vaec","vae2","vae3","vae4","vae5","vae6","vae7","vae8","vae9","vaea","vaeb","Seq2SeqGAN","GAN4","rowSpan","colspan","GAN6","GAN7","SocialLSTM","slstmcode","slstmtrainloss","slstmtestloss","App","exact","path","component","page","from","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"ydAQMA,E,kDAEF,WAAYC,GAAQ,uCACVA,G,qDAIN,OACI,iCACK,eAACC,EAAA,EAAD,CACGC,kBAAgB,EAChBC,GAAG,OACHC,QAAQ,OACRC,OAAO,KAJV,UAKG,cAACJ,EAAA,EAAOK,MAAR,8DACA,sBACA,cAACL,EAAA,EAAOM,OAAR,CACIC,gBAAc,qBAClB,eAACP,EAAA,EAAOQ,SAAR,CAAiBC,GAAG,mBAApB,UACI,eAACC,EAAA,EAAD,CAAKC,UAAU,sBAAsBC,UAAU,IAA/C,UACI,cAACF,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,QAHP,kBAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,cAHP,wBAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,gBAHP,0BAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,YAHP,sBAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,cAHP,wBAMA,cAACP,EAAA,EAAIG,KAAL,CACIC,SAAS,IACTC,GAAIC,IACJC,GAAG,UAHP,4BAOJ,cAACjB,EAAA,EAAOK,MAAR,yC,GAzDHa,aAiENC,cAAWrB,GCzEX,MAA0B,wCCA1B,MAA0B,kCCA1B,MAA0B,mC,MCM1B,SAASsB,IACpB,OACI,gCACI,mDACA,27BAGA,qBAAKX,GAAG,iBAAiBY,IAAI,cAAcC,IAAKC,EAAaC,MAAM,WACnE,sBAAKf,GAAG,2BAAR,UACI,qBAAKY,IAAI,WAAWC,IAAKG,IACzB,qBAAKJ,IAAI,WAAWC,IAAKI,UCZ1B,SAASC,IACpB,OACI,gCACI,4CACA,wsCCRG,mzRCAA,MAA0B,mCCA1B,MAA0B,mCCA1B,MAA0B,mCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iCCA1B,MAA0B,iCCY1B,SAASC,IACpB,OACI,gCACI,yCACA,0LACS,cAAC,IAAD,CAAMX,GAAG,cAAT,SAAuB,6CADhC,QAC6D,cAAC,IAAD,CAAMA,GAAG,gBAAT,SAAyB,oDADtF,gFACkM,cAAC,IAAD,CAAMA,GAAG,YAAT,SAAqB,2CADvN,QACkP,cAAC,IAAD,CAAMA,GAAG,cAAT,SAAuB,8CADzQ,6BAC4T,cAAC,IAAD,CAAMA,GAAG,UAAT,SAAmB,8CAD/U,8DAEA,gMACA,4BAAG,4BAAG,8BAAG,cAAC,IAAD,CAAMA,GAAG,cAAcN,UAAU,OAAjC,yBAAH,0BACN,yBAAQkB,MAAO,CAACC,UAAW,UAA3B,UAAsC,qBAAKR,IAAKS,EAAMP,MAAM,aAAiB,gFAC7E,yBAAQK,MAAO,CAACC,UAAW,UAA3B,UAAsC,qBAAKR,IAAKU,EAAMR,MAAM,aAAY,mFACxE,iIAAmG,6CAAnG,6PAEA,mIACA,yBAAQK,MAAO,CAACC,UAAW,UAA3B,UAAsC,qBAAKR,IAAKW,EAAMT,MAAM,aAAY,uGACxE,4JACA,wBAAOK,MAAO,CAACK,MAAM,MAAOC,YAAa,SAAzC,UACI,2FACA,+BACI,wCACA,wCACA,mDAEJ,+BACI,qDACA,mCACA,2CAEJ,+BACI,6BAAI,sDACJ,6BAAI,oCACJ,6BAAI,+CAGZ,6VACA,yBAAQN,MAAO,CAACC,UAAW,UAA3B,UAAsC,qBAAKD,MAAO,CAACO,SAAS,OAAQd,IAAKe,IAAO,wGAChF,mKAAqI,uBAArI,iGACA,uBAEA,4BAAG,4BAAG,8BAAG,cAAC,IAAD,CAAMpB,GAAG,UAAUN,UAAU,OAA7B,yBAAH,0BACN,yBAAQkB,MAAO,CAACC,UAAW,UAA3B,UACI,qBAAKR,IAAKgB,EAAQd,MAAM,UACxB,yEAGJ,2PACA,qBAAKK,MAAO,CAACC,UAAW,UAAxB,SAAmC,qBAAKR,IAAKiB,MAC7C,uBACA,mFACI,+BACI,6MACA,2NACA,0SAJR,8FAOI,+BACI,+IACA,mWACA,oHAVR,2FAcA,yBAAQV,MAAO,CAACC,UAAW,UAA3B,UAAsC,qBAAKR,IAAKkB,EAAQhB,MAAM,UAAS,0EACvE,uPACA,qBAAKK,MAAO,CAACC,UAAW,UAAxB,SACI,qBAAKR,IAAKmB,EAAQjB,MAAM,cAE5B,qIC3EG,SAASkB,IACpB,OACI,gCACI,6CACA,8BAAG,4BAAG,6EAAqD,uBAA3D,wbAEA,8BAAG,4BAAG,6EAAqD,uBAA3D,0qBCNG,SAASC,IACpB,OACI,8BACI,wBAAOd,MAAO,CAACK,MAAO,QAASC,YAAa,SAA5C,UACI,yFACA,+BACI,6BAAI,yCACJ,6BAAI,+CAER,+BACI,mDACA,2CAEJ,+BACI,6BAAI,iDACJ,6BAAI,4CAER,+BACI,2CACA,2CAEJ,+BACI,mDACA,2CAEJ,+BACI,mDACA,2CAEJ,+BACI,qDACA,2CAEJ,+BACI,2CACA,2CAEJ,+BACI,qDACA,2CAEJ,+BACI,4CACA,2CAEJ,+BACI,qDACA,2CAEJ,+BACI,4CACA,2CAEJ,+BACI,6BAAI,sDACJ,6BAAI,iD,2BC1DT,MAA0B,qCCA1B,MAA0B,qCCA1B,MAA0B,qCCA1B,MAA0B,qCCA1B,MAA0B,qCCA1B,MAA0B,qCCa1B,SAASS,IACpB,OACI,gCACI,yCACA,kRACA,eAACC,EAAA,EAAD,CAAWhB,MAAO,CAACC,UAAU,UAA7B,UACI,eAACgB,EAAA,EAAD,WACI,eAACC,EAAA,EAAD,CAAKlB,MAAO,CAACC,UAAU,UAAvB,UACI,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAc1B,IAAK2B,IAAW,cAACH,EAAA,EAAOI,QAAR,qCACtC,eAACJ,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAc1B,IAAK6B,IAAW,cAACL,EAAA,EAAOI,QAAR,qCACtC,eAACJ,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAc1B,IAAK8B,IAAW,cAACN,EAAA,EAAOI,QAAR,wCAE1C,cAACJ,EAAA,EAAOI,QAAR,mEAEJ,eAACJ,EAAA,EAAD,WACI,eAACC,EAAA,EAAD,WACI,eAACD,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAc1B,IAAK+B,IAAW,cAACP,EAAA,EAAOI,QAAR,qCACtC,eAACJ,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAc1B,IAAKgC,IAAW,cAACR,EAAA,EAAOI,QAAR,qCACtC,eAACJ,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAc1B,IAAKiC,IAAW,cAACT,EAAA,EAAOI,QAAR,wCAE1C,cAACJ,EAAA,EAAOI,QAAR,wEAGR,uBACA,uOACA,cAACP,EAAD,IAEA,oaACA,oFACI,+BACI,sJACA,oKAIR,sFACI,+BACI,6NACA,2PACA,8SACA,kgBAIR,6NCtDG,SAASa,IACpB,OACI,gCACI,4CACA,uSAAoQ,uBAApQ,woBAEA,uDAAyB,mBAAGC,KAAK,qEAAR,yBCNtB,SAASC,IACpB,OACI,gCACI,6CACA,mQACA,0EACI,+BACI,mHACA,sHAIR,8JACI,+BACI,kGACA,oGACA,oHAJR,oJASA,gFACI,6BAAI,kMAGR,4SACA,+BACI,0PACA,kMACA,wGAEJ,sW,qBC/BSC,E,kLAEbC,KAAKC,iBAAmBC,IAAW,wBAAyB,CAAEC,aAAcH,KAAK7D,MAAMiE,c,6CAKvFJ,KAAKC,qB,+BAKL,OACI,qBAAKlD,UAAU,mB,GAbmBsD,IAAM/C,WCQrC,SAASgD,IACpB,OACI,qBAAKzD,GAAG,kBAAR,SACI,sBAAKA,GAAG,YAAR,UACI,cAACW,EAAD,IACA,cAACO,EAAD,IACA,cAACC,EAAD,IACA,cAACc,EAAD,IACA,cAACE,EAAD,IACA,cAACc,EAAD,IACA,cAACF,EAAD,IACA,cAAC,EAAD,S,ICpBKW,E,kLAEbC,OAAOC,SAAS,EAAG,O,GAFgBnD,aCF5B,MAA0B,uCCA1B,MAA0B,uCCA1B,MAA0B,uCCA1B,OAA0B,uCCA1B,OAA0B,uCCA1B,OAA0B,uCCA1B,OAA0B,uCCA1B,OAA0B,uCCA1B,OAA0B,4CCA1B,OAA0B,4CCA1B,OAA0B,iCCepBoD,I,6KAEb,OAEI,qBAAK7D,GAAG,kBAAR,SACI,sBAAKA,GAAG,YAAR,UACA,kDACA,m8BAEuM,uBAFvM,g8BAK6H,uBAL7H,m9BAQJ,sBAAKoB,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKR,IAAKiD,IACV,sDACA,qBAAKjD,IAAKkD,IACV,4DAEA,olBAEJ,sBAAK3C,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKR,IAAKmD,GAAiBjD,MAAM,eACjC,uBACA,qBAAKF,IAAKoD,IACV,2EACA,qBAAKpD,IAAKqD,KACV,2EACA,qBAAKrD,IAAKsD,KACV,2EACA,qFACA,qBAAKtD,IAAKuD,KACV,+EACA,yIACA,qBAAKvD,IAAKwD,GAAiBtD,MAAM,eACjC,6FACA,qBAAKF,IAAKyD,KACV,4EACA,qBAAKzD,IAAK0D,KACV,iEAEJ,qEACA,kSAAoQ,uBAApQ,uOACR,uBADQ,4VAEI,qBAAK1D,IAAK2D,c,GA9Cad,ICfxB,OAA0B,iCCA1B,I,YAAA,IAA0B,sCCA1B,OAA0B,qCCA1B,OAA0B,sCCA1B,OAA0B,qCCA1B,OAA0B,sCCA1B,OAA0B,2CCA1B,OAA0B,wCCYnCe,GAAM,MAESC,G,uKAEb,OACE,qBAAK1E,GAAG,kBAAR,SACI,sBAAKA,GAAG,YAAR,UACI,6DACA,2jBACA,sBAAKoB,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKR,IAAK8D,GAAIvD,MAAO,CAACO,SAAU8C,MAAQ,yEAC3E,msCACA,sBAAKrD,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKR,IAAK+D,GAAIxD,MAAO,CAACO,SAAU8C,MAAO,qDAC1E,sBAAKrD,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKR,IAAKgE,GAAIzD,MAAO,CAACO,SAAU8C,MAAO,qDAC1E,ogBACA,sBAAKrD,MAAO,CAACC,UAAW,UAAxB,UAAmC,qBAAKR,IAAKiE,GAAI1D,MAAO,CAACO,SAAU8C,MAAO,oEAC1E,iiBACE,qBAAKrD,MAAO,CAACC,UAAW,UAAxB,SAAmC,qBAAKN,MAAM,OAAOF,IAAKkE,GAAI3D,MAAO,CAACO,SAAU,SAChF,qBAAKP,MAAO,CAACC,UAAW,UAAxB,SAAmC,qBAAKN,MAAM,OAAOF,IC7B1D,ywZD6BmEO,MAAO,CAACO,SAAU,SAClF,4vBACA,qBAAKP,MAAO,CAACC,UAAW,UAAxB,SAAmC,qBAAKN,MAAM,OAAOF,IAAKmE,GAAI5D,MAAO,CAACK,MAAO,WAC7E,0mBACA,qBAAKL,MAAO,CAACC,UAAW,UAAxB,SAAmC,qBAAKN,MAAM,OAAOF,IAAKoE,GAAI7D,MAAO,CAACO,SAAU,kB,GAnBhE+B,GEdnB,I,MAAA,IAA0B,kCCA1B,OAA0B,iCCA1B,OAA0B,iCCA1B,OAA0B,iCCA1B,OAA0B,iCCA1B,OAA0B,iCCA1B,OAA0B,iCCA1B,OAA0B,iCCA1B,OAA0B,iCCA1B,OAA0B,iCCA1B,OAA0B,iCCA1B,OAA0B,iCCiBpBwB,G,uKAEb,OACI,qBAAKlF,GAAG,kBAAR,SACI,sBAAKA,GAAG,YAAR,UACI,0CACA,wPACA,wSACA,yMACA,qBAAKoB,MAAO,CAACC,UAAW,UAAxB,SACI,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,OAAOF,IAAKsE,KAAO,cAAC9C,EAAA,EAAOI,QAAR,6CAGnD,uBACA,mOACA,wBAAOrB,MAAO,CAACK,MAAO,OAAtB,UACI,gCACI,+BACI,+CACA,oDACA,+CACA,+CACA,4CACA,wDAGR,+BACI,mCACA,2HACA,yDACA,sCACA,uCACA,2CAEJ,+BACI,6BAAI,oCACJ,6BAAI,4HACJ,6BAAI,0DACJ,6BAAI,uCACJ,6BAAI,wCACJ,6BAAI,+CAGZ,uBAEA,qBAAKL,MAAO,CAACC,UAAW,UAAxB,SAAmC,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,OAAOF,IAAKuE,KAAO,cAAC/C,EAAA,EAAOI,QAAR,gFAClF,qBAAKrB,MAAO,CAACC,UAAW,UAAxB,SAAmC,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,OAAOF,IAAKwE,KAAO,cAAChD,EAAA,EAAOI,QAAR,uDAClF,qBAAKrB,MAAO,CAACC,UAAW,UAAxB,SAAmC,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,OAAOF,IAAKyE,KAAO,cAACjD,EAAA,EAAOI,QAAR,uDAClF,qBAAKrB,MAAO,CAACC,UAAW,UAAxB,SAAmC,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,OAAOF,IAAK0E,KAAO,cAAClD,EAAA,EAAOI,QAAR,6EAClF,+IACA,inBACA,2RACA,qBAAKrB,MAAO,CAACC,UAAW,UAAxB,SAAmC,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAe1B,IAAK2E,KAAO,cAACnD,EAAA,EAAOI,QAAR,2EACtE,+aACA,sBAAKrB,MAAO,CAACC,UAAW,UAAxB,UACI,eAACgB,EAAA,EAAD,WACI,qBAAKtB,MAAM,OAAOF,IAAK4E,KACvB,qBAAK1E,MAAM,OAAOF,IAAK6E,KACvB,qBAAK3E,MAAM,OAAOF,IAAK8E,KACvB,uBACA,qBAAK5E,MAAM,OAAOF,IAAK+E,KACvB,qBAAK7E,MAAM,OAAOF,IAAKgF,KACvB,qBAAK9E,MAAM,OAAOF,IAAKiF,QAE3B,cAACzD,EAAA,EAAOI,QAAR,kE,GAhEaiB,GCjBtB,I,MAAA,IAA0B,kCCA1B,OAA0B,iCCWpBqC,G,uKAEb,OACI,qBAAK/F,GAAG,kBAAR,SACI,sBAAKA,GAAG,YAAR,UACI,mEACI,6bAQA,sBAAKoB,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKR,IAAKS,EAAMF,MAAS,CAACO,SAAS,OAAQF,MAAM,SACjD,uBACA,+BACI,8EAMR,sBAAKL,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKR,IAAKU,EAAMH,MAAS,CAACO,SAAS,OAAQF,MAAM,SACjD,uBACA,+BACI,kFAKR,uBAEA,ylBAKA,sBAAKL,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKR,IAAKW,EAAMJ,MAAS,CAACO,SAAS,OAAQF,MAAM,SACjD,uBACA,+BACI,yFAKR,uBACA,6ZAIR,8BACI,wBAAOL,MAAO,CAACO,SAAU,OAAQD,YAAa,SAA9C,UACI,kCACI,4FAIJ,+BACI,6BAAI,wCACJ,6BAAI,wCACJ,6BAAI,oDAER,+BACI,qDACA,mCACA,2CAEJ,+BACI,6BAAI,sDACJ,6BAAI,oCACJ,6BAAI,iDAIhB,uBACA,sBAAKN,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKR,IAAKmF,GAAM5E,MAAS,CAACO,SAAS,OAAQF,MAAO,SAClD,uBACA,+BACI,uFAKR,uBACA,qfAGA,6CACA,oOAKD,8BACK,wBAAOL,MAAO,CAACO,SAAU,OAAQD,YAAa,SAA9C,UACI,kCACI,kGAIJ,+BACI,6BAAI,gDACJ,6BAAI,wCACJ,6BAAI,oDAER,+BACI,qDACA,mCACA,2CAEJ,+BACI,6BAAI,sDACJ,6BAAI,oCACJ,6BAAI,iDAIhB,0IAGA,0bAGD,8BACK,wBAAON,MAAO,CAACO,SAAU,OAAQD,YAAa,SAA9C,UACI,kCACI,iIAIJ,+BACI,6BAAI,gDACJ,6BAAI,yCACJ,6BAAI,iEACJ,6BAAI,oDAER,+BACI,mDACA,mCACA,qCACA,4CAEJ,+BACI,6BAAI,sDACJ,6BAAI,oCACJ,6BAAI,sCACJ,6BAAI,4CAER,+BACI,mDACA,mCACA,qCACA,4CAEJ,+BACI,6BAAI,sDACJ,6BAAI,oCACJ,6BAAI,sCACJ,6BAAI,iDAIhB,+UAGA,qfAGD,8BACK,wBAAON,MAAO,CAACO,SAAU,OAAQD,YAAa,SAA9C,UACI,kCACI,6HAIJ,+BACI,oBAAID,MAAM,MAAMwE,QAAQ,IAAxB,SAA4B,gDAC5B,oBAAIC,QAAQ,IAAZ,SAAgB,0DAChB,oBAAIzE,MAAM,MAAMwE,QAAQ,IAAxB,SAA4B,oDAEhC,+BACI,6BAAI,sDACJ,6BAAI,yDAGR,+BACI,sDACA,sCACA,6CACA,2CAGJ,+BACI,6BAAI,uDACJ,6BAAI,6CACJ,6BAAI,0CACJ,6BAAI,4CAER,+BACI,qDACA,sCACA,6CACA,2CAEJ,+BACI,6BAAI,sDACJ,6BAAI,6CACJ,6BAAI,0CACJ,6BAAI,iDAIhB,oJAGA,yjBAGD,8BACK,wBAAO7E,MAAO,CAACO,SAAU,OAAQD,YAAa,SAA9C,UACI,kCACI,6GAIJ,+BACI,6BAAI,gDACJ,6BAAI,qEACJ,6BAAI,oDAER,+BACI,qDACA,sCACA,2CAGJ,+BACI,qDACA,sCACA,2CAEJ,+BACI,6BAAI,sDACJ,6BAAI,uCACJ,6BAAI,iDAIhB,iJAGA,sBAAKN,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKR,IAAKsF,GAAM/E,MAAS,CAACO,SAAU,OAAQF,MAAO,SACnD,uBACA,+BACI,kGAKR,uBACA,ySAGA,8DACA,mWAGA,sBAAKL,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKR,IAAKe,EAAMR,MAAS,CAACO,SAAU,OAAQF,MAAO,SACnD,uBACA,+BACI,uGAKR,uBACA,sBAAKL,MAAO,CAACC,UAAW,UAAxB,UACI,qBAAKR,IAAKuF,GAAMhF,MAAS,CAACO,SAAU,OAAQF,MAAO,SACnD,uBACA,+BACI,8HAKR,uBACA,mLAGA,2IAIA,8BACI,wBAAOL,MAAO,CAACO,SAAU,OAAQD,YAAa,SAA9C,UACI,kCACI,2EAIJ,+BACI,oBAAID,MAAM,MAAMwE,QAAQ,IAAxB,SAA4B,gDAC5B,oBAAIxE,MAAM,KAAKwE,QAAQ,IAAvB,SAA2B,4CAC3B,oBAAIxE,MAAM,KAAKwE,QAAQ,IAAvB,SAA2B,6CAC3B,oBAAIC,QAAQ,IAAZ,SAAgB,gDAChB,oBAAIA,QAAQ,IAAZ,SAAgB,8DAChB,oBAAIzE,MAAM,KAAKwE,QAAQ,IAAvB,SAA2B,wCAC3B,oBAAIxE,MAAM,KAAKwE,QAAQ,IAAvB,SAA2B,gDAC3B,oBAAIxE,MAAM,KAAKwE,QAAQ,IAAvB,SAA2B,oDAE/B,+BACI,2CACA,+CACA,qDACA,wDAGJ,+BACI,mDACA,sCACA,oCACA,qCACA,qCACA,sCACA,mCACA,mCACA,sCACA,4CAEJ,+BACI,mDACA,sCACA,oCACA,qCACA,qCACA,sCACA,mCACA,mCACA,sCACA,4CAEJ,+BACI,qDACA,sCACA,oCACA,qCACA,qCACA,sCACA,mCACA,mCACA,sCACA,2CAEJ,+BACI,qDACA,sCACA,oCACA,qCACA,qCACA,4CACA,mCACA,mCACA,sCACA,2CAEJ,+BACI,qDACA,sCACA,oCACA,qCACA,qCACA,4CACA,mCACA,mCACA,sCACA,2CAEJ,+BACI,qDACA,sCACA,oCACA,qCACA,qCACA,sCACA,mCACA,mCACA,sCACA,2CAEJ,+BACI,qDACA,sCACA,oCACA,qCACA,qCACA,4CACA,mCACA,mCACA,sCACA,2CAEJ,+BACI,qDACA,sCACA,oCACA,qCACA,qCACA,4CACA,mCACA,mCACA,sCACA,2CAEJ,+BACI,6BAAI,sDACJ,6BAAI,uCACJ,6BAAI,qCACJ,6BAAI,sCACJ,6BAAI,sCACJ,6BAAI,uCACJ,6BAAI,oCACJ,6BAAI,oCACJ,6BAAI,uCACJ,6BAAI,iDAIhB,sEACyC,uBADzC,oDAEqD,uBAFrD,0CAG2C,uBAH3C,0CAI2C,uBAJ3C,qEAKsE,uBALtE,uEAMwE,uBANxE,uCAOwC,uBAPxC,uCAQwC,uBARxC,uCASwC,0BAExC,6VAGA,8CACA,8MACA,+CACA,8BACI,4HACA,yHACA,4HAEJ,8JACA,8BACI,mGACA,oGACA,oHAEJ,kLACA,sEACI,mBAAGjD,KAAK,gFAAR,gC,GAhdoBU,GCXzB,I,MAAA,IAA0B,uCCA1B,OAA0B,2CCA1B,OAA0B,0CCcpB2C,G,uKAEb,OACI,qBAAKrG,GAAG,kBAAR,SACI,sBAAKA,GAAG,YAAR,UACI,qEACI,qmBAGA,qBAAKoB,MAAQ,CAACC,UAAW,UAAzB,SACI,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,SAASF,IAAKgB,IAAS,cAACQ,EAAA,EAAOI,QAAR,4DAGvD,kRAKA,qBAAKrB,MAAQ,CAACC,UAAW,UAAzB,SACI,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,SAASF,IAAKiB,IAAS,cAACO,EAAA,EAAOI,QAAR,6DAGvD,yEAIA,qBAAKrB,MAAQ,CAACC,UAAW,UAAzB,SACI,cAACgB,EAAA,EAAD,UAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,YAAYF,IAAKyF,SAGjD,mFAEY,+BACI,8MACA,4NACA,2SALhB,+DAQY,+BACI,sJACA,icACA,+BACI,yWACA,0YAEJ,qHAfhB,qEAqBA,qBAAKlF,MAAQ,CAACC,UAAW,UAAzB,SACI,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,SAASF,IAAKkB,IAAS,cAACM,EAAA,EAAOI,QAAR,oEAIvD,6OAKA,qBAAKrB,MAAQ,CAACC,UAAW,UAAzB,SACI,cAACgB,EAAA,EAAD,UAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,SAASF,IAAKmB,QAG9C,8PAKA,qBAAKZ,MAAQ,CAACC,UAAW,UAAzB,SACI,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,iBAAiBF,IAAK0F,KAAiB,cAAClE,EAAA,EAAOI,QAAR,2EAIvE,6JAEgB,+BACI,0PACA,kMACA,wGALpB,iDASgB,6BACI,oOAVpB,2NAiBA,qBAAKrB,MAAQ,CAACC,UAAW,UAAzB,SACI,eAACgB,EAAA,EAAD,WAAQ,cAACA,EAAA,EAAOE,MAAR,CAAcxB,MAAM,gBAAgBF,IAAK2F,KAAgB,cAACnE,EAAA,EAAOI,QAAR,+DAGrE,wR,GAhGgBiB,GCDzB,SAAS+C,KACpB,OACI,oCACI,cAAC,EAAD,IACA,eAAC,IAAD,WACI,cAAC,IAAD,CAAOC,OAAK,EACJC,KAAK,QACLC,UAAW,kBAAM,cAAC,EAAD,CAA0BC,KAAK,QAAZ,WAE5C,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,cACLC,UAAW,kBAAM,cAAC,GAAD,CAA4BC,KAAK,cAAlB,iBAExC,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,gBACLC,UAAW,kBAAM,cAAC,GAAD,CAAiBC,KAAK,QAAZ,WAEnC,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,YACLC,UAAW,kBAAM,cAAC,GAAD,CAAwBC,KAAK,YAAhB,eAEtC,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,cACLC,UAAW,kBAAM,cAAC,GAAD,CAA6BC,KAAK,cAAlB,iBAEzC,cAAC,IAAD,CAAOH,OAAK,EACJC,KAAK,UACLC,UAAW,kBAAM,cAAC,GAAD,CAAYlC,MAAI,EAAcmC,KAAK,UAAd,aAE9C,cAAC,IAAD,CAAUC,KAAK,IAAItG,GAAG,eA3BjB,OCfrB,IAYeuG,GAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,6BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCAdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,IAAD,UAAQ,cAAC,GAAD,QAEVC,SAASC,eAAe,SAM1Bb,O","file":"static/js/main.d8e91f70.chunk.js","sourcesContent":["import React, {Component} from \"react\"\nimport 'bootstrap/dist/css/bootstrap.min.css'\nimport Navbar from 'react-bootstrap/Navbar'\nimport Nav from 'react-bootstrap/Nav'\nimport './Header.css'\nimport { withRouter } from 'react-router-dom'\nimport { NavLink as RouterNavLink } from 'react-router-dom';\n\nclass Header extends Component {\n\n    constructor(props) {\n        super(props)\n    }\n\n    render() {\n        return (\n            <header>\n                 <Navbar\n                    collapseOnSelect\n                    bg=\"dark\"\n                    variant=\"dark\"\n                    expand=\"lg\">\n                    <Navbar.Brand>Lyft5 Motion Prediction for Autonomous Vehicles</Navbar.Brand>\n                    <t/>\n                    <Navbar.Toggle\n                        aria-controls=\"basic-navbar-nav\" />\n                    <Navbar.Collapse id=\"basic-navbar-nav\">\n                        <Nav className='mr-auto nav-section' activeKey='6'>\n                            <Nav.Link\n                                eventKey='0'\n                                as={RouterNavLink}\n                                to=\"/home\">\n                                Home\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='1'\n                                as={RouterNavLink}\n                                to=\"/resnet-gru\">\n                                Resnet-GRU\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='2'\n                                as={RouterNavLink}\n                                to=\"/lstm-seq2seq\">\n                                LSTM+Seq2Seq\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='3'\n                                as={RouterNavLink}\n                                to=\"/vae-lstm\">\n                                VAE+LSTM\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='4'\n                                as={RouterNavLink}\n                                to=\"/seq2seqGAN\">\n                                Seq2SeqGAN\n                            </Nav.Link>\n                            <Nav.Link\n                                eventKey='5'\n                                as={RouterNavLink}\n                                to=\"/s-lstm\">\n                                Social LSTM\n                            </Nav.Link>\n                        </Nav>\n                        <Navbar.Brand>Deep New World</Navbar.Brand>\n                    </Navbar.Collapse>\n                </Navbar>\n            </header>\n        )\n    }\n}\n\nexport default withRouter(Header)","export default __webpack_public_path__ + \"static/media/Motivation1.7da28e5e.png\";","export default __webpack_public_path__ + \"static/media/SDCAR.f1dc928a.gif\";","export default __webpack_public_path__ + \"static/media/SDCAR2.462f0ff0.gif\";","import React from \"react\";\nimport Motivation1 from './images/main/Motivation1.png'\nimport SDCAR from './images/main/SDCAR.gif'\nimport SDCAR2 from './images/main/SDCAR2.gif'\nimport \"./MainContainer.css\";\n\nexport default function ProblemStatement() {\n    return (\n        <div>\n            <h2>Problem Statement</h2>\n            <p>One of the most primitive tasks of autonomous vehicles is to predict the future positions of other vehicles amidst traffic. In this project, we aim to predict the future positions of the other vehicles/agents in a given scene such as cyclists, cars, and pedestrians. \nWe are using Lyft’s l5kit dataset. This dataset contains over 1000 hours of scenes, including labeled positional information of objects such as vehicles, cyclists, and pedestrians. The scenes in the dataset are from a 6.8 mile route chosen by the Lyft team. The input data will be the sequence of spatial coordination about each vehicle (both for train and test) which can also include road information of the given agents environment.  \nThe output will be the future spatial coordination. Given the uncertainty of live traffic, we have also aimed to predict the future positions for multiple trajectories with a level of certainty for each trajectory.</p>\n            <img id='motivation_pic' alt=\"Motivation1\" src={Motivation1} class='center'/>\n            <div id='more_motivation_pics_div'>\n                <img alt=\"Picture2\" src={SDCAR}/>\n                <img alt=\"Picture3\" src={SDCAR2}/>\n            </div>\n        </div>\n    )\n}","import React from \"react\";\n\nimport \"./MainContainer.css\";\n\nexport default function Motivation() {\n    return (\n        <div>\n            <h2>Motivation</h2>\n            <p>Autonomous vehicles are the future of on land mobility for mankind. But, we are still far from seamless autonomous transportation and we need to resolve many significant challenges before we can truly travel autonomously. One of the most important aspects of any autonomous vehicle is its ability to predict the next moves of other agents. The challenge to predict the next move is significant for its success. It's of utmost importance to correctly predict the next position of other vehicles as it will not only determine the direction in which the vehicle should move but will also impact the safety of the passenger and also others on the road. What makes this challenge even more critical is the fact that the next position of the vehicle should be determined keeping in mind not only the surroundings but also the destination and traffic agents. The impact of this solution is enormous in autonomous vehicles being successful and making transportation seamless and safe which has been a driving force behind us choosing to work on this problem statement. It is an exciting opportunity to work on a problem that will solve some pressing real world use cases and make a global impact.</p>\n        </div>\n    )\n}","export default \"data:image/png;base64,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\"","export default __webpack_public_path__ + \"static/media/SLSTM1.53c7ec37.png\";","export default __webpack_public_path__ + \"static/media/SLSTM2.87ffcdf1.png\";","export default __webpack_public_path__ + \"static/media/SLSTM3.0e772885.png\";","export default __webpack_public_path__ + \"static/media/GAN1.a5d75e69.png\";","export default __webpack_public_path__ + \"static/media/GAN2.f236eade.png\";","export default __webpack_public_path__ + \"static/media/GAN3.b1c2e4d2.png\";","export default __webpack_public_path__ + \"static/media/GAN5.bc8a36cc.png\";","import React from \"react\";\nimport { Link } from \"react-router-dom\";\nimport SLSTM0 from './images/main/SLSTM0.png'\nimport SLSTM1 from './images/main/SLSTM1.png'\nimport SLSTM2 from './images/main/SLSTM2.png'\nimport SLSTM3 from './images/main/SLSTM3.png'\nimport GAN1 from './images/main/GAN1.png'\nimport GAN2 from './images/main/GAN2.png'\nimport GAN3 from './images/main/GAN3.png'\nimport GAN5 from './images/main/GAN5.png'\nimport \"./MainContainer.css\";\n\nexport default function Methods() {\n    return (\n        <div>\n            <h2>Methods</h2>\n            <p>To resolve this issue, we have implemented various methods and compared their performance by test loss in the Result section below. \nFirstly, we explored <Link to='/resnet-gru'><b>Resnet-Gru</b></Link> and <Link to='/lstm-seq2seq'><b>LSTM/Seq2Seq LSTM</b></Link> models. Based on the LSTM model, we added VAE and GAN structure and created <Link to='/vae-lstm'><b>VAE+LSTM</b></Link> and <Link to='/seq2seqGAN'><b>Seq2Seq GAN</b></Link>. Finally, we implemented <Link to='/s-lstm'><b>Social LSTM</b></Link> which incorporates the neighbors' effect into the model</p>\n            <p>In this page, we focus on the two most interesting models among these. For the detailed information about other models we created, please check the other tabs.</p>\n            <p><i><b><Link to='/seq2seqGAN' className=\"link\">Seq2Seq GAN</Link> and its variant</b></i></p>\n            <figure style={{textAlign: 'center'}}><img src={GAN1} class='ganmodel'></img><figcaption>Figure 1. Generator of Seq2Seq GAN1</figcaption></figure>\n            <figure style={{textAlign: 'center'}}><img src={GAN2} class='ganmodel'/><figcaption>Figure 2. Discriminator of Seq2Seq GAN</figcaption></figure>\n            <p>To the baseline LSTM model, we added the GAN and created a new model. We call this architecture <b>Seq2Seq GAN.</b> In this model, the generator generates the next 50 moves based on the past 11 positions. The discriminator determines whether it’s real or fake. We expected that this structure can avoid blurry predictions and make more accurate predictions. \n</p>                 \n            <p>To improve Seq2Seq GAN1, we incorporated the past yaw information as well and made a Seq2Seq GAN2.</p>\n            <figure style={{textAlign: 'center'}}><img src={GAN3} class='ganmodel'/><figcaption>Figure 3. Generator of Seq2Seq GAN2 (with yaw information)</figcaption></figure>\n            <p>As you see in Table 1, compared to the Seq2Seq GAN1, the Seq2Seq GAN2 achieved a lower test loss under the same conditions.</p>\n            <table style={{width:'90%', tableLayout: 'fixed'}}>\n                <caption>Table 1. Test Losses of different Seq2Seq GAN Models</caption>\n                <tr>\n                    <td>Models</td>\n                    <td>Epochs</td>\n                    <td>Test Loss(MSE)</td>\n                </tr>\n                <tr>\n                    <td>Seq2Seq GAN1-v2-1-1</td>\n                    <td>1</td>\n                    <td>2.6551</td>\n                </tr>\n                <tr>\n                    <td><b>Seq2Seq GAN2-v2-1-1</b></td>\n                    <td><b>1</b></td>\n                    <td><b>2.2576</b></td>\n                </tr>\n            </table>\n            <p>After training, we tested our Seq2Seq model to check if it doesn’t fall into the mode failure and it can generate diverse predictions for the inputs. We generated one hundred sample predictions for the same batch(32) input and plotted all these generated future trajectories for the randomly-picked four inputs.</p>                \n            <figure style={{textAlign: 'center'}}><img style={{maxWidth:'80%'}} src={GAN5}/><figcaption>Figure 4. Predicted Trajectories from the Seq2Seq GAN Model</figcaption></figure>\n            <p>As you see in Figure 4, the Seq2Seq GAN model does not suffer from mode collapse and successfully generates diverse trajectories. <br></br>More detailed explanation about this Seq2Seq GAN model is available on the other tab above.</p>\n            <br></br>\n\n            <p><i><b><Link to='/s-lstm' className=\"link\">Social LSTM</Link> and its variant</b></i></p>\n            <figure style={{textAlign: 'center'}}>\n                <img src={SLSTM1} class='model'/>\n                <figcaption>Figure 5. Social LSTM method</figcaption>\n            </figure>\n\n            <p>-Based on Alexandre etc’s research, we tried to take the neighbor’s effect into consideration combining with the LSTM model. For this, we added a max-pooling layer between each time stamp, its formulation is:</p>\n            <div style={{textAlign: 'center'}}><img src={SLSTM0}/></div>\n            <br></br>\n            <p>However, this method has the following limitation:\n                <ul>\n                    <li>Simply adding all the neighbor’s information does not make sense because people care more about the neighborhood closer to them than those who are far away from them.</li>\n                    <li>This social LSTM architecture is a generalization method. However, if we can give a more specific classification, it is highly likely that we can improve the prediction’s accuracy.</li>\n                    <li>For the social LSTM architecture, when the agent falls into a grid, it will be considered as a neighborhood. For the architecture, the grid we considered is a square. It seems unreasonable because agents move faster in the speed’s direction than the other.</li>\n                </ul>\n                Based on these two observations, we modified the current social-LSTM architecture as below:\n                <ul>\n                    <li>Introducing the spatial information max-pooling part, which can be realized by using the convolutional layer.</li>\n                    <li>In the real world, when a car moves, it only has two classes: longitude and latitude. For the longitude classes, we split into 3 situations: speed up, normal and slow down; turning left, staying the same and turning right are these 3 situations for the latitude classes. More details are available on the Social LSTM tab.</li>\n                    <li>For the grid, instead of considering the square, tried to consider rectangular.</li>\n                </ul>\n                As a result of the modifications, we created a Social LSTM variant model as Figure 6.\n            </p>\n            <figure style={{textAlign: 'center'}}><img src={SLSTM2} class='model'/><figcaption>Figure 6. Social-LSTM variant</figcaption></figure>\n            <p>For this model, we slightly changed our goal from directly predicting the future trajectory to maximizing the probability of prediction and returning the one with the highest probability. The objective function is:</p>\n            <div style={{textAlign: 'center'}}>\n                <img src={SLSTM3} class='formula'/>\n            </div>\n            <p>More detailed explanation about this Social LSTM model is available on the Social LSTM tab above.</p>\n        </div>\n    )\n}","import React from \"react\";\nimport \"./MainContainer.css\";\n\nexport default function FineTuning() {\n    return (\n        <div>\n            <h2>Fine Tuning</h2>\n            <p><i><b>Seq2Seq GAN and Seq2Seq GAN variant models</b></i><br></br>\n            Based on the two Seq2Seq GAN models, we tried parameter-tuning by changing learning rates(1e-3, 5e-3, 1e-4), loss functions(BCE, MSE), and the layers on the discriminators. So far, the best Seq2Seq GAN Model among these is the Seq2Seq GAN-v1 with a learning rate of 0.001, batch size of 32, MSE loss function on the generator, and epochs of 2 and the test loss of this model is 1.9143. More detailed information is on the Seq2Seq tab.</p>\n            <p><i><b>Social-LSTM and social-LSTM variant Models</b></i><br></br>\n            For social-LSTM and its variants, there exist multiple parameters that we can tune.  For both models, we tried six different learning rates(1e-3, 3e-3, 5e-3, 8e-3, 1e-2, 2e-2) and found that 1e-03 is the best one. For social-LSTM’s neighborhood grid, we tried the grid’s size from (1,1) to (10, 10), it seems that (7,7) gives the minimum loss for the training part. For the variant method, we tried one direction from 1 to 5 and another one is from 7 to 15. Among all these trials, (13, 3) returns the best loss for the training datasets. In addition, for the convolutional layer in the variant method, we tried 3 by 3, 4 by 4 and 5 by 5. At the end, 3 by 3 wins.</p>\n        </div>\n    )\n}","import React from \"react\";\nimport \"./MainContainer.css\";\n\nexport default function ResultsTable() {\n    return (\n        <div>\n            <table style={{width: '300pt', tableLayout: 'fixed'}}>\n                <caption>Table 2. Comparison on the test loss of our models</caption>\n                <tr>\n                    <th><b>Method</b></th>\n                    <th><b>Test loss</b></th>\n                </tr>\n                <tr>\n                    <td>Seq2Seq LSTM - v2</td>\n                    <td>3.6188</td>\n                </tr>\n                <tr>\n                    <td><b>Social LSTM-v2</b></td>\n                    <td><b>3.5578</b></td>\n                </tr>\n                <tr>\n                    <td>LSTM - v2</td>\n                    <td>3.1169</td>\n                </tr>\n                <tr>\n                    <td>Seq2Seq LSTM - v1</td>\n                    <td>2.9981</td>\n                </tr>\n                <tr>\n                    <td>Seq2Seq LSTM - v3</td>\n                    <td>2.2907</td>\n                </tr>\n                <tr>\n                    <td>Seq2Seq GAN2-v2-1-1</td>\n                    <td>2.2576</td>\n                </tr>\n                <tr>\n                    <td>LSTM - v1</td>\n                    <td>2.2572</td>\n                </tr>\n                <tr>\n                    <td>Seq2Seq GAN2-v2-2-1</td>\n                    <td>2.1781</td>\n                </tr>\n                <tr>\n                    <td>VAE+LSTM A</td>\n                    <td>2.1587</td>\n                </tr>\n                <tr>\n                    <td>Seq2Seq GAN1-v2-2-3</td>\n                    <td>2.1463</td>\n                </tr>\n                <tr>\n                    <td>VAE+LSTM B</td>\n                    <td>1.9585</td>\n                </tr>\n                <tr>\n                    <td><b>Seq2Seq GAN1-v2-1-1</b></td>\n                    <td><b>1.9143</b></td>\n                </tr>\n            </table>\n        </div>\n    )\n}","export default __webpack_public_path__ + \"static/media/results1.a9d40824.png\";","export default __webpack_public_path__ + \"static/media/results2.d9bdbf0d.png\";","export default __webpack_public_path__ + \"static/media/results3.20498194.png\";","export default __webpack_public_path__ + \"static/media/results4.5afba3fa.png\";","export default __webpack_public_path__ + \"static/media/results5.ad10f1c9.png\";","export default __webpack_public_path__ + \"static/media/results6.9d8b216c.png\";","import React from \"react\";\nimport ResultsTable from './ResultsTable'\nimport Container from 'react-bootstrap/Container'\nimport Row from 'react-bootstrap/Row'\nimport Figure from 'react-bootstrap/Figure'\nimport results1 from './images/main/results1.png'\nimport results2 from './images/main/results2.png'\nimport results3 from './images/main/results3.png'\nimport results4 from './images/main/results4.png'\nimport results5 from './images/main/results5.png'\nimport results6 from './images/main/results6.png'\nimport \"./MainContainer.css\";\n\nexport default function Results() {\n    return (\n        <div>\n            <h2>Results</h2>\n            <p>After training all of our models, we examined our models performance on the testset and generated the next fifty positions for each agent. Figure 7 and Figure 8 shows the future trajectories predicted by one of our models on top of the maps.</p>\n            <Container style={{textAlign:'center'}}>\n                <Figure>\n                    <Row style={{textAlign:'center'}}>\n                        <Figure><Figure.Image src={results1}/><Figure.Caption>Model after cycle 1</Figure.Caption></Figure>\n                        <Figure><Figure.Image src={results2}/><Figure.Caption>Model after cycle 2</Figure.Caption></Figure>\n                        <Figure><Figure.Image src={results3}/><Figure.Caption>Model after cycle 3</Figure.Caption></Figure>\n                    </Row>\n                    <Figure.Caption>Figure 7. Semantic view of predicted trajectories</Figure.Caption>\n                </Figure>\n                <Figure>\n                    <Row>\n                        <Figure><Figure.Image src={results4}/><Figure.Caption>Model after cycle 1</Figure.Caption></Figure>\n                        <Figure><Figure.Image src={results5}/><Figure.Caption>Model after cycle 2</Figure.Caption></Figure>\n                        <Figure><Figure.Image src={results6}/><Figure.Caption>Model after cycle 3</Figure.Caption></Figure>\n                    </Row>\n                    <Figure.Caption>Figure 8.  Satellite view of predicted trajectories</Figure.Caption>\n                </Figure>\n            </Container>\n            <br></br>\n            <p>We computed MSE test loss against the ground truth trajectories to compare our model’s performances. The following chart shows the average MSE test loss from our the most representative models:</p>\n            <ResultsTable/>\n\n            <p>It is clear that our Seq2Seq Gan architecture achieved the best performance, performing more than 6x better than the baseline model. While the Seq2Seq architecture is often used for text translations, we were interested to see its performance in the trajectory prediction domain. To our surprise, Seq2Seq performed very well, allowing our Seq2Seq GAN model to achieve the best test loss.</p>\n            <p>Here are some more analysis on the compared result:\n                <ul>\n                    <li>While the LSTM and Seq2Seq based LSTM performed well, they were still outperformed by other models like Seq2Seq GAN.</li>\n                    <li>VAE+LSTM average losses seem to be slightly better than other LSTM models, but not as good as other models like Seq2Seq GAN.</li>\n                </ul>\n            </p>\n  \n            <p>From these observations, we can infer the followings:\n                <ul>\n                    <li>While testing with a basic recurrent model like LSTM was definitely a step in the right direction, it is clear that a more powerful recurrent model would be necessary to improve the loss.</li>\n                    <li>About the result of LSTM+VAE, we assume that this is because the model is creating in some cases more sharp trajectories since the model is not tuned or trained enough to output smooth trajectories for such scenarios.</li>\n                    <li>With the LSTM+VAE, while most trajectories are good approximations when compared to ground truth trajectories in general seem to be a little bit unnatural where Seq2SeqGAN model with the help of the discriminator seems to output more natural (real) future predictions.</li>\n                    <li>While Social LSTM was not the best performer, we believe in its potential and that our current architecture or even a slightly modified version can achieve a much lower loss,  especially with more training. Moreover, due to computational constraints, our social LSTM does not differentiate different types of objects (i.e. Car, Cyclist, Pedestrian) when attempting to compute their trajectories. This loss of information could prevent the model from achieving a better loss.</li>\n                </ul>\n            </p>\n\n            <p>Overall, we are happy to report that all of our models perform significantly better than the provided Lyft baseline, with all presented models achieving over 3 times better performance.</p>\n        </div>\n    )\n}","import React from \"react\";\nimport \"./MainContainer.css\";\n\nexport default function Conclusion() {\n    return (\n        <div>\n            <h2>Conclusion</h2>\n            <p>All in all, we are excited about our results and our models’ ability to surpass the baseline model. Our approach to this problem was to perform an in-depth analysis of several deep learning approaches, testing many architectures like CNN-GRU, LSTM, and GAN.<br></br>\n            Through our testing, we were able to iterate on our architectures through both parameter tuning and changes to the structure of each model. With these trials, we were able to get a holistic view of the problem and gain an understanding of how changes to our architectures can impact its performance. Our iterative approach sparked our interest in utilizing the Seq2Seq model on top of our existing architectures or even prompted us to apply advanced techniques like Social GAN and tweak its architecture to best match our dataset. For more information on other architectures we tested, please see the other tabs listed at the top of the page.</p>\n            <p>You can find our code <a href='https://github.com/deepnewworld/csci566-project/tree/master/Models' >here.</a></p>\n        </div>\n    )\n}\n\n","import React from \"react\";\nimport \"./MainContainer.css\";\n\nexport default function FutureWork() {\n    return (\n        <div>\n            <h2>Future Work</h2>\n            <p>Although we implemented various models and tried many different approaches with those models, we found some limitations on our models from our experiments and they still give us chances to enhance our models with future works.</p>\n            <p>For Seq2Seq GAN,  it didn’t consider\n                <ul>\n                    <li>the road information therefore, the generated predictions can fall off the roads.</li>\n                    <li>the interaction with neighbors including other cars, pedestrians, or cyclists.</li>\n                </ul>\n            </p>\n        \n            <p>To complement these limitations and improve our Seq2Seq GAN Model, we can introduce the following methods in the future work.\n                <ul>\n                    <li>Training the model with optimal parameters and with more epochs.</li>\n                    <li>Combining with a CNN model including road information from images.</li>\n                    <li>Applying the concept of Social GAN which embraces interactions among neighbors.</li>\n                </ul>\n                With the above approach, we believe that our Seq2Seq GAN model can generate more plausible and precise predictions on the multiple next moves. \n            </p>\n\n            <p>For our Social LSTM, the current model does not \n                <ul><li>differentiate different types of objects(i.e. Car, Cyclist, Pedestrian) when attempting to compute their trajectories because of computational constraints.</li></ul>\n            </p>\n\n            <p>However, we believe that our Social LSTM model can be improved much more from the current model, if we apply the addressed approach and train it with more epochs. Some attributes of social LSTM that make us hopeful that it can improve accuracy with more training are:</p>\n            <ul>\n                <li>Since we consider the spatial information about the neighbourhood: the closer, the more important. So social LSTM variant models should capture the information more precisely, which will return more accurate results.</li>\n                <li>Same as considering the speed influence: not using the square grid, instead using rectangular grid. This change will make us consider more useful neighborhoods.</li>\n                <li>Using the classes to give different input to make it more accurate.</li>\n            </ul>\n            <p>One potential approach we had hoped to try was utilizing a transformer based network to compare its performance with other models. This dataset contains multiple facets of vehicle information like historical availability, various views like semantic, satellite, etc.  which can be utilized to get more precise predictions.</p>\n        </div>\n    )\n}","import React from 'react';\nimport commentBox from 'commentbox.io';\n\nexport default class PageWithComments extends React.Component {\n    componentDidMount() {\n        this.removeCommentBox = commentBox('5712513450639360-proj', { defaultBoxId: this.props.articleId });\n    }\n\n    componentWillUnmount() {\n\n        this.removeCommentBox()\n    }\n\n    render() {\n\n        return (\n            <div className=\"commentbox\" />\n        )\n    }\n}","import React from \"react\";\nimport ProblemStatement from './ProblemStatement'\nimport Motivation from './Motivation'\nimport Methods from './Methods'\nimport FineTuning from './FineTuning'\nimport Results from './Results'\nimport Conclusion from './Conclusion'\nimport FutureWork from \"./FutureWork\";\nimport PageWithComments from './PageWithComments';\nimport \"./MainContainer.css\";\n\nexport default function MainContainer() {\n    return (\n        <div id='outer_container'>\n            <div id='container'>\n                <ProblemStatement/>\n                <Motivation/>\n                <Methods/>\n                <FineTuning/>\n                <Results/>\n                <FutureWork/>\n                <Conclusion/>\n                <PageWithComments/>\n            </div>\n        </div>\n    )\n}","import { Component } from \"react\";\n\nexport default class BaseComponent extends Component {\n    componentDidMount() {\n        window.scrollTo(0, 0);\n    }\n}","export default __webpack_public_path__ + \"static/media/resnetgru1.8940d720.png\";","export default __webpack_public_path__ + \"static/media/resnetgru2.57e9661d.png\";","export default __webpack_public_path__ + \"static/media/resnetgru3.2cd47887.png\";","export default __webpack_public_path__ + \"static/media/resnetgru4.49f870f3.png\";","export default __webpack_public_path__ + \"static/media/resnetgru5.44b320fd.png\";","export default __webpack_public_path__ + \"static/media/resnetgru6.652db087.png\";","export default __webpack_public_path__ + \"static/media/resnetgru7.4cab73d5.png\";","export default __webpack_public_path__ + \"static/media/resnetgru8.e78d3c08.png\";","export default __webpack_public_path__ + \"static/media/resnetgrutable1.95a707de.png\";","export default __webpack_public_path__ + \"static/media/resnetgrutable2.1dd9b81d.png\";","export default __webpack_public_path__ + \"static/media/img6.8c3e6644.png\";","import React from \"react\";\nimport BaseComponent from './BaseComponent'\nimport resnetgru1 from './images/resnetgru/resnetgru1.png'\nimport resnetgru2 from './images/resnetgru/resnetgru2.png'\nimport resnetgru3 from './images/resnetgru/resnetgru3.png'\nimport resnetgru4 from './images/resnetgru/resnetgru4.png'\nimport resnetgru5 from './images/resnetgru/resnetgru5.png'\nimport resnetgru6 from './images/resnetgru/resnetgru6.png'\nimport resnetgru7 from './images/resnetgru/resnetgru7.png'\nimport resnetgru8 from './images/resnetgru/resnetgru8.png'\nimport resnetgrutable1 from './images/resnetgru/resnetgrutable1.png'\nimport resnetgrutable2 from './images/resnetgru/resnetgrutable2.png'\nimport resnetgru10 from './images/resnetgru/img6.png'\nimport \"./ResnetGNU.css\";\n\nexport default class ResnetGNU extends BaseComponent {\n    render() {\n        return (\n\n            <div id='outer_container'>\n                <div id='container'>\n                <h2>Resnet-GRU model</h2>\n                <p>Our first model was an attempt to create a baseline for the problem at hand. The model was based on a pre-trained Resnet_34 network which was extended using a bi-directional GRU.\n    The input dataset consisted of input images, target positions of the agent vehicle as well as the ego vehicle. It also included target availability for the agent and ego vehicle. The input also provided us with historical positions of both sets of vehicles. For our model we used the input image of the agent as input and trained it on its given target positions. We optimized the model by calculating losses on target availability. The input number of channels was given by the number of historical positions/frames we wanted to consider. \n    For our scenario we considered 10 historical frames which gave us the number of input channels as 22 because we considered 10 for each agent and ego vehicle and also included the current frame for both of them. <br/>\n    The Resnet network was fed input images of the agent vehicle frame and the number of input channels were 22 as described above. The output from the resnet was of shape (bathc_size, 512). For our experiments we considered batch_size to be 16.\n    The output from the Resnet was further fed to the bi-directional GRU. The hidden size for the network was chosen to be 2048. The gru layer gave output with shape ((batch_size, 1, 512), 2*hidden_size). The output features from this layer of GRU was then further fed to a sequential network consisting of a linear layer followed by a RElu and another linear layer and generated output with 1024 features. The hidden state from the previous GRU layer and the output features from the sequential network was fed to another bi-directional GRU layer and a similar cycle was followed.\n    The GRU layer returned an output with 1024 features which was fed to a linear network to finally get the desired target output features. <br/>\n    The total number of targets considered of the number of future positions we wanted to predict * the number of trajectories we wished to predict. In our scenario we decided to predict 50 future positions for 3 trajectories as kaggle was evaluating the results with the same. We also chose to predict confidences for 3 trajectories given the high uncertainty of real time traffic. Each future position had X and Y coordinate values, hence total number of target became number_of_trajectories * num_of_future_positons * 2 + confidences_for_3_trajectories i.e (3*50*2 + 3 = 303).\n    Once we had the desired output, we split it into future positions (300) and confidences (3). The confidences were further transformed to probabilities using softmax. The final predicted outputs had shape (batch_size,num_of_trajectories,50,2) and confidences had shape (batch_size, num_of_trajectories). The model and output generation process described above is represented diagrammatically below.</p>\n            <div style={{textAlign: 'center'}}>\n                <img src={resnetgru1}></img>\n                <p>Fig 1. Model journey </p>\n                <img src={resnetgru2}></img>\n                <p>Fig 2. Output generation</p>\n            </div>\n                <p>For our experiments we executed our model multiple times on the training data to improve its performance. One of the critical things we ensured in this methodology was to use the trained model and latest optimizer state for the next cycle of training. Each cycle of training consisted of 12000 iterations. We generated our predictions from test data after each such cycle to observe the models performance. The predictions were evaluated by Kaggle using a negative log likelihood function. Below are the details of the three rounds of experiments conducted by us.\n                </p>\n            <div style={{textAlign: 'center'}}>\n                <img src={resnetgrutable1} class='imagetable'></img>\n                <br></br>\n                <img src={resnetgru3}></img>\n                <p>Fig 3. Cycle 1 average training loss curve</p>\n                <img src={resnetgru4}></img>\n                <p>Fig 4. Cycle 2 average training loss curve</p>\n                <img src={resnetgru5}></img>\n                <p>Fig 5. Cycle 3 average training loss curve</p>\n                <p>Prediction loss on test dataset evaluated by Kaggle:</p>\n                <img src={resnetgru6}></img>\n                <p>Fig 6. Best test result from kaggle submission</p>\n                <p>We also tried to train our model using a different loss function to compare the model’s performance</p>\n                <img src={resnetgrutable2} class='imagetable'></img>\n                <p>Table 2. Experiment details using MSE and rMSE loss function</p>\n                <img src={resnetgru7}></img>\n                <p>Fig 7. Training and Validation loss for MSE</p>\n                <img src={resnetgru8}></img>\n                <p>Fig 8. Training loss for rMSE</p>\n            </div>\n            <h4>Prediction trajectory visualization</h4>\n            <p>Every agent is identified by its track id and timestamp. In our submission file we have every record with timestamp and track id as index. For comparing the results generated by each of our models, we have compared trajectories predicted for the same agent.<br/> For my prediction I have used 3 modes or 3 prediction trajectories. Each trajectory holds 50 two dimensional (X,Y) predictions and each trajectory will have its own confidence. The sum of all three confidences will sum up to 1.\n    <br/>In the given visualization, we compared predicted trajectories for an agent with track id “18431”. We can observe that trajectory 3 represented by color green is dominant and as the model improves it reaffirms trajectory 3 with the highest confidence of 0.943. The visualizations are represented in both semantic and satellite view.</p>\n                <img src={resnetgru10}></img>\n                </div>\n            </div>\n        )\n    }\n}\n","export default __webpack_public_path__ + \"static/media/GAN4.2d86396c.png\";","export default __webpack_public_path__ + \"static/media/LSTM-POS.ceb583c7.png\";","export default __webpack_public_path__ + \"static/media/LSTM-CNN.53f2e082.png\";","export default __webpack_public_path__ + \"static/media/LSTM-2CNN.de0202b9.png\";","export default __webpack_public_path__ + \"static/media/LSTM-All.fd1785c2.png\";","export default __webpack_public_path__ + \"static/media/LSTM-Loss.e90b2b18.png\";","export default __webpack_public_path__ + \"static/media/SeqLSTM-Models.2d1a3ea9.png\";","export default __webpack_public_path__ + \"static/media/SeqLSTM-Bar.eb6f9f5b.png\";","import React from \"react\";\nimport BaseComponent from './BaseComponent'\nimport \"./LSTM+Seq2Seq.css\";\n\nimport l1 from './images/lstm/LSTM-POS.png'\nimport l2 from './images/lstm/LSTM-CNN.png'\nimport l3 from './images/lstm/LSTM-2CNN.png'\nimport l4 from './images/lstm/LSTM-All.png'\nimport l5 from './images/lstm/LSTM-Loss.png'\nimport l6 from './images/lstm/LSTM-Bar.png'\nimport l7 from './images/lstm/SeqLSTM-Models.png'\nimport l8 from './images/lstm/SeqLSTM-Bar.png'\nconst m_w = '60%'\n\nexport default class LSTM extends BaseComponent {\n    render() {\n        return (\n          <div id='outer_container'>\n              <div id='container'>\n                  <h2>LSTM and Seq2Seq LSTM model</h2>\n                  <p>We wanted to see how well a standard LSTM would perform and how adding more data to the architecture would affect its training performance. Lstm is one of the simplest recurrent models, allowing us to take advantage of temporal data in a simple, yet effective way. Our first LSTM (LSTM V1) architecture utilizes only the past 11 history positions of the given agent we are predicting trajectory for. By just using these positions, the model is able to get a sense of the speed of the vehicle given the constant time between all positions.</p>\n                  <div style={{textAlign: 'center'}}><img src={l1} style={{maxWidth: m_w,}}/><p>LSTM V1 - Positional Information Only</p></div>\n                  <p>Next, we assumed that adding more data would allow the model to get a better picture of the road conditions to use in predictions. As a result, we added a Resnet CNN to convert an image of the road and all nearby vehicles into a 1000 length vector encoding that would also be passed into the LSTM (LSTM V2). With 11 past images of the road state from a bird’s eye view and the 11 historical positions, the model was able to figure out which parameters were most important and make predictions accordingly. Our dataset provided these images with 2 channels per history frame, 1 channel containing outlines of other agents, 1 containing the agent we are predicting for a total of 22 channels (2 * 11). In addition to these 22 channels, the dataset provided an RGB image containing the road information like lanes and traffic signal values. For this architecture, we tested by appending the road information image to the 2 channels (for a total of 5 channels per image) before passing it through the CNN (LSTM V2). We also tested another architecture which had two separate CNNs, one for the 2 channel car information input and one with the 3 channel road information input (LSTM V3).</p>\n                  <div style={{textAlign: 'center'}}><img src={l2} style={{maxWidth: m_w}}/><p>LSTM V2 - One CNN</p></div>\n                  <div style={{textAlign: 'center'}}><img src={l3} style={{maxWidth: m_w}}/><p>LSTM V3 - Two CNN</p></div>\n                  <p>Finally, we wanted to implement an LSTM architecture where we throw as much available data as possible (LSTM V4). The intuition is to add all seemingly useful data and allow the model to figure out what input is actually important during training. As a result, these models might have performed better if trained for longer. We also found that this model trained better with a lower learning rate compared to the models described above, likely due to having so many input parameters.</p>\n                  <div style={{textAlign: 'center'}}><img src={l4} style={{maxWidth: m_w}}/><p>LSTM V4 - All Useful Information</p></div>\n                  <p>All of these LSTM variations performed well, quickly being able to stabilize and achieve a decent loss. Interestingly, the models with these least parameters required to learn seemed to be most stable whereas the more complicated variations like using all available data (LSTM All) performed well but had many spikes in average training loss. In this case, LSTM using only positional data achieved the best MSE test loss, though I assume with more training time the other models could also achieve similar loss. </p>\n                    <div style={{textAlign: 'center'}}><img class='loss' src={l5} style={{maxWidth: 400}}/></div>\n                    <div style={{textAlign: 'center'}}><img class='loss' src={l6} style={{maxWidth: 400}}/></div>\n                  <p>Given the success of these architectures, we wanted to see if we could apply the Seq2Seq architecture using the same general structure. As a result, we developed four similar models, replacing the single LSTM from the previous models with an encoder and decoder LSTM. The encoder took in the temporal input from our dataset. The decoder took in the last hidden state from the encoder and predicted the next 50 positions of the agent. We also tested a variation in which only the last decoder output was used to make the prediction, putting the output through a fully connected layer and predicting 50 time steps at once. This variation achieved decent accuracy though still performed worse than the standard Seq2Seq implementation.</p>\n                  <div style={{textAlign: 'center'}}><img class='loss' src={l7} style={{width: '80%'}}/></div>\n                  <p>Interestingly, while the position only variant of the base LSTM performed the best of the 4 architectures, the double CNN Seq2Seq variation performed the best among the Seq2Seq LSTM models, achieving an MSE test loss of 2.29. It was exciting to see Seq2Seq’s ability to perform well in the trajectory prediction domain, despite it often being thought of as a tool for text-based predictions, especially translations. While Seq2Seq performance is still around the level of the LSTM’s performance, we consider that with more tuning and training, it might be able to surpass it.</p>\n                  <div style={{textAlign: 'center'}}><img class='loss' src={l8} style={{maxWidth: 400}}/></div>\n              </div>\n          </div>\n        )\n    }\n}\n","export default \"data:image/png;base64,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\"","export default __webpack_public_path__ + \"static/media/vae1.510ea3d0.png\";","export default __webpack_public_path__ + \"static/media/vae2.ebd21294.png\";","export default __webpack_public_path__ + \"static/media/vae3.9466ba20.png\";","export default __webpack_public_path__ + \"static/media/vae4.b7fa9d7f.png\";","export default __webpack_public_path__ + \"static/media/vae5.dd1d9795.png\";","export default __webpack_public_path__ + \"static/media/vae6.794464c8.png\";","export default __webpack_public_path__ + \"static/media/vae7.9b7e52c9.png\";","export default __webpack_public_path__ + \"static/media/vae8.750d2809.png\";","export default __webpack_public_path__ + \"static/media/vae9.02e156ca.png\";","export default __webpack_public_path__ + \"static/media/vaea.667bf8ac.png\";","export default __webpack_public_path__ + \"static/media/vaeb.04767882.png\";","export default __webpack_public_path__ + \"static/media/vaec.5c7b7c39.png\";","import React from \"react\";\nimport BaseComponent from './BaseComponent'\nimport \"./VAELSTM.css\";\nimport Figure from 'react-bootstrap/Figure'\nimport vae1 from './images/vaelstm/vae1.png'\nimport vae2 from './images/vaelstm/vae2.png'\nimport vae3 from './images/vaelstm/vae3.png'\nimport vae4 from './images/vaelstm/vae4.png'\nimport vae5 from './images/vaelstm/vae5.png'\nimport vae6 from './images/vaelstm/vae6.png'\nimport vae7 from './images/vaelstm/vae7.png'\nimport vae8 from './images/vaelstm/vae8.png'\nimport vae9 from './images/vaelstm/vae9.png'\nimport vaea from './images/vaelstm/vaea.png'\nimport vaeb from './images/vaelstm/vaeb.png'\nimport vaec from './images/vaelstm/vaec.png'\n\nexport default class VAELSTM extends BaseComponent {\n    render() {\n        return (\n            <div id='outer_container'>\n                <div id='container'>\n                    <h2>VAE+LSTM</h2>\n                    <p>This model follows the variational model architecture seen in class using LSTM layers to learn the probability distribution of the trajectories and as a result output similar trajectory within the same distribution.</p> \n                    <p>We present two configurations, Configuration A, uses a hidden layer and splits the hidden layer into two tensors mean and standard deviation. The output of parametrization trick is concatenated with the cell state and feeds the decoder to output next n positions.</p>\n                    <p>Configuration B uses both hidden layer and cell state in two parametrization phases and the output is concatenated and is fed to the decoder to output next n positions.</p>\n                    <div style={{textAlign: 'center'}}>\n                        <Figure><Figure.Image class='vae1' src={vae1}/><Figure.Caption>Figure 1. VAE+LSTM method</Figure.Caption></Figure>\n                    </div>\n\n                    <br></br>\n                    <p>The following table shows the losses at evaluation time using MSE as criterion achieved after 10000 iterations, number of history positions was 10, predicted positions 50 and beta = 0.2 for VAE.</p>\n                    <table style={{width: '90%'}}>\n                        <thead>\n                            <tr>\n                                <th>Configuration</th>\n                                <th>Experiment details</th>\n                                <th>Loss function</th>\n                                <th>Learning rate</th>\n                                <th>Iterations</th>\n                                <th>Average test loss</th>\n                            </tr>\n                        </thead>\n                        <tr>\n                            <td>A</td>\n                            <td>Model is trained with batch size = 32, beta = 0.2, AdamW optimizer, weight decay = 0.0005</td>\n                            <td>MSE Loss reduction none</td>\n                            <td>1e-3</td>\n                            <td>10000</td>\n                            <td>2.1587</td>\n                        </tr>\n                        <tr>\n                            <td><b>B</b></td>\n                            <td><b>Model is trained with batch size = 32, beta = 0.2, AdamW optimizer, weight decay = 0.0005</b></td>\n                            <td><b>MSE Loss reduction none</b></td>\n                            <td><b>1e-3</b></td>\n                            <td><b>10000</b></td>\n                            <td><b>1.9585</b></td>\n                        </tr>\n                    </table>\n                    <br></br>\n\n                    <div style={{textAlign: 'center'}}><Figure><Figure.Image class='loss' src={vaec}/><Figure.Caption>Figure 2. Configuration B total training and validation loss</Figure.Caption></Figure></div>\n                    <div style={{textAlign: 'center'}}><Figure><Figure.Image class='loss' src={vae2}/><Figure.Caption>Figure 3. Configuration A test loss</Figure.Caption></Figure></div>\n                    <div style={{textAlign: 'center'}}><Figure><Figure.Image class='loss' src={vae3}/><Figure.Caption>Figure 4. Configuration B test loss</Figure.Caption></Figure></div>\n                    <div style={{textAlign: 'center'}}><Figure><Figure.Image class='loss' src={vae4}/><Figure.Caption>Figure 5. Configuration B Reconstruction and KL test loss</Figure.Caption></Figure></div>\n                    <p>Both configurations produce similar results, total loss curve, reconstruction and kl loss are similar as well.</p>\n                    <p>As it can be appreciated, VAE+LSTM average losses seem to be slightly better than other LSTM models, but not as good as other models like Seq2Seq GAN. Our assumption is that this is because the model is creating in some cases sharper trajectories since the model is not tuned or trained enough to output smooth trajectories for such scenarios and while most trajectories are good approximations when compared to ground truth trajectories in general seem to be a little bit unnatural where Seq2SeqGAN model with the help of the generator seems to output more natural (real) future predictions.</p>\n                    <p>In the following example we can see the predicted positions for an agent. Here we can see that the first 5 positions have sharp changes with respect to the last history positions, but after those positions it seems to project a reasonable trajectory.</p>\n                    <div style={{textAlign: 'center'}}><Figure><Figure.Image  src={vae5}/><Figure.Caption>Figure 6. Next 50 calculated positions for random agent</Figure.Caption></Figure></div>\n                    <p>Finally, here are some random sampled trajectories generated from the agent's test set during evaluation time, as we can see the model generates different predicted trajectories based on the history positions which shows that while some trajectories behavior seem similar the coordinate positions are not the same and as a result there is some diversity on the trajectories generated by this model.</p>\n                    <div style={{textAlign: 'center'}}>\n                        <Figure>\n                            <img class='grid' src={vae6}/>\n                            <img class='grid' src={vae7}/>\n                            <img class='grid' src={vae8}/>\n                            <br></br>\n                            <img class='grid' src={vae9}/>\n                            <img class='grid' src={vaea}/>\n                            <img class='grid' src={vaeb}/>\n                        </Figure>\n                        <Figure.Caption>Figure 7. Randomly sampled trajectories</Figure.Caption>\n                    </div>\n                </div>\n            </div>\n        )\n    }\n}","export default __webpack_public_path__ + \"static/media/GAN6.a9ee5802.png\";","export default __webpack_public_path__ + \"static/media/GAN7.da686091.png\";","import React from \"react\";\nimport BaseComponent from './BaseComponent'\nimport \"./Seq2SeqGAN.css\";\nimport GAN1 from './images/main/GAN1.png'\nimport GAN2 from './images/main/GAN2.png'\nimport GAN3 from './images/main/GAN3.png'\nimport GAN4 from './images/main/GAN4.png'\nimport GAN5 from './images/main/GAN5.png'\nimport GAN6 from './images/main/GAN6.png'\nimport GAN7 from './images/main/GAN7.png'\n\nexport default class Seq2SeqGAN extends BaseComponent {\n    render() {\n        return (\n            <div id='outer_container'>\n                <div id='container'>\n                    <h2>Seq2Seq GAN and its variant model</h2>\n                        <p>\n                            To the baseline LSTM model, we applied GAN architecture and created a new model. We call this\n                            model as SeqtoSeq GAN here. With the GAN’s generator and discriminator, we expected that our \n                            model generates more plausible future positions, avoiding the blurry predictions with the help of \n                            discriminator. Figure1 and Figure2 two shows the architecture of the generator and the \n                            discriminator of our Seq2Seq model. \n                        </p>\n            \n                        <div style={{textAlign: 'center'}}>\n                            <img src={GAN1} style = {{maxWidth:'100%', width:'80%'}}/>\n                            <br/>\n                            <span>\n                                <strong>\n                                    Figure 1. Generator of Seq2Seq GAN1 \n                                </strong>\n                            </span>\n                        </div>\n            \n                        <div style={{textAlign: 'center'}}>\n                            <img src={GAN2} style = {{maxWidth:'100%', width:'80%'}}/>\n                            <br/>\n                            <span>\n                                <strong>\n                                    Figure 2. Discriminator of Seq2Seq GAN1 \n                                </strong>\n                            </span>\n                        </div>\n                        <br/>\n            \n                        <p>\n                            As you see the generator’s architecture in Figure1, we firstly extracted  latent temporal features from history positions with encoder and decoder. After that, we added noise vectors for the generator and used LSTM and fully connected layers to generate the next 50 trajectories. \n                            For the discriminator, we gave the generated trajectories and the target trajectories which is the ground truth to it as input. And we used an LSTM layer to extract latent features and used fully connected layers to distinguish whether our input to a discriminator is forgery or not.            \n                        </p>\n                \n                        <div style={{textAlign: 'center'}}>\n                            <img src={GAN3} style = {{maxWidth:'100%', width:'80%'}}/>\n                            <br/>\n                            <span>\n                                <strong>\n                                    Figure 3. Generator of Seq2Seq GAN2 (with yaw)\n                                </strong>\n                            </span>\n                        </div>\n                        <br/>\n                        <p>\n                            Other than the generator model we have discussed above, we have implemented a new generator that contains yaw information taking the direction information into our model. As in Figure3, the encoder takes history positions and yaw information as input. We believed that this would improve our model and found out that the Seq2Seq GAN2 has lower MSE test loss than the Seq2Seq GAN1.             \n                        </p>\n\n                <div>\n                    <table style={{maxWidth: '100%', tableLayout: 'fixed'}}>\n                        <caption>\n                            <strong>\n                                Table 1. Test Losses of different Seq2Seq GAN Models \n                            </strong>\n                        </caption>\n                        <tr>\n                            <th><b>Model</b></th>\n                            <th><b>Epoch</b></th>\n                            <th><b>Test Loss(MSE)</b></th>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN1-v2-1-1</td>\n                            <td>1</td>\n                            <td>2.6551</td>\n                        </tr>\n                        <tr>\n                            <td><b>Seq2Seq GAN2-v2-1-1</b></td>\n                            <td><b>1</b></td>\n                            <td><b>2.2576</b></td>\n                        </tr>\n                    </table>\n                </div>\n                <br/>\n                <div style={{textAlign: 'center'}}>\n                    <img src={GAN4} style = {{maxWidth:'100%', width: '70%'}}/>\n                    <br/>\n                    <span>\n                        <strong>\n                            Figure 4.  Training loss graphs( lr = 1e-3 )\n                        </strong>\n                    </span>\n                </div>\n                <br/>\n                <p>\n                    These are the loss graphs from a Seq2Seq GAN model. In this model, we have used MSE loss for the generator loss and BCE loss for the discriminator. MSE loss function allows a model to learn directly from the target value. As you see from Figure 4, generator loss converges to 0. This is desirable since we use a generator to generate future trajectory. Moreover, validation loss follows similar behavior as training loss, and it shows that our model is not overfitted.                                \n                </p>\n                <h4>Fine Tuning</h4>\n                <p>\n                    We have tested many different hyperparameters for fine tuning  to improve  two Seq2Seq GAN models above.\n                    Firstly, we tested the Seq2Seq GAN1 with different epochs and we got the following result.\n                </p>\n\n               <div>\n                    <table style={{maxWidth: '100%', tableLayout: 'fixed'}}>\n                        <caption>\n                            <strong>\n                                Table 2. Test Losses on Seq2Seq GAN1 with different epochs \n                            </strong>        \n                        </caption>\n                        <tr>\n                            <th><b>Configuration</b></th>\n                            <th><b>Epoch</b></th>\n                            <th><b>Test Loss(MSE)</b></th>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN1-v2-1-1</td>\n                            <td>1</td>\n                            <td>2.6551</td>\n                        </tr>\n                        <tr>\n                            <td><b>Seq2Seq GAN1-v2-1-1</b></td>\n                            <td><b>2</b></td>\n                            <td><b>1.9143</b></td>\n                        </tr>\n                    </table>\n               </div>\n                <p>\n                    As the result in Table2, the MSE test loss becomes much lower when we trained the model with more epochs.                                \n                </p>\n                <p>\n                    Next, we have tried training our models with different loss functions in the generator - BCE and MSE, remaining other conditions the same. With BCE loss, the generator loss indicates how well generated trajectories resemble the target trajectories. On the contrary, applying MSE loss function on the generator directly calculated the loss between the predicted future positions and the ground truth positions.                 \n                </p>\n               <div>\n                    <table style={{maxWidth: '100%', tableLayout: 'fixed'}}>\n                        <caption>\n                            <strong>\n                                Table 3. Test Losses on Seq2Seq GAN models with different Loss functions in the generator \n                            </strong>\n                        </caption>\n                        <tr>\n                            <th><b>Configuration</b></th>\n                            <th><b>Epochs</b></th>\n                            <th><b>Generator’s Loss function</b></th>                                \n                            <th><b>Test Loss(MSE)</b></th>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN1-v1-1</td>\n                            <td>1</td>\n                            <td>BCE</td>\n                            <td>78.5733</td>\n                        </tr>\n                        <tr>\n                            <td><b>Seq2Seq GAN1-v2-1-1</b></td>\n                            <td><b>1</b></td>\n                            <td><b>MSE</b></td>\n                            <td><b>2.6551</b></td>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN2-v1-1</td>\n                            <td>1</td>\n                            <td>BCE</td>\n                            <td>78.5536</td>\n                        </tr>\n                        <tr>\n                            <td><b>Seq2Seq GAN2-v2-1-1</b></td>\n                            <td><b>1</b></td>\n                            <td><b>MSE</b></td>\n                            <td><b>2.2576</b></td>\n                        </tr>   \n                    </table>\n               </div>\n                <p>\n                    As in the Table3, on both models, the MSE test loss becomes much lower when we trained the model with more epochs because the model directly learns from the target trajectories with MSE loss functions and it might be difficult for BCE loss to give meaningful feedback when we predicted 50 future moves.                                \n                </p>\n                <p>\n                    We modified the layers in the discriminator and compared the test losses because we believe that the discriminator loss converges to 0 too quickly, which prevents the generator from learning effectively. Instead of the Relu activation function layer, we used LeakyRelu and also applied dropout and the batch normalization. Table 4 shows the result from this trial. For all the models in Table 4, we applied MSE loss function to the generators and trained with 1 epoch.                        \n                </p>\n               <div>\n                    <table style={{maxWidth: '100%', tableLayout: 'fixed'}}>\n                        <caption>\n                            <strong>\n                                Table 4. Test Loss on Seq2Seq GAN models with different layers in the discriminators. \n                            </strong>\n                        </caption>\n                        <tr>\n                            <td width=\"25%\" rowSpan='2'><b>Configuration</b></td>\n                            <td colspan=\"2\"><b>Layers in discriminator</b></td>\n                            <td width=\"25%\" rowSpan='2'><b>Test Loss(MSE)</b></td>\n                        </tr>\n                        <tr>\n                            <th><b>Activation Function</b></th>\n                            <th><b>Dropout, Batch Norm</b></th>                  \n                        </tr>\n\n                        <tr>\n                            <td>Seq2Seq GANl1-v2-1-1</td>\n                            <td>Relu</td>\n                            <td>Not Applied</td>\n                            <td>2.6551</td>\n                        </tr>\n\n                        <tr>\n                            <td><b>Seq2Seq GANl1-v2-2-1</b></td>\n                            <td><b>Leaky Relu</b></td>\n                            <td><b>Applied</b></td>\n                            <td><b>2.5579</b></td>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN2-v2-1-1</td>\n                            <td>Relu</td>\n                            <td>Not Applied</td>\n                            <td>2.2576</td>\n                        </tr>\n                        <tr>\n                            <td><b>Seq2Seq GAN2-v2-2-1</b></td>\n                            <td><b>Leaky Relu</b></td>\n                            <td><b>Applied</b></td>\n                            <td><b>2.1781</b></td>\n                        </tr>\n                    </table>\n               </div>\n                <p>\n                    As in the Table4, applying LeakyRelu, dropout and batch normalization could reduce the MSE test loss on our models.                                \n                </p>\n                <p>\n                    With this success, we applied another change on the optimizer to see that the generator can learn better with this change. We’re currently using Adam in the generator and the discriminator as optimizers. To prevent the discriminator from learning too fast which might affect the generator’s performance, we applied different values of learning rate on the optimizer in the discriminator and saw the result. The other conditions for all the models in the Table 5 are the same - epoch of 1, MSE loss function for the generators. \n                </p>\n               <div>\n                    <table style={{maxWidth: '100%', tableLayout: 'fixed'}}>\n                        <caption>\n                            <strong>    \n                            Table 5. Test Loss on Seq2Seq GAN models with different learning rate \n                            </strong>\n                        </caption>\n                        <tr>\n                            <th><b>Configuration</b></th>\n                            <th><b>Learning rate in the discriminator</b></th>\n                            <th><b>Test Loss(MSE)</b></th>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN1-v2-2-1</td>\n                            <td>1e-3</td>\n                            <td>2.5579</td>\n                        </tr>\n\n                        <tr>\n                            <td>Seq2Seq GAN1-v2-2-2</td>\n                            <td>5e-3</td>\n                            <td>2.5574</td>\n                        </tr>\n                        <tr>\n                            <td><b>Seq2Seq GAN1-v2-2-3</b></td>\n                            <td><b>1e-4</b></td>\n                            <td><b>2.1463</b></td>\n                        </tr>\n                    </table>\n               </div>\n                <p>\n                    As in Table 5, we could get the lowest test MSE loss when we trained the discriminator with learning rate, 1e-4. \n                </p>\n                <div style={{textAlign: 'center'}}>\n                    <img src={GAN6} style = {{maxWidth: '100%', width: '70%'}}/>\n                    <br/>\n                    <span>\n                        <strong>\n                            Figure 5. Training loss graphs with learning rate 1e-4. \n                        </strong>\n                    </span>\n                </div>\n                <br/>\n                <p>\n                    From the graph above, we can observe that  the discriminator loss in Figure 5 converges to 0 slower than Figure 4. It shows that when the learning rate is smaller, the optimizer has more gradient to update weights, which helps the model to enhance its performance.                                \n                </p>\n                <h4>Evaluation on Seq2Seq models</h4>\n                <p>\n                    After training, we tested our Seq2Seq models to check if it doesn’t fall into the mode failure and it can generate diverse predictions for the inputs. We generated one hundred sample predictions for the same batch(32) input and plotted all these generated future trajectories(100) for the randomly-picked four inputs. \n                </p>\n                <div style={{textAlign: 'center'} }>\n                    <img src={GAN5} style = {{maxWidth: '100%', width: '50%'}}/>\n                    <br/>\n                    <span>\n                        <strong>\n                            Figure 6. Predicted Trajectories from the Seq2Seq GAN Model1\n                        </strong>\n                    </span>\n                </div>\n                <br/>\n                <div style={{textAlign: 'center'}}>\n                    <img src={GAN7} style = {{maxWidth: '100%', width: '50%'}}/>\n                    <br/>\n                    <span>\n                        <strong>\n                            Figure 7. Predicted Trajectories from the Seq2Seq GAN Model2 (with yaw information)\n                        </strong>\n                    </span>\n                </div>\n                <br/>\n                <p>\n                    As you see in Figure 6 and 7, both Seq2Seq GAN1 and Seq2Seq GAN1 do not suffer from mode collapse and successfully generates diverse trajectories.                \n                </p>                 \n                <p>\n                    You can find all the variants of Seq2Seq GAN with detailed hyper parameters and the structures in Table 6. \n                </p>\n\n                <div>\n                    <table style={{maxWidth: '100%', tableLayout: 'fixed'}}>\n                        <caption>\n                            <strong>\n                                Table 6. All the models with tuning \n                            </strong>\n                        </caption>\n                        <tr>\n                            <td width=\"15%\" rowSpan='2'><b>Configuration</b></td>\n                            <td width=\"9%\" rowSpan='2'><b>Optimizer</b></td>\n                            <td width=\"9%\" rowSpan='2'><b>Batch Size</b></td>\n                            <td colspan=\"2\"><b>Loss function</b></td>\n                            <td colspan=\"2\"><b>Layers in the Discriminator</b></td>\n                            <td width=\"8%\" rowSpan='2'><b>Epoch</b></td>\n                            <td width=\"9%\" rowSpan='2'><b>Learning rate</b></td>\n                            <td width=\"9%\" rowSpan='2'><b>Test Loss(MSE)</b></td>        \n                        </tr>\n                        <tr>\n                            <th>Generator</th>\n                            <th>Discriminator</th>\n                            <th>Activation Function</th>\n                            <th>Dropout, Batch Norm</th>\n\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN1-v1-1</td>\n                            <td>Adam</td>\n                            <td>32</td>\n                            <td>BCE</td>\n                            <td>BCE</td>\n                            <td>Relu</td>\n                            <td>X</td>\n                            <td>1</td>\n                            <td>1e-3</td>\n                            <td>78.5733</td>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN2-v1-1</td>\n                            <td>Adam</td>\n                            <td>32</td>\n                            <td>BCE</td>\n                            <td>BCE</td>\n                            <td>Relu</td>\n                            <td>X</td>\n                            <td>1</td>\n                            <td>1e-3</td>\n                            <td>78.5536</td>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN1-v2-1-1</td>\n                            <td>Adam</td>\n                            <td>32</td>\n                            <td>MSE</td>\n                            <td>BCE</td>\n                            <td>Relu</td>\n                            <td>X</td>\n                            <td>1</td>\n                            <td>1e-3</td>\n                            <td>2.6551</td>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN1-v2-2-1</td>\n                            <td>Adam</td>\n                            <td>32</td>\n                            <td>MSE</td>\n                            <td>BCE</td>\n                            <td>Leaky Relu</td>\n                            <td>O</td>\n                            <td>1</td>\n                            <td>1e-3</td>\n                            <td>2.5579</td>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN1-v2-2-2</td>\n                            <td>Adam</td>\n                            <td>32</td>\n                            <td>MSE</td>\n                            <td>BCE</td>\n                            <td>Leaky Relu</td>\n                            <td>O</td>\n                            <td>1</td>\n                            <td>5e-3</td>\n                            <td>2.5574</td>\n                        </tr>\n                        <tr>\n                            <td>Seq2Seq GAN2-v2-1-1</td>\n                            <td>Adam</td>\n                            <td>32</td>\n                            <td>MSE</td>\n                            <td>BCE</td>\n                            <td>Relu</td>\n                            <td>X</td>\n                            <td>1</td>\n                            <td>1e-3</td>\n                            <td>2.2576</td>\n                        </tr>       \n                        <tr>\n                            <td>Seq2Seq GAN2-v2-2-1</td>\n                            <td>Adam</td>\n                            <td>32</td>\n                            <td>MSE</td>\n                            <td>BCE</td>\n                            <td>Leaky Relu</td>\n                            <td>O</td>\n                            <td>1</td>\n                            <td>1e-3</td>\n                            <td>2.1781</td>\n                        </tr>     \n                        <tr>\n                            <td>Seq2Seq GAN1-v2-2-3</td>\n                            <td>Adam</td>\n                            <td>32</td>\n                            <td>MSE</td>\n                            <td>BCE</td>\n                            <td>Leaky Relu</td>\n                            <td>O</td>\n                            <td>1</td>\n                            <td>1e-4</td>\n                            <td>2.1463</td>\n                        </tr>    \n                        <tr>\n                            <td><b>Seq2Seq GAN1-v2-1-1</b></td>\n                            <td><b>Adam</b></td>\n                            <td><b>32</b></td>\n                            <td><b>MSE</b></td>\n                            <td><b>BCE</b></td>\n                            <td><b>Relu</b></td>\n                            <td><b>X</b></td>\n                            <td><b>2</b></td>\n                            <td><b>1e-3</b></td>\n                            <td><b>1.9143</b></td>\n                        </tr>                                                   \n                    </table>\n               </div>\n                <p>\n                    1 : Train with only history positions<br/>\n                    2 : Train with history positions and history yaw <br/>\n                    V1 : BCE loss function in the generator<br/>\n                    V2 : MSE loss function in the generator<br/>\n                    Vx - 1 : Relu, without Dropout and Batch Norm in the discriminator<br/>\n                    Vx - 2 : LeakyRelu, with Dropout and Batch Norm in the discriminator<br/>\n                    Vx - x - 1 : With learning rate 1e-3<br/>\n                    Vx - x - 2 : With learning rate 5e-3<br/>\n                    Vx - x - 3 : With learning rate 1e-4<br/>\n                </p>\n                <p>\n                    For now, the best Seq2Seq GAN model is the one with epoch2. While this model achieved the lowest test loss with 1.9143, we believe that we can achieve much lower MSE test loss if we apply Leaky Relu, dropout, and the batch norm in the layers and apply the learning rate of 1e-4 to the optimizer in the discriminator.                     \n                </p>\n                <h4>Future Work </h4>\n                <p>Although we have implemented many variant models of Seq2Seq GAN, we found some following limitations and these still give us chances to enhance our models with future works.</p>\n                <p>It didn’t</p>\n                <p>\n                    <li>consider the road information therefore, the generated predictions can fall off the roads.</li>\n                    <li>consider the interaction with neighbors including other cars, pedestrians, or cyclists.</li>\n                    <li>have a chance to learn enough with more epochs because of time and computational limit </li>\n                </p>                    \n                <p>To complement these limitations and improve our Seq2Seq GAN Model, we can introduce the following methods in the future work.</p>\n                <p>\n                    <li>Training the model with optimal parameters and with more epochs. </li>    \n                    <li>Combining with a CNN model including road information from images.</li>    \n                    <li>Applying the concept of Social GAN which embraces interactions among neighbors.</li>    \n                </p>                    \n                <p>With the above approaches, we believe that our Seq2Seq GAN model can generate more plausible and precise predictions on the multiple next moves. </p>\n                <p>You can find the code for Seq2Seq GAN \n                    <a href=\"https://github.com/deepnewworld/csci566-project/tree/master/Models/Seq2SeqGAN\"> here!</a>\n                </p>\n              </div>\n           </div>\n        )\n    }\n    }\n","export default __webpack_public_path__ + \"static/media/slstmcode.7c9fbe99.png\";","export default __webpack_public_path__ + \"static/media/slstmtrainloss.46c9177c.png\";","export default __webpack_public_path__ + \"static/media/slstmtestloss.caf9bc79.png\";","import React from \"react\";\nimport BaseComponent from './BaseComponent'\nimport \"./SocialLSTM.css\";\n\nimport Figure from 'react-bootstrap/Figure'\nimport SLSTM1 from './images/main/SLSTM1.png'\nimport SLSTM0 from './images/main/SLSTM0.png'\nimport slstmcode from './images/main/slstmcode.png'\nimport SLSTM2 from './images/main/SLSTM2.png'\nimport SLSTM3 from './images/main/SLSTM3.png'\nimport slstmtrainloss from './images/main/slstmtrainloss.png'\nimport slstmtestloss from './images/main/slstmtestloss.png'\n\n\nexport default class SocialLSTM extends BaseComponent {\n    render() {\n        return (\n            <div id='outer_container'>\n                <div id='container'>\n                    <h2> Social LSTM and its variant models</h2>\n                        <p>\n                            Alexandre, Kratarth, et. al. pointed out that humans have the innate ability to  “read” one another. Any autonomous vehicle navigating such a scene should be able to foresee the future positions of people (including vehicles driven by other people or pedestrians) and accordingly adjust its path to avoid collisions. Inspired by this paper, we want to take the “neighbor's” influence into consideration. For this project, we have one LSTM for each vehicle. This LSTM learns the spatial coordination and predicts their future positions as shown in the Figure 1.\n                        </p>\n                        <div style= {{textAlign: 'center'}}>\n                            <Figure><Figure.Image class='slstm1' src={SLSTM1}/><Figure.Caption>Figure 1. Social LSTM Model Architecture</Figure.Caption></Figure>\n                        </div>\n                        \n                        <p>\n                                        The LSTM weights are shared across all the sequences. Different from the vanilla LSTM, it has an additional layer: social pooling, which combines the information from all neighboring states. Their calculation follows the following equations:\n\n                        </p>\n        \n                        <div style= {{textAlign: 'center'}}>\n                            <Figure><Figure.Image class='slstm0' src={SLSTM0}/><Figure.Caption>Equation 1. Social Pooling Layer Equation</Figure.Caption></Figure>\n                        </div>\n        \n                        <p>\n                                            The pseudo code is shown in Algorithm 1:\n                        </p>\n        \n                        <div style= {{textAlign: 'center'}}>\n                            <Figure><Figure.Image class='slstmcode' src={slstmcode}/></Figure>\n                        </div>\n        \n                        <p>\n                                However, this model has some inherent limitations:\n                                    <ul>\n                                        <li>Simply adding all the neighbor’s information does not make sense. People care more about the neighborhood closer to them than those people who are far away from them. </li>\n                                        <li>This social LSTM architecture is a generalization method. However, if we can give a more specific classification, it is highly likely that we can improve the prediction’s accuracy. </li>\n                                        <li>For the social LSTM architecture, when the agent falls into a grid, it will be considered as a neighborhood. For the architecture, the grid we considered is a square. It seems unreasonable because agents move faster in the speed’s direction than the other. </li>\n                                    </ul>\n                                In order to solve these, we need to modify current model to:\n                                    <ul>\n                                        <li>Introducing the spatial information to the max-pooling part, which can be realized by using the convolutional layer.</li>\n                                        <li>In the real world, when a car’s move, it only has two classes: longitude and latitude. For the longitude classes, it can be split into 3 situations: speed up, normal and slow down; turning left, staying the same and turning right are these 3 situations for the latitude class. For the dataset we used for this project, it does not have classes, so we preprocess the dataset, giving them corresponding classes. </li>\n                                        <ul>\n                                            <li>For the speed class, we define that if the average speed over the next 5 time stamps is over than 1.2 speed at the current time, it will be classified as speed up. If the average speed over the next 5 time stamps is less than 0.8 speed at the current time, it will be classified as slow down. Between 0.8 and 1.2, it is normal.</li>\n                                            <li>For the direction class, using the next 5 time stamps its move over 5 feet or not to decide whether this agent turn left or right. If its move within 5 feet. It will be thought as stay same. This 5 feet is the half-width of the US urban lane. We assume that if the agent/car moves half of the urban lane, it is highly likely that it will turn its direction.</li>\n                                        </ul>\n                                        <li>For the grid, instead of considering the square, tried to consider rectangular. </li>\n                                    </ul>\n     \n                                Based on this changes, the architecture changes as the follows:\n                        </p>\n        \n                        <div style= {{textAlign: 'center'}}>\n                            <Figure><Figure.Image class='slstm2' src={SLSTM2}/><Figure.Caption>Figure 2. Social LSTM Variant Model Architecture</Figure.Caption></Figure>\n                        </div>\n        \n        \n                        <p>\n                                        For this model, the goal changes to maximize the probability of the prediction based on the observation (here the observation is the sequence of each agent’s position). The the objective function is:\n                        </p>\n        \n        \n                        <div style= {{textAlign: 'center'}}>\n                            <Figure><Figure.Image class='slstm3' src={SLSTM3}/></Figure>\n                        </div>\n        \n                        <p>For this model, since we predict the probability, we want to minimize the negative log likelihood. But in order to be comparable with other models, here we calculated the MSE loss.\n                        \n                        The training result is shown as follows:\n                        </p>\n        \n                        <div style= {{textAlign: 'center'}}>\n                            <Figure><Figure.Image class='slstmtrainloss' src={slstmtrainloss}/><Figure.Caption>Figure 3. Training Loss for Social LSTM and its Variant</Figure.Caption></Figure>\n                        </div>\n        \n                        \n                        <p>\n                                        As we can see that social LSTM variant is slightly better than the social LSTM model. This comes from the following reasons:\n                                        <ul>\n                                            <li>Since we consider the spatial information about the neighbourhood: the closer, the more important. So social LSTM variant models should capture the information more precisely, which will return more accurate results.</li>\n                                            <li>Same as considering the speed influence: not using the square grid, instead using rectangular grid. This change will make us consider more useful neighborhoods.</li>\n                                            <li>Using the classes to give different input to make it more accurate.</li>\n                                        </ul>\n        \n                                        However, this slight advantage may comes from:\n                                        <ul>\n                                            <li>Since the dataset does not have any classes. We  define the classes by ourselves. Maybe this is not accurate, in the future, perhaps we should try some other criterion to classify the dataset.</li>\n                                        </ul>\n                                        Since we found that the social-LSTM variant  model is better than social-LSTM, we should use the social-LSTM variant model for testing data. The result about the testing loss w.r.t prediction length is as follows:\n\n                        </p>\n        \n                        \n                        <div style= {{textAlign: 'center'}}>\n                            <Figure><Figure.Image class='slstmtestloss' src={slstmtestloss}/><Figure.Caption>Figure 4. Test Loss for Social LSTM Variant</Figure.Caption></Figure>\n                        </div>\n        \n                        <p>\n                                        We changed the predicting length from 1 to 50.  The longer prediction length, the larger test loss. The loss is like a linear regression. Their loss is similar to the training loss, which confirm that we do not have the overfitting issue. \n                        </p>\n        \n                            \n            \n                </div>\n            </div>\n        )\n    }\n}\n","import React from \"react\";\nimport \"./App.css\";\nimport Header from \"./Header\";\nimport { Route, Switch, Redirect } from \"react-router-dom\";\nimport \"./App.css\";\nimport MainContainer from \"./MainContainer\";\nimport ResnetGNU from './ResnetGNU';\nimport Seq2Seq from './Seq2Seq';\nimport LSTM from './LSTM+Seq2Seq';\nimport VAELSTM from './VAELSTM';\nimport Seq2SeqGAN from './Seq2SeqGAN';\nimport SocialLSTM from './SocialLSTM';\n\nexport default function App() {\n    return (\n        <section key='app'>\n            <Header/>\n            <Switch>\n                <Route exact\n                        path=\"/home\"\n                        component={() => <MainContainer key='home' page='home'/>}\n                />\n                <Route exact\n                        path=\"/resnet-gru\"\n                        component={() => <ResnetGNU key='resnet-gru' page='resnet-gru'/>}\n                />\n                <Route exact\n                        path=\"/lstm-seq2seq\"\n                        component={() => <LSTM key='lstm' page='lstm'/>}\n                />\n                <Route exact\n                        path=\"/vae-lstm\"\n                        component={() => <VAELSTM key='vae-lstm' page='vae-lstm'/>}\n                />\n                <Route exact\n                        path=\"/seq2seqGAN\"\n                        component={() => <Seq2SeqGAN key='seq2seqGAN' page='seq2seqGAN'/>}\n                />\n                <Route exact\n                        path=\"/s-lstm\"\n                        component={() => <SocialLSTM LSTM key='s-lstm' page='s-lstm'/>}\n                />\n                <Redirect from=\"/\" to=\"/home\" />\n            </Switch>\n        </section>\n    );\n};","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\nimport {HashRouter as Router} from \"react-router-dom\";\n\nReactDOM.render(\n  <React.StrictMode>\n    <Router><App /></Router>\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();"],"sourceRoot":""}