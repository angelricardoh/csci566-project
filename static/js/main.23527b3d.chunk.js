(this["webpackJsonpcsci566-project"]=this["webpackJsonpcsci566-project"]||[]).push([[0],{17:function(e,t,i){},33:function(e,t,i){},43:function(e,t,i){},45:function(e,t,i){},54:function(e,t,i){},55:function(e,t,i){},56:function(e,t,i){},57:function(e,t,i){},58:function(e,t,i){},59:function(e,t,i){},60:function(e,t,i){"use strict";i.r(t);var s=i(0),n=i(1),r=i.n(n),c=i(23),a=i.n(c),o=(i(43),i(33),i(9)),d=i(10),l=i(12),h=i(11),j=(i(44),i(24)),b=i(21),u=(i(45),i(7)),m=i(8),x=function(e){Object(l.a)(i,e);var t=Object(h.a)(i);function i(e){return Object(o.a)(this,i),t.call(this,e)}return Object(d.a)(i,[{key:"render",value:function(){return Object(s.jsx)("header",{children:Object(s.jsxs)(j.a,{collapseOnSelect:!0,bg:"dark",variant:"dark",expand:"lg",children:[Object(s.jsx)(j.a.Brand,{children:"Lyft5 Motion Prediction for Autonomous Vehicles"}),Object(s.jsx)("t",{}),Object(s.jsx)(j.a.Toggle,{"aria-controls":"basic-navbar-nav"}),Object(s.jsxs)(j.a.Collapse,{id:"basic-navbar-nav",children:[Object(s.jsxs)(b.a,{className:"mr-auto nav-section",activeKey:"6",children:[Object(s.jsx)(b.a.Link,{eventKey:"0",as:m.c,to:"/home",children:"Home"}),Object(s.jsx)(b.a.Link,{eventKey:"1",as:m.c,to:"/resnet-gru",children:"Resnet-GRU"}),Object(s.jsx)(b.a.Link,{eventKey:"2",as:m.c,to:"/lstm-seq2seq",children:"LSTM+Seq2Seq"}),Object(s.jsx)(b.a.Link,{eventKey:"3",as:m.c,to:"/vae-lstm",children:"VAE+LSTM"}),Object(s.jsx)(b.a.Link,{eventKey:"4",as:m.c,to:"/seq2seqGAN",children:"Seq2SeqGAN"}),Object(s.jsx)(b.a.Link,{eventKey:"5",as:m.c,to:"/s-lstm",children:"Social LSTM"})]}),Object(s.jsx)(j.a.Brand,{children:"Deep New World"})]})]})})}}]),i}(n.Component),O=Object(u.g)(x),p=i.p+"static/media/Motivation1.7da28e5e.png",g=i.p+"static/media/SDCAR.f1dc928a.gif",f=i.p+"static/media/SDCAR2.462f0ff0.gif";i(17);function w(){return Object(s.jsxs)("div",{children:[Object(s.jsx)("h2",{children:"Problem Statement"}),Object(s.jsx)("p",{children:"One of the most primitive tasks of autonomous vehicles is to predict the future positions of other vehicles amidst traffic. In this project, we aim to predict the future positions of the other vehicles/agents in a given scene such as cyclists, cars, and pedestrians. We are using Lyft\u2019s l5kit dataset. This dataset contains over 1000 hours of scenes, including labeled positional information of objects such as vehicles, cyclists, and pedestrians. The scenes in the dataset are from a 6.8 mile route chosen by the Lyft team. The input data will be the sequence of spatial coordination about each vehicle (both for train and test) which can also include road information of the given agents environment. The output will be the future spatial coordination. Given the uncertainty of live traffic, we have also aimed to predict the future positions for multiple trajectories with a level of certainty for each trajectory."}),Object(s.jsx)("img",{id:"motivation_pic",alt:"Motivation1",src:p,class:"center"}),Object(s.jsxs)("div",{id:"more_motivation_pics_div",children:[Object(s.jsx)("img",{alt:"Picture2",src:g}),Object(s.jsx)("img",{alt:"Picture3",src:f})]})]})}function v(){return Object(s.jsxs)("div",{children:[Object(s.jsx)("h2",{children:"Motivation"}),Object(s.jsx)("p",{children:"Autonomous vehicles are the future of on land mobility for mankind. But, we are still far from seamless autonomous transportation and we need to resolve many significant challenges before we can truly travel autonomously. One of the most important aspects of any autonomous vehicle is its ability to predict the next moves of other agents. The challenge to predict the next move is significant for its success. It's of utmost importance to correctly predict the next position of other vehicles as it will not only determine the direction in which the vehicle should move but will also impact the safety of the passenger and also others on the road. What makes this challenge even more critical is the fact that the next position of the vehicle should be determined keeping in mind not only the surroundings but also the destination and traffic agents. The impact of this solution is enormous in autonomous vehicles being successful and making transportation seamless and safe which has been a driving force behind us choosing to work on this problem statement. It is an exciting opportunity to work on a problem that will solve some pressing real world use cases and make a global impact."})]})}var A="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAALcAAABPCAYAAACpr1TrAAAAAXNSR0IArs4c6QAAAHhlWElmTU0AKgAAAAgABAEaAAUAAAABAAAAPgEbAAUAAAABAAAARgEoAAMAAAABAAIAAIdpAAQAAAABAAAATgAAAAAAAACWAAAAAQAAAJYAAAABAAOgAQADAAAAAQABAACgAgAEAAAAAQAAALegAwAEAAAAAQAAAE8AAAAAiEiDbwAAAAlwSFlzAAAXEgAAFxIBZ5/SUgAAGXBJREFUeAHtnQeYFtW5x7fB0mHpRZHdAIqGYKRbYAlCFEGBCAqoIBDQBG5iomCMheuTGPEqMSJEQZBeBIUEW7wWigax65ULClJEuCudpe2usHt//2Fmn/l63fl2v53zPPPNmVPec973/M973lNmvtQU18VNAiNHjuxRUlLSK24EIyd0PC0tbcm8efPyIs+afDkyko+lxHFUXFx8WWpq6oPUIC0RtaDsfdThHcp2wZ2oRkhEwztR5sKFC5+knHkZGaF1BkBMieRyov7JVkboVkg2jsuYn9OnT0+oXr162/T09CvPnj0bqLQfMF/OAO7UQAns4aRNI2kGNNO4UkRXF+H2ZK7fSwIuuL0EEuvjihUrTmN73wH41kGrgTc94RlQriP+F23atDnhHR/qeceOHVmkuRY6k7jauwAPLLGwNEfg7G5MIAncdtttE4iTmZLunUbaF9t48vz58x/zjovkmU40FXD/jjyGkgLs+3juv2DBgk8joVPR044ZM+aCoqKiQZiDnz7//PNSKoZLyMTHKjyZ7wDsafhbxeqFD5sAW9r7kVtuuSXXJzKCADrHZJL/XaNBZXS33nprO5TIUz/88MO7APuvyLW1XQ6+krfHuv6YJJCTkzMCge/zBp9MCcJQ4Omrhw8fLjMjakfneQR626MmUEEzMmpdB+8zkeP5sNAUM09zGI9JTsLBPW7cuLr0vvG33357owoq54DVnjJlShHg7ikwezuFcdWtUqXKGu+4SJ61pk0ja/mvXDu07ABd8aoko9YrXL0AtOYeh/zRTSi4hwwZUquwsHAeFRt75syZU/4qWNHDFi1aJK3660B8APDudO77AsWHE04HWqB0/jpROPnLOs2oUaP6AMB/cvn28hgLl8aG72J/ZBIGbnpxpxo1arwKwwOp2FrWiE/6q2AyhB04cGAOfKzyxwv8qw3uQx5d/cWHE4Z9/y7pjnKVufHNaBQRZui4EwHfUupWXKtWrbXxrmNmZiYk/TvHlwIlnJ07d06WrUivs7TN3Zgld584caKGltL8V7Xihr722muFw4YNuxMT5Cq4aGjnRNoWgNck7AVGshz497Ab7WmD+aGxHlqXo8XjBfBUNG4T2qgDtC/l6kn5PVmK/Df3PsHqojjyNqU+X3A1MnlMoX3zsZU/xpzoHCp/POIDgptKvEwBnbjqcpUKjIpa/v2E34XWWMndYAbBSkMpT32EUdpIZp4i7o8C7j8ziXqG2e1u0s3mKqxateoVJ0+ePEzDFvCclG7p0qXfI9MRyOB1GLRkaPBKWAqTy5Zs/iwjYEg0AoDGCvLtR+5HosnvnWf06NFtMRUXQK8NbZVF26agkFIIe9M7rb9n2jSfVYxLyPcXOvUY8v0n9+eOHTvmmPkZcIjBFh5HhQYgtH9Reel+60oj7D6YHshu3H9bjLVq1Wo/vXw8zyO4thBvpc/E/yV5biRcYE5ZsmTJEcJy8Fbn/r/PPffcFjU+z3G3yVReeXForDfgdxJg8amSRjEAfiPD+D0+kaEDUlEyi7h+iXm3M3Ty0Cnmzp37FfS6AtB24ACMloJbnSikmzVr1inWnA+Q8GesQWuEfpt2/g4Fdjhk5sAJPJRC4GTnYnylbOZYtmzZPrz7EPZ7VOwGBdPzUmB2MgKcZiYrvaGRZdR/p4s879OI7SUQOXr8b8mzwXjgJzc3V+V2pDFTadSXrPDKcD916tQMNHQneL9JgLY7PSO3e7G/P0Be6+xxIfxlphSwaY9RpzS1JSDdeeTIkb0h6lIazUpYMzpGNtdWAo18hFVBcf6G537gylMApTk9PZQvvLyHcrjfMyb4U0DNbcvWxfIDbNnDhhlihfm7U5neFrCJP8RyVSmwlR4tfx6MtSGdOsyL/mgka5jmFHT2KQD5mwA81if8EWz0hgHiHQ2mHQ3FJpME9waTwjPhVkC7hrSzOuw2OvUe5aOzaORvyyUbvkc4F2lzSXcJV0QuoOYWFTRITQhfLBDqgtFNaJz8YCWgtVsA2BwNRaTVAZ+lftKfT9iFxOXRG2V7VypHZ9/K7uRDyGcB8vVQMDynYK9eDpi0PKit9US7G6WoBG7qtjaSCS/5ZIoKO5+Tr0j+6dOnF8H7Y9BbyhXWiEN+dQhZEhG5oOCm8Esh2kgUBW4aYxMNcyxYCaQboeFVTuBmSFrklT4VpjvQgzMKCgpe8IqrNI+sfy9mgvkTFMEkRkQPvvWM7O8ifiudf5ZHpPMPV6jD0aYHaduvwi1eipG07blOkm+9LV+Jufav9f8ydR5aw7skmOpAmAFuAFkAg5/xHLS3kedW9XR1BhrpFLbjB3a62FwZxPVWPBOr5SwBXhzlJMpOtkL6Ae5kOv8GgOxTf8kQWT6JbH7qE+lQAMt53SiqloqjLl/TnoZpEWbxV5KuOlcB+xkb6aiPT5w4UYsMcXWYOwEnmQE1tyrCsk1HCV6Cxmnp7/NgNaO3NiZe9pShtcn3Ms8enQFzJZ2wXmjtImiTpHgl16BgdJM5DnAPp5N/RGdvIrl5Oa0mPYtC6MPqQ9AR0ytfXB6pjwVQjSRa0ToowoC+DXF34R3G6FwPHl7iGqtVMKtg6q2OUYP7FpZ5R+IvyMvLC9tet+iEuiO75qRRpxHI69nTBwT30aNH68JQN5vAd6KFt9gze/tJ34f0hhrCL5NE4PZw9OIMZst1RZf4lTA/mmEq7OGOjY50aEhwTcnrgwaPwkI8UIdUrgyWNFdGYkuGIBtR9OLFi7W6dDeZ5nBV9ZO5M/KS/T3ZT1yZBiGbzshYGCnEr82bFOo6FmWkteuGtJ/aUMEDAdkq7nYTtBlhqcR3hMYOzNBxjFTn7FXliNGxDl8b2lMg0485iibhWs2ZTP1agL0ZmM+7AoIbBhrCwMVm5ZW3Az12rcCgBz9O6j2Hy4gn3ymY8tH0jRs3PsUu12DoiP5brKfu8EMrYBAaP5OlNK0F94VGTOBWIdSxBjddCdsZZT15EY1yNXUZ6c2SngmfxLC+HnC8ojo74QQSypGGVnEFAPV1TMjx4OIZnldjot5JnPw3ACYptPO4Sh1xU8GAlOE3NWvWXDtz5swTpZFx8LRs2fI0OJpP0fMwlwzTgjrpTSUtL2t9/RwQ/ZVFpr72cARcHca628O8/BoWqiiMtLp9QY/SxoyHM9fD1cujdgiuCmVoKDKYipoQGTXpLQ8OgKM7RuVSlwvgz6NKkidhIwl0DNy0//m0d2tVhPK/B6hSXNPx/4lO9oDC6QDGm0akKyG9x8k8RvmdJPmb0pWFA0caMr4IRjuQFhZDxhKQmfkk9w4AIRNGfAx4woqJ68ptOumsCcjns2fP3s+uVLDyI45bs2bNKbTYVMpaRh09URAxtRStAqhDGstUUWSPaxYA3JvrE/iqYwEc0GjO8zWj3B1xLSw4MZlr7dDWtSlbHesD6jST+zMWsDlCcQHPzQkXJe06bgtO0vlYv+BGg1SjKleIMQkXJnR29ptg1aMXdydtLVMYmix+HA/w+SuTuqz3F17Bw1IlYyblXyG7zsjcYAd5apJ2x5w5c2LZto5INLm5uemU29tse7V/Twhsq1at2r0WIYCvZb5G5vMh2nqzFVde7rKVfBzMXC/G5DRs87zEJ5EtwFzi6WELOoZwNtmeXW9oCZQwIv0BeXeWgjCdEP44Jss7VoAT9/bt28tWu1r1MDtZFvexOi9iK/8nYKS2+bzdPEdii06816/mhpHhloAZtlPosa8Gq+rx48e1ZNXTFISSHqJBgtpDweiFimPFpCqTFL8dM1Ref/HMrAv8hTsZBrClKY1jwCrX1JoaMR9xsh4qi/MjrZi0N2GVxqrHg7TnTqseeskEfyfrmXZfbfnL090vuKngzwRuU8Br6bGeW2heHKBtNFNupZ012WAwG1TTYMJoZeB0dnb2RnOC6UUx8KOATWf7LZ2uA6mk2WJy1KM6NIdZ28MxEYsyM2ZgG+Q9V4pETqMl/g9YovxFlCRjykb5xqEusy230cH+aifIsp40dhdLmZF+hT1e/muvvTazSZMmveFjP5PLj7zjnXj2ATfrh72YGcvmNsCNP+Q7fqQZrAaRMwXiw6wRyY8mIqTRMdqTLOVkc/eYZVvpAt2zsrK0bd+Hjnd1oDSRhDNR08gkOSRkUknH0irUNGTSUmCR/ADEXvgbmagORz1uVkeTbFBYWoP3cKorNncLc3TfyMint4A8HEu+46HzMGkPgqkeLPnu80jgwIMPuKmwlgANpIpBmAhn8jbUZFQz+5JQNiJCS0NoVbl8Vl5C8dy8efMi3uQ5SDnHEVypcRoqX6B4Fv5rs4Ub8wgQiH6ocMyrCSiG/hr15ABEAcCeBGC2hspbFvHshtalLu3UnsjmIIprg3c51K+fwrirI86Tn8NQV2HGbKVDGmvMBDWGl5rcz0LDWEFTOiedB7iZJWdQ2SsAjT7fJUFvoWLadg/o9NY6TF6ixhGz5Alqf7FFu5theBBpj1x44YURrwCY65vDAlaoAkUgh34A+zEL2JIfoJoLsINO4MuSRQB9PW1ojCCUs6ZBgwb2SaRRNPHXCfyqL/VfynyhI2ErsdO1XGnsYRw+fPgR8u4m3XaU3ddlWedAtD3A3apVq05U5se2xDt5udVnI8YWr+3Xx8WknO40VEhNT+MF7QB2+snqZ97xU2S9SCCRk1mH/61vv/32N4nkGZAaZ7BVB+qzniOqhfb6EJ+KmdFR9abtDzAKtyZsOWlWUfdSE1b7EYTNNvNqhHZ8dDTMglw0NsP95ZggC9HYhu2nSuFXBZ/mvoqlnk3cjQqOGDGiDkz1hMFxxF/DZXQS4vGmHIDZhwH6OjrLFlPTKtxwOoRPQ17Jw5ZIzpSY2ZPiNmbMmPoogZdhxtjxldyQ2V5eBLgo3tvUkQjMPCynLfNs6vQ97XsDk0GPJV2A3Zy23YuGF2n1zCLSrq9Tp8719o6AmdKMdr4cS2A77exzDEOZy9oZoGzUqFE1gH0TAs7k2k1ljXLx664jrKOHDh3aEr9x/oJK9yBMvVWTwdLXjgizhrMHYOr2PXv2DCZ+t4TGQaz7EcpEypFNp3T9ifuKq1I5JpDpaLzJyLA7MjJ4R86S6+BEAlsVyc/P16qX2lQvB3zJ8QmfXUdNDBl1tMwrPJwl3TLMjgn4DYeJshGPdjd18E52+8/NKMdv51DsULEI5XeA+gkEspWrn3n+wKHSy0cx7EDeDLCXShGYrhiZTGK57QkrINY786DL6DhXUc4GRtxPYqXnnZ+2S6UtL4KHvZxqzPeOB+BD4Wm5eAT4jmLMXhcPm9seURZ+GG5v0t1dGYHNBPJSGnwe4DDEgDxkjvwjnsAWYcrQEtx1jBC38Rh3cENbDMh88evgqaq5jPiW3wQOBcZtly+c+iJ0/WeMNJXP8lI4+StyGs1T4F12tkw/w3zjvgfNJtMtbk6rV9DtwCWj+NO4EQ6TkMwukl5pdtyELhw4Bm60lg6+14Hxs4D8X2HKKimSsXZcBRNhFoBrIYbMhj+GTdoj3gxSRi409VGkM4yOiTjMJHD3oh7i87V48xcJPcfADbNdqFgW1zGE/lEklazoadlR/SU8DLXxUUjDj2NJdJctLGYv9nxNFMdt0NbLF6J9zv6JmXL4BJhIZmGStGXRYAebYwfDzxn/lI6Am+VAzb4vRXulo61eiT8b5ZcigPsxYJtBDa2JFaIomY2d/UIZ1HoEIu5PebLl3ysD+iFJssLyc62SqPymTZsm9ECaIxPK3bt3N0Aq7cQ0k5wlzKZlezfG3nyRcMe1S8gWilMC1oT1nl/p/EKgw30KsCfGqQiDjA4pNWzYUF8UeFbLi2hPbYv/O55lhEuLdh3I6CGTZKN93Tvc/PFM5wi4GaK05mlsDnHPQvATEIDWRpMW2OaGyCoauR4NbtnZ+fA9nPlHNb0oHW1D8lWCNA57SY6NodEC+qMB9ADtH8ipExGXEM1N8ddQj+PcE3I2RvxbLmoBWwTCuWODFSLsIobMFA7XPMV9KFo7IbtW4dQ3HmkA4B+ho9fGDHK6c+kF1oUAPIPP+UZdDDRSAZC+BlYHZVEPQpkWsEWU0UIfG/XZgIm6wDAzaqmTpDrluNnfBlCYZOKWzBFws5GwB8avZreqLWeUN3ByzNjpjBsX5YwQy3E96cz3eVcLjVoLQHb2Do/2WRpancbuZPoBroQoDsodqDrhtuiLrvZ6JcLvCLjFmLkysCsRTDpZJuu8TWnk5TRyujfwnKiHCa4PnShLZWiFhtWvk2Z5/eG5mA6muVTCnWPgTjinDlRA69ks+62mcfWPBA6U6FuENDcA+8A3Jv4hbEz1pbwJbMUv4P45nfpHdK4tKDKdO0q4c8EdxybA7JoNua6JArZYkebm+jiObAUkxSR2BJEDuPYB7K4AXPa/NqY8bSUCEuFccMdJ6mivkTTwsDiRi4qMgE0HO8mE8lBUBCLMBL//RZmXke0W7ru4t0NrJ3yVxGLDBbcliRjuWtqjoaW15tPIZ2IgFVNWzBFtyuVRhyMxEQozM7b2lyRtr/MkLBIkxg4Ls65uMlcCrgRcCbgScCXgSsCVgCsBVwKuBFwJuBJwJeBKwJWAKwFXAq4EXAm4EqhQErBejKhQlY53ZV0hxFuiDtPjPEtDvrp63PvFAF4I0X/W3MSGzlxe91pcGTdZ3B3K2MCYBoj04m9rdijHO/wFLX075A8c1JrAWRad59husUL4DOo0inP0NTjnncXZ6tXE+XxfxEqfrHdH3qFMVuGx7d6FFwfGcPXkENEgJ/mkU/0R0P6ZMrcB4NLtdup0M2F90dhdCNdBrml79+7VZ/EqnXM1dwxNzp+HfsiXTadDohmae2UwUpx7bkwH8Dkth9YtzsnJOcJL1MXB8tvjeBlCH4efAnh1AvAAZonx8oe+w0g9fo/WnsCbTpuheUckdO1lJIPfBXcMrWjasf/hjwRnnc/jCOg9gE+fKdaXUH3emjHzHeAF6o749/ij4x2mTgKw9ZbPR9DXP8gd528+CpUOkHfhuYAO976eKzOwxb8LbkkhBocZUA8NWkv/BGyRIUxvgM8EbM14Z3Q//g8Bt16aLQHsVjLrruOpBdZDsDtgTePD+7+GxkZoNlVa6J6wJouE3UXQ/ET8lXaweicqzgV3FJJHew5Gaw4hqz6C0xBzYwf+W0TK/AjlKuL1MvRM4mfh/w7T49jmzZt9zBLlscApfzC3a9cuve0+gvPa10BzqUYDgG7Y03SobpTVjvhVwWhUpjgX3FG0NhO5tZgGVQHT3wBXY+6lH3wk/CUBm7CpfH7hgVB/lhVJ8YD5Qei+z6rMdiaU9QVuLgPchD8BreXY2o68qBBJvROV1l0tiULy+sNTAKz3FI/KzEBz6wOXKQDuOp4vAGifYQc/HE9gjx07NpsiRmO//0llAWqB+yzlHWQkkcbuStyjinPdOQm4mjtKJADgH5H1fABWyKtVm0QGfyeTXGG9evWuZ71Z3+wL6sgjzbvG9ga53/TY9Qso81nSWa9xZZLwDODWv/c+xDW7PP7RqV9mHAp0wR2loAHaRWySVAd0/7SRkAaVHayXZZfawgN6oXMAjauvQ1mfR/BJiz09gE7Qmg2bXK9IfaY4h6sbu5B6l9F1Ngm44LYJI1wvYNM7k91kWwO60vVtQP09zwoTWPWhy6qhaELjBPmOBkpnfpbtbtLcbU08eWexFum17FJEWbnEvUrc4UA0rHBGkhb436XM6Yw206zwZL274I6iZZlQZqJFuwNw2duv20h8DND0mM7HLp+yhUft5b+EBkEzg+tFiwjr2PXxqyB9h1D/inavFRfsTlp1tl2krx0sXbLEuRPKKFqSJb6GmCTZLMlt5l6qdbOzsxcBoHyuzmjJyZGSZmLYk3wvMjEdqrwciqqBlr0feo+iaQsseoQ1wQ/eU/Wh99epj95CD+mw13eSqD8rKg+HTJwECVxwR9GIgEp/zy3z421WRUq3zbUjSNggNm8Eur8A1BmRkIfufOgOhsbv9TcjjA5v4/8EUK6x0yGsLZe+eX6YPI9iksj2DupEjzV4zQXGUU/VL+mda5ZE0cSsZd+AOaKc6y072CIDEN8G1BMBnf589lf478S/DiBu47m0I1jpTYA+LM1MugyepZL13+9vci9iNBhlpbXupNEH7bUhNAXzZ7MVHuzOpPVX1PsB8u0/dOjQTNIm7PsqweoZzzgX3FFIE4D05ctOeQDQ7/9oMuw/Dai3YtuOIe2lFNGRuz6471MaYfug83ci9uC/lXQ6+FSb+zqAe49PhnMB+tOsycTr0JZsb1/CXhn5uu40bPUx0H4tLy8v6YEt9l1we4Eg1COg7YXWTsPe/oJTgaVnqL3zAfA3WenYwHe4GwOommjODDqEdzL9LfYZ7PY8RZDnHezsTdDN8Pf/jlZmNPBUW3xIYCsfH7s/n4lwa+zzt7xHG4tust1dcIfRotirF6NdSzA5tpB8JMAGryVvAJLTwbKbb8fsCZbGO45dTWM73Tvc/mwDtj04qB9NPwBgH6Nj6BxMpXDuhDJEM/Mf5q35T/bNgGOZQE5yrWT8D5O9J0NkLVfRdMYbqdBnjDqnzP+KLFf1K4vKuOAOIVUOPx3HXv0/wN0OE+IfJNcGzZAKOLRfAQ8Hqfu4rKwsbd0nvTN2HJKeyxgZRNNVB+S1sVnP5Ofn6/y0r/EcYxllnZ25wkOUcRzz6iVWZnaVdXnlgf7/A4f5+34ZoLaJAAAAAElFTkSuQmCC",S=i.p+"static/media/SLSTM1.53c7ec37.png",y=i.p+"static/media/SLSTM2.87ffcdf1.png",T=i.p+"static/media/SLSTM3.0e772885.png",M=i.p+"static/media/GAN1.a5d75e69.png",q=i.p+"static/media/GAN2.f236eade.png",E=i.p+"static/media/GAN3.b1c2e4d2.png",N=i.p+"static/media/GAN5.bc8a36cc.png";function I(){return Object(s.jsxs)("div",{children:[Object(s.jsx)("h2",{children:"Methods"}),Object(s.jsxs)("p",{children:["To resolve this issue, we have implemented various methods and compared their performance by test loss in the Result section below. Firstly, we explored ",Object(s.jsx)(m.b,{to:"/resnet-gru",children:Object(s.jsx)("b",{children:"Resnet-Gru"})})," and ",Object(s.jsx)(m.b,{to:"/lstm-seq2seq",children:Object(s.jsx)("b",{children:"LSTM/Seq2Seq LSTM"})})," models. Based on the LSTM model, we added VAE and GAN structure and created ",Object(s.jsx)(m.b,{to:"/vae-lstm",children:Object(s.jsx)("b",{children:"VAE+LSTM"})})," and ",Object(s.jsx)(m.b,{to:"/seq2seqGAN",children:Object(s.jsx)("b",{children:"Seq2Seq GAN"})}),". Finally, we implemented ",Object(s.jsx)(m.b,{to:"/s-lstm",children:Object(s.jsx)("b",{children:"Social LSTM"})})," which incorporates the neighbors' effect into the model"]}),Object(s.jsx)("p",{children:"In this page, we focus on the two most interesting models among these. For the detailed information about other models we created, please check the other tabs."}),Object(s.jsx)("p",{children:Object(s.jsx)("i",{children:Object(s.jsxs)("b",{children:[Object(s.jsx)(m.b,{to:"/seq2seqGAN",className:"link",children:"Seq2Seq GAN"})," and its variant"]})})}),Object(s.jsxs)("figure",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:M,class:"ganmodel"}),Object(s.jsx)("figcaption",{children:"Figure 1. Generator of Seq2Seq GAN1"})]}),Object(s.jsxs)("figure",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:q,class:"ganmodel"}),Object(s.jsx)("figcaption",{children:"Figure 2. Discriminator of Seq2Seq GAN"})]}),Object(s.jsxs)("p",{children:["To the baseline LSTM model, we added the GAN and created a new model. We call this architecture ",Object(s.jsx)("b",{children:"Seq2Seq GAN."})," In this model, the generator generates the next 50 moves based on the past 11 positions. The discriminator determines whether it\u2019s real or fake. We expected that this structure can avoid blurry predictions and make more accurate predictions."]}),Object(s.jsx)("p",{children:"To improve Seq2Seq GAN1, we incorporated the past yaw information as well and made a Seq2Seq GAN2."}),Object(s.jsxs)("figure",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:E,class:"ganmodel"}),Object(s.jsx)("figcaption",{children:"Figure 3. Generator of Seq2Seq GAN2 (with yaw information)"})]}),Object(s.jsx)("p",{children:"As you see in Table 1, compared to the Seq2Seq GAN1, the Seq2Seq GAN2 achieved a lower test loss under the same conditions."}),Object(s.jsxs)("table",{style:{width:"90%",tableLayout:"fixed"},children:[Object(s.jsx)("caption",{children:"Table 1. Test Losses of different Seq2Seq GAN Models"}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Models"}),Object(s.jsx)("td",{children:"Epochs"}),Object(s.jsx)("td",{children:"Test Loss(MSE)"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN1"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"2.6551"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN2"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"2.2576"})]})]}),Object(s.jsx)("p",{children:"After training, we tested our Seq2Seq model to check if it doesn\u2019t fall into the mode failure and it can generate diverse predictions for the inputs. We generated one hundred sample predictions for the same batch(32) input and plotted all these generated future trajectories for the randomly-picked four inputs."}),Object(s.jsxs)("figure",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{style:{maxWidth:"80%"},src:N}),Object(s.jsx)("figcaption",{children:"Figure 4. Predicted Trajectories from the Seq2Seq GAN Model"})]}),Object(s.jsxs)("p",{children:["As you see in Figure 4, the Seq2Seq GAN model does not suffer from mode collapse and successfully generates diverse trajectories. ",Object(s.jsx)("br",{}),"More detailed explanation about this Seq2Seq GAN model is available on the other tab above."]}),Object(s.jsx)("br",{}),Object(s.jsx)("p",{children:Object(s.jsx)("i",{children:Object(s.jsxs)("b",{children:[Object(s.jsx)(m.b,{to:"/s-lstm",className:"link",children:"Social LSTM"})," and its variant"]})})}),Object(s.jsxs)("figure",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:S,class:"model"}),Object(s.jsx)("figcaption",{children:"Fig 5. Social LSTM method"})]}),Object(s.jsx)("p",{children:"-Based on Alexandre etc\u2019s research, we tried to take the neighbor\u2019s effect into consideration combining with the LSTM model. For this, we added a max-pooling layer between each time stamp, its formulation is:"}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsx)("img",{src:A})}),Object(s.jsx)("br",{}),Object(s.jsxs)("p",{children:["However, this method has the following limitation:",Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"Simply adding all the neighbor\u2019s information does not make sense because people care more about the neighborhood closer to them than those who are far away from them."}),Object(s.jsx)("li",{children:"This social LSTM architecture is a generalization method. However, if we can give a more specific classification, it is highly likely that we can improve the prediction\u2019s accuracy."}),Object(s.jsx)("li",{children:"For the social LSTM architecture, when the agent falls into a grid, it will be considered as a neighborhood. For the architecture, the grid we considered is a square. It seems unreasonable because agents move faster in the speed\u2019s direction than the other."})]}),"Based on these two observations, we modified the current social-LSTM architecture as below:",Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"Introducing the spatial information max-pooling part, which can be realized by using the convolutional layer."}),Object(s.jsx)("li",{children:"In the real world, when a car moves, it only has two classes: longitude and latitude. For the longitude classes, we split into 3 situations: speed up, normal and slow down; turning left, staying the same and turning right are these 3 situations for the latitude classes. More details are available on the Social LSTM tab."}),Object(s.jsx)("li",{children:"For the grid, instead of considering the square, tried to consider rectangular."})]}),"As a result of the modifications, we created a Social LSTM variant model as Figure 6."]}),Object(s.jsxs)("figure",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:y,class:"model"}),Object(s.jsx)("figcaption",{children:"Fig 6. Social-LSTM variant"})]}),Object(s.jsx)("p",{children:"For this model, we slightly changed our goal from directly predicting the future trajectory to maximizing the probability of prediction and returning the one with the highest probability. The objective function is:"}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsx)("img",{src:T,class:"formula"})}),Object(s.jsx)("p",{children:"More detailed explanation about this Social LSTM model is available on the Social LSTM tab above."})]})}function L(){return Object(s.jsxs)("div",{children:[Object(s.jsx)("h2",{children:"Fine Tuning"}),Object(s.jsxs)("p",{children:[Object(s.jsx)("i",{children:Object(s.jsx)("b",{children:"Seq2Seq GAN and Seq2Seq GAN variant models"})}),Object(s.jsx)("br",{}),"Based on the two Seq2Seq GAN models, we tried parameter-tuning by changing learning rates(1e-3, 5e-3, 1e-4), loss functions(BCE, MSE), and the layers on the discriminators. So far, the best Seq2Seq GAN Model among these is the Seq2Seq GAN-v1 with a learning rate of 0.001, batch size of 32, MSE loss function on the generator, and epochs of 2 and the test loss of this model is 1.9143. More detailed information is on the Seq2Seq tab."]}),Object(s.jsxs)("p",{children:[Object(s.jsx)("i",{children:Object(s.jsx)("b",{children:"Social-LSTM and social-LSTM variant Models"})}),Object(s.jsx)("br",{}),"For social-LSTM and its variants, there exist multiple parameters that we can tune.  For both models, we tried six different learning rates(1e-3, 3e-3, 5e-3, 8e-3, 1e-2, 2e-2) and found that 1e-03 is the best one. For social-LSTM\u2019s neighborhood grid, we tried the grid\u2019s size from (1,1) to (10, 10), it seems that (7,7) gives the minimum loss for the training part. For the variant method, we tried one direction from 1 to 5 and another one is from 7 to 15. Among all these trials, (13, 3) returns the best loss for the training datasets. In addition, for the convolutional layer in the variant method, we tried 3 by 3, 4 by 4 and 5 by 5. At the end, 3 by 3 wins."]})]})}function G(){return Object(s.jsx)("div",{children:Object(s.jsxs)("table",{style:{width:"300pt",tableLayout:"fixed"},children:[Object(s.jsx)("caption",{children:"Table 2. Comparison on the test loss of our models"}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Method"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Test loss"})})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq LSTM - v2"}),Object(s.jsx)("td",{children:"3.6188"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Social LSTM-v2"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"3.5578"})})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"LSTM - v2"}),Object(s.jsx)("td",{children:"3.1169"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq LSTM - v1"}),Object(s.jsx)("td",{children:"2.9981"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq LSTM - v3"}),Object(s.jsx)("td",{children:"2.2907"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2SeqGAN2-v2-2"}),Object(s.jsx)("td",{children:"2.2576"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"LSTM - v1"}),Object(s.jsx)("td",{children:"2.2572"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN2-v2-1"}),Object(s.jsx)("td",{children:"2.1783"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"VAE+LSTM A"}),Object(s.jsx)("td",{children:"2.1587"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN1-v2-2-1"}),Object(s.jsx)("td",{children:"2.1463"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"VAE+LSTM B"}),Object(s.jsx)("td",{children:"1.9585"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Seq2Seq GAN1-v1"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"1.9143"})})]})]})})}var B=i(36),F=i(32),R=i(3),k=i.p+"static/media/results1.a9d40824.png",z=i.p+"static/media/results2.d9bdbf0d.png",P=i.p+"static/media/results3.20498194.png",C=i.p+"static/media/results4.5afba3fa.png",W=i.p+"static/media/results5.ad10f1c9.png",Z=i.p+"static/media/results6.9d8b216c.png";function U(){return Object(s.jsxs)("div",{children:[Object(s.jsx)("h2",{children:"Results"}),Object(s.jsx)("p",{children:"After training all of our models, we examined our models performance on the testset and generated the next fifty positions for each agent. Figure 7 and Figure 8 shows the future trajectories predicted by one of our models on top of the maps."}),Object(s.jsxs)(B.a,{style:{textAlign:"center"},children:[Object(s.jsxs)(R.a,{children:[Object(s.jsxs)(F.a,{style:{textAlign:"center"},children:[Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{src:k}),Object(s.jsx)(R.a.Caption,{children:"Model after cycle 1"})]}),Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{src:z}),Object(s.jsx)(R.a.Caption,{children:"Model after cycle 2"})]}),Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{src:P}),Object(s.jsx)(R.a.Caption,{children:"Model after cycle 3"})]})]}),Object(s.jsx)(R.a.Caption,{children:"Figure 7. Semantic view of predicted trajectories"})]}),Object(s.jsxs)(R.a,{children:[Object(s.jsxs)(F.a,{children:[Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{src:C}),Object(s.jsx)(R.a.Caption,{children:"Model after cycle 1"})]}),Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{src:W}),Object(s.jsx)(R.a.Caption,{children:"Model after cycle 2"})]}),Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{src:Z}),Object(s.jsx)(R.a.Caption,{children:"Model after cycle 3"})]})]}),Object(s.jsx)(R.a.Caption,{children:"Figure 8.  Satellite view of predicted trajectories"})]})]}),Object(s.jsx)("br",{}),Object(s.jsx)("p",{children:"We computed MSE test loss against the ground truth trajectories to compare our model\u2019s performances. The following chart shows the average MSE test loss from our the most representative models:"}),Object(s.jsx)(G,{}),Object(s.jsx)("p",{children:"It is clear that our Seq2Seq Gan architecture achieved the best performance, performing more than 6x better than the baseline model. While the Seq2Seq architecture is often used for text translations, we were interested to see its performance in the trajectory prediction domain. To our surprise, Seq2Seq performed very well, allowing our Seq2Seq GAN model to achieve the best test loss."}),Object(s.jsxs)("p",{children:["Here are some more analysis on the compared result:",Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"While the LSTM and Seq2Seq based LSTM performed well, they were still outperformed by other models like Seq2Seq GAN."}),Object(s.jsx)("li",{children:"VAE+LSTM average losses seem to be slightly better than other LSTM models, but not as good as other models like Seq2Seq GAN."})]})]}),Object(s.jsxs)("p",{children:["From these observations, we can infer the followings:",Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"While testing with a basic recurrent model like LSTM was definitely a step in the right direction, it is clear that a more powerful recurrent model would be necessary to improve the loss."}),Object(s.jsx)("li",{children:"About the result of LSTM+VAE, we assume that this is because the model is creating in some cases more sharp trajectories since the model is not tuned or trained enough to output smooth trajectories for such scenarios."}),Object(s.jsx)("li",{children:"With the LSTM+VAE, while most trajectories are good approximations when compared to ground truth trajectories in general seem to be a little bit unnatural where Seq2SeqGAN model with the help of the discriminator seems to output more natural (real) future predictions."}),Object(s.jsx)("li",{children:"While Social LSTM was not the best performer, we believe in its potential and that our current architecture or even a slightly modified version can achieve a much lower loss,  especially with more training. Moreover, due to computational constraints, our social LSTM does not differentiate different types of objects (i.e. Car, Cyclist, Pedestrian) when attempting to compute their trajectories. This loss of information could prevent the model from achieving a better loss."})]})]}),Object(s.jsx)("p",{children:"Overall, we are happy to report that all of our models perform significantly better than the provided Lyft baseline, with all presented models achieving over 3 times better performance."})]})}function X(){return Object(s.jsxs)("div",{children:[Object(s.jsx)("h2",{children:"Conclusion"}),Object(s.jsxs)("p",{children:["All in all, we are excited about our results and our models\u2019 ability to surpass the baseline model. Our approach to this problem was to perform an in-depth analysis of several deep learning approaches, testing many architectures like CNN-GRU, LSTM, and GAN.",Object(s.jsx)("br",{}),"Through our testing, we were able to iterate on our architectures through both parameter tuning and changes to the structure of each model. With these trials, we were able to get a holistic view of the problem and gain an understanding of how changes to our architectures can impact its performance. Our iterative approach sparked our interest in utilizing the Seq2Seq model on top of our existing architectures or even prompted us to apply advanced techniques like Social GAN and tweak its architecture to best match our dataset. For more information on other architectures we tested, please see the other tabs listed at the top of the page."]}),Object(s.jsxs)("p",{children:["You can find our code ",Object(s.jsx)("a",{href:"https://github.com/deepnewworld/csci566-project/tree/master/Models",children:"here."})]})]})}function D(){return Object(s.jsxs)("div",{children:[Object(s.jsx)("h2",{children:"Future Work"}),Object(s.jsx)("p",{children:"Although we implemented various models and tried many different approaches with those models, we found some limitations on our models from our experiments and they still give us chances to enhance our models with future works."}),Object(s.jsxs)("p",{children:["For Seq2Seq GAN,  it didn\u2019t consider",Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"the road information therefore, the generated predictions can fall off the roads."}),Object(s.jsx)("li",{children:"the interaction with neighbors including other cars, pedestrians, or cyclists."})]})]}),Object(s.jsxs)("p",{children:["To complement these limitations and improve our Seq2Seq GAN Model, we can introduce the following methods in the future work.",Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"Training the model with optimal parameters and with more epochs."}),Object(s.jsx)("li",{children:"Combining with a CNN model including road information from images."}),Object(s.jsx)("li",{children:"Applying the concept of Social GAN which embraces interactions among neighbors."})]}),"With the above approach, we believe that our Seq2Seq GAN model can generate more plausible and precise predictions on the multiple next moves."]}),Object(s.jsxs)("p",{children:["For our Social LSTM, the current model does not",Object(s.jsx)("ul",{children:Object(s.jsx)("li",{children:"differentiate different types of objects(i.e. Car, Cyclist, Pedestrian) when attempting to compute their trajectories because of computational constraints."})})]}),Object(s.jsx)("p",{children:"However, we believe that our Social LSTM model can be improved much more from the current model, if we apply the addressed approach and train it with more epochs. Some attributes of social LSTM that make us hopeful that it can improve accuracy with more training are:"}),Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"Since we consider the spatial information about the neighbourhood: the closer, the more important. So social LSTM variant models should capture the information more precisely, which will return more accurate results."}),Object(s.jsx)("li",{children:"Same as considering the speed influence: not using the square grid, instead using rectangular grid. This change will make us consider more useful neighborhoods."}),Object(s.jsx)("li",{children:"Using the classes to give different input to make it more accurate."})]}),Object(s.jsx)("p",{children:"One potential approach we had hoped to try was utilizing a transformer based network to compare its performance with other models. This dataset contains multiple facets of vehicle information like historical availability, various views like semantic, satellite, etc.  which can be utilized to get more precise predictions."})]})}var K=i(37),Y=i.n(K),H=function(e){Object(l.a)(i,e);var t=Object(h.a)(i);function i(){return Object(o.a)(this,i),t.apply(this,arguments)}return Object(d.a)(i,[{key:"componentDidMount",value:function(){this.removeCommentBox=Y()("5712513450639360-proj",{defaultBoxId:this.props.articleId})}},{key:"componentWillUnmount",value:function(){this.removeCommentBox()}},{key:"render",value:function(){return Object(s.jsx)("div",{className:"commentbox"})}}]),i}(r.a.Component);function Q(){return Object(s.jsx)("div",{id:"outer_container",children:Object(s.jsxs)("div",{id:"container",children:[Object(s.jsx)(w,{}),Object(s.jsx)(v,{}),Object(s.jsx)(I,{}),Object(s.jsx)(L,{}),Object(s.jsx)(U,{}),Object(s.jsx)(D,{}),Object(s.jsx)(X,{}),Object(s.jsx)(H,{})]})})}var J=function(e){Object(l.a)(i,e);var t=Object(h.a)(i);function i(){return Object(o.a)(this,i),t.apply(this,arguments)}return Object(d.a)(i,[{key:"componentDidMount",value:function(){window.scrollTo(0,0)}}]),i}(n.Component),V=i.p+"static/media/resnetgru1.8940d720.png",_=i.p+"static/media/resnetgru2.57e9661d.png",$=i.p+"static/media/resnetgru3.2cd47887.png",ee=i.p+"static/media/resnetgru4.49f870f3.png",te=i.p+"static/media/resnetgru5.44b320fd.png",ie=i.p+"static/media/resnetgru6.652db087.png",se=i.p+"static/media/resnetgru7.4cab73d5.png",ne=i.p+"static/media/resnetgru8.e78d3c08.png",re=i.p+"static/media/resnetgrutable1.95a707de.png",ce=i.p+"static/media/resnetgrutable2.1dd9b81d.png",ae=i.p+"static/media/img6.8c3e6644.png",oe=(i(54),function(e){Object(l.a)(i,e);var t=Object(h.a)(i);function i(){return Object(o.a)(this,i),t.apply(this,arguments)}return Object(d.a)(i,[{key:"render",value:function(){return Object(s.jsx)("div",{id:"outer_container",children:Object(s.jsxs)("div",{id:"container",children:[Object(s.jsx)("h2",{children:"Resnet-GRU model"}),Object(s.jsxs)("p",{children:["Our first model was an attempt to create a baseline for the problem at hand. The model was based on a pre-trained Resnet_34 network which was extended using a bi-directional GRU. The input dataset consisted of input images, target positions of the agent vehicle as well as the ego vehicle. It also included target availability for the agent and ego vehicle. The input also provided us with historical positions of both sets of vehicles. For our model we used the input image of the agent as input and trained it on its given target positions. We optimized the model by calculating losses on target availability. The input number of channels was given by the number of historical positions/frames we wanted to consider. For our scenario we considered 10 historical frames which gave us the number of input channels as 22 because we considered 10 for each agent and ego vehicle and also included the current frame for both of them. ",Object(s.jsx)("br",{}),"The Resnet network was fed input images of the agent vehicle frame and the number of input channels were 22 as described above. The output from the resnet was of shape (bathc_size, 512). For our experiments we considered batch_size to be 16. The output from the Resnet was further fed to the bi-directional GRU. The hidden size for the network was chosen to be 2048. The gru layer gave output with shape ((batch_size, 1, 512), 2*hidden_size). The output features from this layer of GRU was then further fed to a sequential network consisting of a linear layer followed by a RElu and another linear layer and generated output with 1024 features. The hidden state from the previous GRU layer and the output features from the sequential network was fed to another bi-directional GRU layer and a similar cycle was followed. The GRU layer returned an output with 1024 features which was fed to a linear network to finally get the desired target output features. ",Object(s.jsx)("br",{}),"The total number of targets considered of the number of future positions we wanted to predict * the number of trajectories we wished to predict. In our scenario we decided to predict 50 future positions for 3 trajectories as kaggle was evaluating the results with the same. We also chose to predict confidences for 3 trajectories given the high uncertainty of real time traffic. Each future position had X and Y coordinate values, hence total number of target became number_of_trajectories * num_of_future_positons * 2 + confidences_for_3_trajectories i.e (3*50*2 + 3 = 303). Once we had the desired output, we split it into future positions (300) and confidences (3). The confidences were further transformed to probabilities using softmax. The final predicted outputs had shape (batch_size,num_of_trajectories,50,2) and confidences had shape (batch_size, num_of_trajectories). The model and output generation process described above is represented diagrammatically below."]}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:V}),Object(s.jsx)("p",{children:"Fig 1. Model journey "}),Object(s.jsx)("img",{src:_}),Object(s.jsx)("p",{children:"Fig 2. Output generation"})]}),Object(s.jsx)("p",{children:"For our experiments we executed our model multiple times on the training data to improve its performance. One of the critical things we ensured in this methodology was to use the trained model and latest optimizer state for the next cycle of training. Each cycle of training consisted of 12000 iterations. We generated our predictions from test data after each such cycle to observe the models performance. The predictions were evaluated by Kaggle using a negative log likelihood function. Below are the details of the three rounds of experiments conducted by us."}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:re,class:"imagetable"}),Object(s.jsx)("br",{}),Object(s.jsx)("img",{src:$}),Object(s.jsx)("p",{children:"Fig 3. Cycle 1 average training loss curve"}),Object(s.jsx)("img",{src:ee}),Object(s.jsx)("p",{children:"Fig 4. Cycle 2 average training loss curve"}),Object(s.jsx)("img",{src:te}),Object(s.jsx)("p",{children:"Fig 5. Cycle 3 average training loss curve"}),Object(s.jsx)("p",{children:"Prediction loss on test dataset evaluated by Kaggle:"}),Object(s.jsx)("img",{src:ie}),Object(s.jsx)("p",{children:"Fig 6. Best test result from kaggle submission"}),Object(s.jsx)("p",{children:"We also tried to train our model using a different loss function to compare the model\u2019s performance"}),Object(s.jsx)("img",{src:ce,class:"imagetable"}),Object(s.jsx)("p",{children:"Table 2. Experiment details using MSE and rMSE loss function"}),Object(s.jsx)("img",{src:se}),Object(s.jsx)("p",{children:"Fig 7. Training and Validation loss for MSE"}),Object(s.jsx)("img",{src:ne}),Object(s.jsx)("p",{children:"Fig 8. Training loss for rMSE"})]}),Object(s.jsx)("h4",{children:"Prediction trajectory visualization"}),Object(s.jsxs)("p",{children:["Every agent is identified by its track id and timestamp. In our submission file we have every record with timestamp and track id as index. For comparing the results generated by each of our models, we have compared trajectories predicted for the same agent.",Object(s.jsx)("br",{})," For my prediction I have used 3 modes or 3 prediction trajectories. Each trajectory holds 50 two dimensional (X,Y) predictions and each trajectory will have its own confidence. The sum of all three confidences will sum up to 1.",Object(s.jsx)("br",{}),"In the given visualization, we compared predicted trajectories for an agent with track id \u201c18431\u201d. We can observe that trajectory 3 represented by color green is dominant and as the model improves it reaffirms trajectory 3 with the highest confidence of 0.943. The visualizations are represented in both semantic and satellite view."]}),Object(s.jsx)("img",{src:ae})]})})}}]),i}(J)),de=i.p+"static/media/GAN4.2d86396c.png",le=(i(55),i(56),i.p+"static/media/LSTM-POS.ceb583c7.png"),he=i.p+"static/media/LSTM-CNN.53f2e082.png",je=i.p+"static/media/LSTM-2CNN.de0202b9.png",be=i.p+"static/media/LSTM-All.fd1785c2.png",ue=i.p+"static/media/LSTM-Loss.e90b2b18.png",me=i.p+"static/media/SeqLSTM-Models.2d1a3ea9.png",xe=i.p+"static/media/SeqLSTM-Bar.eb6f9f5b.png",Oe="60%",pe=function(e){Object(l.a)(i,e);var t=Object(h.a)(i);function i(){return Object(o.a)(this,i),t.apply(this,arguments)}return Object(d.a)(i,[{key:"render",value:function(){return Object(s.jsx)("div",{id:"outer_container",children:Object(s.jsxs)("div",{id:"container",children:[Object(s.jsx)("h2",{children:"LSTM and Seq2Seq LSTM model"}),Object(s.jsx)("p",{children:"We wanted to see how well a standard LSTM would perform and how adding more data to the architecture would affect its training performance. Lstm is one of the simplest recurrent models, allowing us to take advantage of temporal data in a simple, yet effective way. Our first LSTM (LSTM V1) architecture utilizes only the past 11 history positions of the given agent we are predicting trajectory for. By just using these positions, the model is able to get a sense of the speed of the vehicle given the constant time between all positions."}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:le,style:{maxWidth:Oe}}),Object(s.jsx)("p",{children:"LSTM V1 - Positional Information Only"})]}),Object(s.jsx)("p",{children:"Next, we assumed that adding more data would allow the model to get a better picture of the road conditions to use in predictions. As a result, we added a Resnet CNN to convert an image of the road and all nearby vehicles into a 1000 length vector encoding that would also be passed into the LSTM (LSTM V2). With 11 past images of the road state from a bird\u2019s eye view and the 11 historical positions, the model was able to figure out which parameters were most important and make predictions accordingly. Our dataset provided these images with 2 channels per history frame, 1 channel containing outlines of other agents, 1 containing the agent we are predicting for a total of 22 channels (2 * 11). In addition to these 22 channels, the dataset provided an RGB image containing the road information like lanes and traffic signal values. For this architecture, we tested by appending the road information image to the 2 channels (for a total of 5 channels per image) before passing it through the CNN (LSTM V2). We also tested another architecture which had two separate CNNs, one for the 2 channel car information input and one with the 3 channel road information input (LSTM V3)."}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:he,style:{maxWidth:Oe}}),Object(s.jsx)("p",{children:"LSTM V2 - One CNN"})]}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:je,style:{maxWidth:Oe}}),Object(s.jsx)("p",{children:"LSTM V3 - Two CNN"})]}),Object(s.jsx)("p",{children:"Finally, we wanted to implement an LSTM architecture where we throw as much available data as possible (LSTM V4). The intuition is to add all seemingly useful data and allow the model to figure out what input is actually important during training. As a result, these models might have performed better if trained for longer. We also found that this model trained better with a lower learning rate compared to the models described above, likely due to having so many input parameters."}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:be,style:{maxWidth:Oe}}),Object(s.jsx)("p",{children:"LSTM V4 - All Useful Information"})]}),Object(s.jsx)("p",{children:"All of these LSTM variations performed well, quickly being able to stabilize and achieve a decent loss. Interestingly, the models with these least parameters required to learn seemed to be most stable whereas the more complicated variations like using all available data (LSTM All) performed well but had many spikes in average training loss. In this case, LSTM using only positional data achieved the best MSE test loss, though I assume with more training time the other models could also achieve similar loss. "}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsx)("img",{class:"loss",src:ue,style:{maxWidth:400}})}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsx)("img",{class:"loss",src:"data:image/png;base64,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",style:{maxWidth:400}})}),Object(s.jsx)("p",{children:"Given the success of these architectures, we wanted to see if we could apply the Seq2Seq architecture using the same general structure. As a result, we developed four similar models, replacing the single LSTM from the previous models with an encoder and decoder LSTM. The encoder took in the temporal input from our dataset. The decoder took in the last hidden state from the encoder and predicted the next 50 positions of the agent. We also tested a variation in which only the last decoder output was used to make the prediction, putting the output through a fully connected layer and predicting 50 time steps at once. This variation achieved decent accuracy though still performed worse than the standard Seq2Seq implementation."}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsx)("img",{class:"loss",src:me,style:{width:"80%"}})}),Object(s.jsx)("p",{children:"Interestingly, while the position only variant of the base LSTM performed the best of the 4 architectures, the double CNN Seq2Seq variation performed the best among the Seq2Seq LSTM models, achieving an MSE test loss of 2.29. It was exciting to see Seq2Seq\u2019s ability to perform well in the trajectory prediction domain, despite it often being thought of as a tool for text-based predictions, especially translations. While Seq2Seq performance is still around the level of the LSTM\u2019s performance, we consider that with more tuning and training, it might be able to surpass it."}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsx)("img",{class:"loss",src:xe,style:{maxWidth:400}})})]})})}}]),i}(J),ge=(i(57),i.p+"static/media/vae1.510ea3d0.png"),fe=i.p+"static/media/vae2.ebd21294.png",we=i.p+"static/media/vae3.9466ba20.png",ve=i.p+"static/media/vae4.b7fa9d7f.png",Ae=i.p+"static/media/vae5.dd1d9795.png",Se=i.p+"static/media/vae6.794464c8.png",ye=i.p+"static/media/vae7.9b7e52c9.png",Te=i.p+"static/media/vae8.750d2809.png",Me=i.p+"static/media/vae9.02e156ca.png",qe=i.p+"static/media/vaea.667bf8ac.png",Ee=i.p+"static/media/vaeb.04767882.png",Ne=i.p+"static/media/vaec.5c7b7c39.png",Ie=function(e){Object(l.a)(i,e);var t=Object(h.a)(i);function i(){return Object(o.a)(this,i),t.apply(this,arguments)}return Object(d.a)(i,[{key:"render",value:function(){return Object(s.jsx)("div",{id:"outer_container",children:Object(s.jsxs)("div",{id:"container",children:[Object(s.jsx)("h2",{children:"VAE+LSTM"}),Object(s.jsx)("p",{children:"This model follows the variational model architecture seen in class using LSTM layers to learn the probability distribution of the trajectories and as a result output similar trajectory within the same distribution."}),Object(s.jsx)("p",{children:"We present two configurations, Configuration A, uses a hidden layer and splits the hidden layer into two tensors mean and standard deviation. The output of parametrization trick is concatenated with the cell state and feeds the decoder to output next n positions."}),Object(s.jsx)("p",{children:"Configuration B uses both hidden layer and cell state in two parametrization phases and the output is concatenated and is fed to the decoder to output next n positions."}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{class:"vae1",src:ge}),Object(s.jsx)(R.a.Caption,{children:"Figure 1. VAE+LSTM method"})]})}),Object(s.jsx)("br",{}),Object(s.jsx)("p",{children:"The following table shows the losses at evaluation time using MSE as criterion achieved after 10000 iterations, number of history positions was 10, predicted positions 50 and beta = 0.2 for VAE."}),Object(s.jsxs)("table",{style:{width:"90%"},children:[Object(s.jsx)("thead",{children:Object(s.jsxs)("tr",{children:[Object(s.jsx)("th",{children:"Configuration"}),Object(s.jsx)("th",{children:"Experiment details"}),Object(s.jsx)("th",{children:"Loss function"}),Object(s.jsx)("th",{children:"Learning rate"}),Object(s.jsx)("th",{children:"Iterations"}),Object(s.jsx)("th",{children:"Average test loss"})]})}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"A"}),Object(s.jsx)("td",{children:"Model is trained with batch size = 32, beta = 0.2, AdamW optimizer, weight decay = 0.0005"}),Object(s.jsx)("td",{children:"MSE Loss reduction none"}),Object(s.jsx)("td",{children:"1e-3"}),Object(s.jsx)("td",{children:"10000"}),Object(s.jsx)("td",{children:"2.1587"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"B"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Model is trained with batch size = 32, beta = 0.2, AdamW optimizer, weight decay = 0.0005"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"MSE Loss reduction none"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"1e-3"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"10000"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"1.9585"})})]})]}),Object(s.jsx)("br",{}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{class:"loss",src:Ne}),Object(s.jsx)(R.a.Caption,{children:"Figure 2. Configuration B total training and validation loss"})]})}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{class:"loss",src:fe}),Object(s.jsx)(R.a.Caption,{children:"Figure 3. Configuration A test loss"})]})}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{class:"loss",src:we}),Object(s.jsx)(R.a.Caption,{children:"Figure 4. Configuration B test loss"})]})}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{class:"loss",src:ve}),Object(s.jsx)(R.a.Caption,{children:"Figure 5. Configuration B Reconstruction and KL test loss"})]})}),Object(s.jsx)("p",{children:"Both configurations produce similar results, total loss curve, reconstruction and kl loss are similar as well."}),Object(s.jsx)("p",{children:"As it can be appreciated, VAE+LSTM average losses seem to be slightly better than other LSTM models, but not as good as other models like Seq2Seq GAN. Our assumption is that this is because the model is creating in some cases sharper trajectories since the model is not tuned or trained enough to output smooth trajectories for such scenarios and while most trajectories are good approximations when compared to ground truth trajectories in general seem to be a little bit unnatural where Seq2SeqGAN model with the help of the generator seems to output more natural (real) future predictions."}),Object(s.jsx)("p",{children:"In the following example we can see the predicted positions for an agent. Here we can see that the first 5 positions have sharp changes with respect to the last history positions, but after those positions it seems to project a reasonable trajectory."}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{src:Ae}),Object(s.jsx)(R.a.Caption,{children:"Figure 6. Next 50 calculated positions for random agent"})]})}),Object(s.jsx)("p",{children:"Finally, here are some random sampled trajectories generated from the agent's test set during evaluation time, as we can see the model generates different predicted trajectories based on the history positions which shows that while some trajectories behavior seem similar the coordinate positions are not the same and as a result there is some diversity on the trajectories generated by this model."}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsxs)(R.a,{children:[Object(s.jsx)("img",{class:"grid",src:Se}),Object(s.jsx)("img",{class:"grid",src:ye}),Object(s.jsx)("img",{class:"grid",src:Te}),Object(s.jsx)("br",{}),Object(s.jsx)("img",{class:"grid",src:Me}),Object(s.jsx)("img",{class:"grid",src:qe}),Object(s.jsx)("img",{class:"grid",src:Ee})]}),Object(s.jsx)(R.a.Caption,{children:"Figure 7. Randomly sampled trajectories"})]})]})})}}]),i}(J),Le=(i(58),i.p+"static/media/GAN6.a9ee5802.png"),Ge=i.p+"static/media/GAN7.da686091.png",Be=function(e){Object(l.a)(i,e);var t=Object(h.a)(i);function i(){return Object(o.a)(this,i),t.apply(this,arguments)}return Object(d.a)(i,[{key:"render",value:function(){return Object(s.jsx)("div",{id:"outer_container",children:Object(s.jsxs)("div",{id:"container",children:[Object(s.jsx)("h2",{children:"Seq2Seq GAN and its variant model"}),Object(s.jsx)("p",{children:"To the baseline LSTM model, we applied GAN architecture and created a new model. We call this model as SeqtoSeq GAN here. With the GAN\u2019s generator and discriminator, we expected that our model generates more plausible future positions, avoiding the blurry predictions with the help of discriminator. Figure1 and Figure2 two shows the architecture of the generator and the discriminator of our Seq2Seq model."}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:M,style:{maxWidth:"100%"}}),Object(s.jsx)("span",{children:Object(s.jsx)("strong",{children:"Figure 1. Generator of Seq2Seq GAN1"})})]}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:q,style:{maxWidth:"100%"}}),Object(s.jsx)("br",{}),Object(s.jsx)("span",{children:Object(s.jsx)("strong",{children:"Figure 2. Discriminator of Seq2Seq GAN1"})})]}),Object(s.jsx)("p",{children:"As you see the generator\u2019s architecture in Figure1, we firstly extracted  latent temporal features from history positions with encoder and decoder. After that, we added noise vectors for the generator and used LSTM and fully connected layers to generate the next 50 trajectories. For the discriminator, we gave the generated trajectories and the target trajectories which is the ground truth to it as input. And we used an LSTM layer to extract latent features and used fully connected layers to distinguish whether our input to a discriminator is forgery or not."}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:E,style:{maxWidth:"100%"}}),Object(s.jsx)("br",{}),Object(s.jsx)("span",{children:Object(s.jsx)("strong",{children:"Figure 3. Generator of Seq2Seq GAN2 (with yaw)"})})]}),Object(s.jsx)("p",{children:"Other than the generator model we have discussed above, we have implemented a new generator that contains yaw information taking the direction information into our model. As in Figure3, the encoder takes history positions and yaw information as input. We believed that this would improve our model and found out that the Seq2Seq GAN2 has lower MSE test loss than the Seq2Seq GAN1."}),Object(s.jsx)("div",{children:Object(s.jsxs)("table",{style:{maxWidth:"100%",tableLayout:"fixed"},children:[Object(s.jsx)("caption",{children:"Table 1. Test Losses of different Seq2Seq GAN Models "}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Models"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Epochs"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Test Loss(MSE)"})})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GANl1-v2"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"2.6551"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GANl2-v2"}),Object(s.jsx)("td",{children:"2"}),Object(s.jsx)("td",{children:"2.2576"})]})]})}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:de,style:{maxWidth:"100%",width:"70%"}}),Object(s.jsx)("br",{}),Object(s.jsx)("span",{children:Object(s.jsx)("strong",{children:"Figure 4.  Training loss graphs( lr = 1e-3 )"})})]}),Object(s.jsx)("p",{children:"These are the loss graphs from a Seq2Seq GAN model. In this model, we have used MSE loss for the generator loss and BCE loss for the discriminator. MSE loss function allows a model to learn directly from the target value. As you see from Figure 4, generator loss converges to 0. This is desirable since we use a generator to generate future trajectory. Moreover, validation loss follows similar behavior as training loss, and it shows that our model is not overfitted."}),Object(s.jsx)("h2",{children:"Fine Tuning"}),Object(s.jsx)("p",{children:"We have tested many different hyperparameters for fine tuning  to improve  two Seq2Seq GAN models above. Firstly, we tested the Seq2Seq GAN1 with different epochs and we got the following result."}),Object(s.jsx)("div",{children:Object(s.jsxs)("table",{style:{maxWidth:"100%",tableLayout:"fixed"},children:[Object(s.jsx)("caption",{children:"Table 2. Test Losses on Seq2Seq GAN1 with different epochs "}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Configuration"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Epochs"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Test Loss(MSE)"})})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GANl1-v2"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"2.6551"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GANl1-v2"}),Object(s.jsx)("td",{children:"2"}),Object(s.jsx)("td",{children:"1.9143"})]})]})}),Object(s.jsx)("p",{children:"As the result in Table2, the MSE test loss becomes much lower when we trained the model with more epochs."}),Object(s.jsx)("p",{children:"Next, we have tried training our models with different loss functions in the generator - BCE and MSE, remaining other conditions the same. With BCE loss, the generator loss indicates how well generated trajectories resemble the target trajectories. On the contrary, applying MSE loss function on the generator directly calculated the loss between the predicted future positions and the ground truth positions."}),Object(s.jsx)("div",{children:Object(s.jsxs)("table",{style:{maxWidth:"100%",tableLayout:"fixed"},children:[Object(s.jsx)("caption",{children:"Table 3. Test Losses on Seq2Seq GAN models with different Loss functions in the generator "}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Configuration"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Epochs"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Generator\u2019s Loss function"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Test Loss(MSE)"})})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GANl1-v1"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"78.5733"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GANl1-v2"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"MSE"}),Object(s.jsx)("td",{children:"2.5441"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN2-v1"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"78.55"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN2-v2"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"MSE"}),Object(s.jsx)("td",{children:"2.2576"})]})]})}),Object(s.jsx)("p",{children:"As in the Table3, on both models, the MSE test loss becomes much lower when we trained the model with more epochs because the model directly learns from the target trajectories with MSE loss functions and it might be difficult for BCE loss to give meaningful feedback when we predicted 50 future moves."}),Object(s.jsx)("p",{children:"We modified the layers in the discriminator and compared the test losses because we believe that the discriminator loss converges to 0 too quickly, which prevents the generator from learning effectively. Instead of the Relu activation function layer, we used LeakyRelu and also applied dropout and the batch normalization. Table 4 shows the result from this trial. For all the models in Table 4, we applied MSE loss function to the generators and trained with 1 epoch."}),Object(s.jsx)("div",{children:Object(s.jsxs)("table",{style:{maxWidth:"100%",tableLayout:"fixed"},children:[Object(s.jsx)("caption",{children:"Table 4. Test Loss on Seq2Seq GAN models with different layers in the discriminators. "}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{width:"25%",children:Object(s.jsx)("b",{children:"Configuration"})}),Object(s.jsx)("td",{colspan:"2",children:Object(s.jsx)("b",{children:"Layers in discriminator"})}),Object(s.jsx)("td",{width:"25%",children:Object(s.jsx)("b",{children:"Test Loss(MSE)"})})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("th",{}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Activation Function"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Dropout, Batch Normalization"})}),Object(s.jsx)("th",{})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GANl1-v2-1"}),Object(s.jsx)("td",{children:"Relu"}),Object(s.jsx)("td",{children:"Not Applied"}),Object(s.jsx)("td",{children:"72.6551"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Seq2Seq GANl1-v2-2"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Leaky Relu"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Applied"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"2.5579"})})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN2-v2-1"}),Object(s.jsx)("td",{children:"Relu"}),Object(s.jsx)("td",{children:"Not Applied"}),Object(s.jsx)("td",{children:"2.2576"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Seq2Seq GAN2-v2-2"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Leaky Relu"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Applied"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"2.1783"})})]})]})}),Object(s.jsx)("p",{children:"As in the Table4, applying LeakyRelu, dropout and batch normalization could reduce the MSE test loss on our models."}),Object(s.jsx)("p",{children:"With this success, we applied another change on the optimizer to see that the generator can learn better with this change. We\u2019re currently using Adam in the generator and the discriminator as optimizers. To prevent the discriminator from learning too fast which might affect the generator\u2019s performance, we applied different values of learning rate on the optimizer in the discriminator and saw the result. The other conditions for all the models in the Table 5 are the same - epoch of 1, MSE loss function for the generators."}),Object(s.jsx)("div",{children:Object(s.jsxs)("table",{style:{maxWidth:"100%",tableLayout:"fixed"},children:[Object(s.jsx)("caption",{children:"Table 5. Test Loss on Seq2Seq GAN models with different learning rate "}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Configuration"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Learning rate in the discriminator"})}),Object(s.jsx)("th",{children:Object(s.jsx)("b",{children:"Test Loss(MSE)"})})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN1-v2-2-1"}),Object(s.jsx)("td",{children:"5e-3"}),Object(s.jsx)("td",{children:"2.5574"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN1-v2-2-1"}),Object(s.jsx)("td",{children:"1e-3"}),Object(s.jsx)("td",{children:"2.5579"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN1-v2-2-1"}),Object(s.jsx)("td",{children:"1e-4"}),Object(s.jsx)("td",{children:"2.1463"})]})]})}),Object(s.jsx)("p",{children:"As in Table 5, we could get the lowest test MSE loss when we trained the discriminator with learning rate, 1e-4."}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:Le,style:{maxWidth:"100%",width:"70%"}}),Object(s.jsx)("br",{}),Object(s.jsx)("span",{children:Object(s.jsx)("strong",{children:"Figure 5. Training loss graphs with learning rate 1e-4."})})]}),Object(s.jsx)("p",{children:"From the graph above, we can observe that  the discriminator loss in Figure 5 converges to 0 slower than Figure 4. It shows that when the learning rate is smaller, the optimizer has more gradient to update weights, which helps the model to enhance its performance."}),Object(s.jsx)("h4",{children:"Evaluation on Seq2Seq models"}),Object(s.jsx)("p",{children:"After training, we tested our Seq2Seq models to check if it doesn\u2019t fall into the mode failure and it can generate diverse predictions for the inputs. We generated one hundred sample predictions for the same batch(32) input and plotted all these generated future trajectories(100) for the randomly-picked four inputs."}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:N,style:{maxWidth:"100%",width:"50%"}}),Object(s.jsx)("br",{}),Object(s.jsx)("span",{children:Object(s.jsx)("strong",{children:"Figure 6. Predicted Trajectories from the Seq2Seq GAN Model1"})})]}),Object(s.jsxs)("div",{style:{textAlign:"center"},children:[Object(s.jsx)("img",{src:Ge,style:{maxWidth:"100%",width:"50%"}}),Object(s.jsx)("br",{}),Object(s.jsx)("span",{children:Object(s.jsx)("strong",{children:"Figure 7. Predicted Trajectories from the Seq2Seq GAN Model2 (with yaw information)"})})]}),Object(s.jsx)("p",{children:"As you see in Figure 6 and 7, both Seq2Seq GAN1 and Seq2Seq GAN1 do not suffer from mode collapse and successfully generates diverse trajectories."}),Object(s.jsx)("p",{children:"You can find all the variants of Seq2Seq GAN with detailed hyper parameters and the structures in Table 6."}),Object(s.jsx)("div",{children:Object(s.jsxs)("table",{style:{maxWidth:"100%",tableLayout:"fixed"},children:[Object(s.jsx)("caption",{children:"Table 6. All the models with tuning "}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{width:"10%",rowSpan:"2",children:Object(s.jsx)("b",{children:"Configuration"})}),Object(s.jsx)("td",{width:"10%",rowSpan:"2",children:Object(s.jsx)("b",{children:"Optimizer"})}),Object(s.jsx)("td",{width:"10%",rowSpan:"2",children:Object(s.jsx)("b",{children:"Batch Size"})}),Object(s.jsx)("td",{colspan:"2",children:Object(s.jsx)("b",{children:"Loss function"})}),Object(s.jsx)("td",{colspan:"2",children:Object(s.jsx)("b",{children:"Layers in the Discriminator"})}),Object(s.jsx)("td",{width:"10%",rowSpan:"2",children:Object(s.jsx)("b",{children:"Epoch"})}),Object(s.jsx)("td",{width:"10%",rowSpan:"2",children:Object(s.jsx)("b",{children:"Learning rate"})}),Object(s.jsx)("td",{width:"10%",rowSpan:"2",children:Object(s.jsx)("b",{children:"Test Loss(MSE)"})})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Generator"}),Object(s.jsx)("td",{children:"Discriminator"}),Object(s.jsx)("td",{children:"Activation Function"}),Object(s.jsx)("td",{children:"Dropout, Batch Norm"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN1-v1-1"}),Object(s.jsx)("td",{children:"Adam"}),Object(s.jsx)("td",{children:"32"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"Relu"}),Object(s.jsx)("td",{children:"X"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"1e-3"}),Object(s.jsx)("td",{children:"78.5733"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN2-v1-1"}),Object(s.jsx)("td",{children:"Adam"}),Object(s.jsx)("td",{children:"32"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"Relu"}),Object(s.jsx)("td",{children:"X"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"1e-3"}),Object(s.jsx)("td",{children:"78.5536"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN1-v2-1-1"}),Object(s.jsx)("td",{children:"Adam"}),Object(s.jsx)("td",{children:"32"}),Object(s.jsx)("td",{children:"MSE"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"Relu"}),Object(s.jsx)("td",{children:"X"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"1e-3"}),Object(s.jsx)("td",{children:"2.6551"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN1-v2-2-1"}),Object(s.jsx)("td",{children:"Adam"}),Object(s.jsx)("td",{children:"32"}),Object(s.jsx)("td",{children:"MSE"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"Leaky Relu"}),Object(s.jsx)("td",{children:"O"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"1e-3"}),Object(s.jsx)("td",{children:"2.5579"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN1-v2-2-2"}),Object(s.jsx)("td",{children:"Adam"}),Object(s.jsx)("td",{children:"32"}),Object(s.jsx)("td",{children:"MSE"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"Leaky Relu"}),Object(s.jsx)("td",{children:"O"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"1e-3"}),Object(s.jsx)("td",{children:"2.5574"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN2-v2-1-1"}),Object(s.jsx)("td",{children:"Adam"}),Object(s.jsx)("td",{children:"32"}),Object(s.jsx)("td",{children:"MSE"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"Relu"}),Object(s.jsx)("td",{children:"X"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"1e-3"}),Object(s.jsx)("td",{children:"2.2576"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN2-v2-2-1"}),Object(s.jsx)("td",{children:"Adam"}),Object(s.jsx)("td",{children:"32"}),Object(s.jsx)("td",{children:"MSE"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"Leaky Relu"}),Object(s.jsx)("td",{children:"O"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"1e-3"}),Object(s.jsx)("td",{children:"2.1781"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:"Seq2Seq GAN1-v2-2-3"}),Object(s.jsx)("td",{children:"Adam"}),Object(s.jsx)("td",{children:"32"}),Object(s.jsx)("td",{children:"MSE"}),Object(s.jsx)("td",{children:"BCE"}),Object(s.jsx)("td",{children:"Leaky Relu"}),Object(s.jsx)("td",{children:"O"}),Object(s.jsx)("td",{children:"1"}),Object(s.jsx)("td",{children:"1e-4"}),Object(s.jsx)("td",{children:"2.1463"})]}),Object(s.jsxs)("tr",{children:[Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Seq2Seq GAN1-v2-1-1"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Adam"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"32"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"MSE"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"BCE"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"Relu"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"X"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"2"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"1e-3"})}),Object(s.jsx)("td",{children:Object(s.jsx)("b",{children:"1.9143"})})]})]})}),Object(s.jsxs)("p",{children:["1 : Train with only history positions",Object(s.jsx)("br",{}),"2 : Train with history positions and history yaw ",Object(s.jsx)("br",{}),"V1 : BCE loss function in the generator",Object(s.jsx)("br",{}),"V2 : MSE loss function in the generator",Object(s.jsx)("br",{}),"Vx - 1 : Relu, without Dropout and Batch Norm in the discriminator",Object(s.jsx)("br",{}),"Vx - 2 : LeakyRelu, with Dropout and Batch Norm in the discriminator",Object(s.jsx)("br",{}),"Vx - x - 1 : With learning rate 1e-3",Object(s.jsx)("br",{}),"Vx - x - 2 : With learning rate 5e-3",Object(s.jsx)("br",{}),"Vx - x - 3 : With learning rate 1e-4",Object(s.jsx)("br",{})]}),Object(s.jsx)("p",{children:"For now, the best Seq2Seq GAN model is the one with epoch2. While this model achieved the lowest test loss with 1.9143, we believe that we can achieve much lower MSE test loss if we apply Relu, dropout, and the batch norm in the layers and apply the learning rate of 1e-4 to the optimizer in the discriminator."}),Object(s.jsx)("h4",{children:"Future Work "}),Object(s.jsx)("p",{children:"Although we have implemented many variant models of Seq2Seq GAN, we found some following limitations and these still give us chances to enhance our models with future works."}),Object(s.jsx)("p",{children:"It didn\u2019t"}),Object(s.jsxs)("p",{children:[Object(s.jsx)("li",{children:"consider the road information therefore, the generated predictions can fall off the roads."}),Object(s.jsx)("li",{children:"consider the interaction with neighbors including other cars, pedestrians, or cyclists."}),Object(s.jsx)("li",{children:"have a chance to learn enough with more epochs because of time and computational limit "})]}),Object(s.jsx)("p",{children:"To complement these limitations and improve our Seq2Seq GAN Model, we can introduce the following methods in the future work."}),Object(s.jsxs)("p",{children:[Object(s.jsx)("li",{children:"Training the model with optimal parameters and with more epochs. "}),Object(s.jsx)("li",{children:"Combining with a CNN model including road information from images."}),Object(s.jsx)("li",{children:"Applying the concept of Social GAN which embraces interactions among neighbors."})]}),Object(s.jsx)("p",{children:"With the above approaches, we believe that our Seq2Seq GAN model can generate more plausible and precise predictions on the multiple next moves. "})]})})}}]),i}(J),Fe=(i(59),i.p+"static/media/slstmcode.7c9fbe99.png"),Re=i.p+"static/media/slstmtrainloss.46c9177c.png",ke=i.p+"static/media/slstmtestloss.caf9bc79.png",ze=function(e){Object(l.a)(i,e);var t=Object(h.a)(i);function i(){return Object(o.a)(this,i),t.apply(this,arguments)}return Object(d.a)(i,[{key:"render",value:function(){return Object(s.jsx)("div",{id:"outer_container",children:Object(s.jsxs)("div",{id:"container",children:[Object(s.jsx)("h2",{children:" Social LSTM and its variant models"}),Object(s.jsx)("p",{children:"Alexandre, Kratarth, et. al. pointed out that humans have the innate ability to  \u201cread\u201d one another. Any autonomous vehicle navigating such a scene should be able to foresee the future positions of people (including vehicles driven by other people or pedestrians) and accordingly adjust its path to avoid collisions. Inspired by this paper, we want to take the \u201cneighbor's\u201d influence into consideration. For this project, we have one LSTM for each vehicle. This LSTM learns the spatial coordination and predicts their future positions as shown in the Figure 1."}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{class:"slstm1",src:S}),Object(s.jsx)(R.a.Caption,{children:"Figure 1. Social LSTM Model Architecture"})]})}),Object(s.jsx)("p",{children:"The LSTM weights are shared across all the sequences. Different from the vanilla LSTM, it has an additional layer: social pooling, which combines the information from all neighboring states. Their calculation follows the following equations:"}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{class:"slstm0",src:A}),Object(s.jsx)(R.a.Caption,{children:"Equation 1. Social Pooling Layer Equation"})]})}),Object(s.jsx)("p",{children:"The pseudo code is shown in Algorithm 1:"}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsx)(R.a,{children:Object(s.jsx)(R.a.Image,{class:"slstmcode",src:Fe})})}),Object(s.jsxs)("p",{children:["However, this model has some inherent limitations:",Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"Simply adding all the neighbor\u2019s information does not make sense. People care more about the neighborhood closer to them than those people who are far away from them. "}),Object(s.jsx)("li",{children:"This social LSTM architecture is a generalization method. However, if we can give a more specific classification, it is highly likely that we can improve the prediction\u2019s accuracy. "}),Object(s.jsx)("li",{children:"For the social LSTM architecture, when the agent falls into a grid, it will be considered as a neighborhood. For the architecture, the grid we considered is a square. It seems unreasonable because agents move faster in the speed\u2019s direction than the other. "})]}),"In order to solve these, we need to modify current model to:",Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"Introducing the spatial information to the max-pooling part, which can be realized by using the convolutional layer."}),Object(s.jsx)("li",{children:"In the real world, when a car\u2019s move, it only has two classes: longitude and latitude. For the longitude classes, it can be split into 3 situations: speed up, normal and slow down; turning left, staying the same and turning right are these 3 situations for the latitude class. For the dataset we used for this project, it does not have classes, so we preprocess the dataset, giving them corresponding classes. "}),Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"For the speed class, we define that if the average speed over the next 5 time stamps is over than 1.2 speed at the current time, it will be classified as speed up. If the average speed over the next 5 time stamps is less than 0.8 speed at the current time, it will be classified as slow down. Between 0.8 and 1.2, it is normal."}),Object(s.jsx)("li",{children:"For the direction class, using the next 5 time stamps its move over 5 feet or not to decide whether this agent turn left or right. If its move within 5 feet. It will be thought as stay same. This 5 feet is the half-width of the US urban lane. We assume that if the agent/car moves half of the urban lane, it is highly likely that it will turn its direction."})]}),Object(s.jsx)("li",{children:"For the grid, instead of considering the square, tried to consider rectangular. "})]}),"Based on this changes, the architecture changes as the follows:"]}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{class:"slstm2",src:y}),Object(s.jsx)(R.a.Caption,{children:"Figure 2. Social LSTM Variant Model Architecture"})]})}),Object(s.jsx)("p",{children:"For this model, the goal changes to maximize the probability of the prediction based on the observation (here the observation is the sequence of each agent\u2019s position). The the objective function is:"}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsx)(R.a,{children:Object(s.jsx)(R.a.Image,{class:"slstm3",src:T})})}),Object(s.jsx)("p",{children:"For this model, since we predict the probability, we want to minimize the negative log likelihood. But in order to be comparable with other models, here we calculated the MSE loss. The training result is shown as follows:"}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{class:"slstmtrainloss",src:Re}),Object(s.jsx)(R.a.Caption,{children:"Figure 3. Training Loss for Social LSTM and its Variant"})]})}),Object(s.jsxs)("p",{children:["As we can see that social LSTM variant is slightly better than the social LSTM model. This comes from the following reasons:",Object(s.jsxs)("ul",{children:[Object(s.jsx)("li",{children:"Since we consider the spatial information about the neighbourhood: the closer, the more important. So social LSTM variant models should capture the information more precisely, which will return more accurate results."}),Object(s.jsx)("li",{children:"Same as considering the speed influence: not using the square grid, instead using rectangular grid. This change will make us consider more useful neighborhoods."}),Object(s.jsx)("li",{children:"Using the classes to give different input to make it more accurate."})]}),"However, this slight advantage may comes from:",Object(s.jsx)("ul",{children:Object(s.jsx)("li",{children:"Since the dataset does not have any classes. We  define the classes by ourselves. Maybe this is not accurate, in the future, perhaps we should try some other criterion to classify the dataset."})}),"Since we found that the social-LSTM variant  model is better than social-LSTM, we should use the social-LSTM variant model for testing data. The result about the testing loss w.r.t prediction length is as follows:"]}),Object(s.jsx)("div",{style:{textAlign:"center"},children:Object(s.jsxs)(R.a,{children:[Object(s.jsx)(R.a.Image,{class:"slstmtestloss",src:ke}),Object(s.jsx)(R.a.Caption,{children:"Figure 4. Test Loss for Social LSTM Variant"})]})}),Object(s.jsx)("p",{children:"We changed the predicting length from 1 to 50.  The longer prediction length, the larger test loss. The loss is like a linear regression. Their loss is similar to the training loss, which confirm that we do not have the overfitting issue."})]})})}}]),i}(J);function Pe(){return Object(s.jsxs)("section",{children:[Object(s.jsx)(O,{}),Object(s.jsxs)(u.d,{children:[Object(s.jsx)(u.b,{exact:!0,path:"/home",component:function(){return Object(s.jsx)(Q,{page:"home"},"home")}}),Object(s.jsx)(u.b,{exact:!0,path:"/resnet-gru",component:function(){return Object(s.jsx)(oe,{page:"resnet-gru"},"resnet-gru")}}),Object(s.jsx)(u.b,{exact:!0,path:"/lstm-seq2seq",component:function(){return Object(s.jsx)(pe,{page:"lstm"},"lstm")}}),Object(s.jsx)(u.b,{exact:!0,path:"/vae-lstm",component:function(){return Object(s.jsx)(Ie,{page:"vae-lstm"},"vae-lstm")}}),Object(s.jsx)(u.b,{exact:!0,path:"/seq2seqGAN",component:function(){return Object(s.jsx)(Be,{page:"seq2seqGAN"},"seq2seqGAN")}}),Object(s.jsx)(u.b,{exact:!0,path:"/s-lstm",component:function(){return Object(s.jsx)(ze,{LSTM:!0,page:"s-lstm"},"s-lstm")}}),Object(s.jsx)(u.a,{from:"/",to:"/home"})]})]},"app")}var Ce=function(e){e&&e instanceof Function&&i.e(3).then(i.bind(null,62)).then((function(t){var i=t.getCLS,s=t.getFID,n=t.getFCP,r=t.getLCP,c=t.getTTFB;i(e),s(e),n(e),r(e),c(e)}))};a.a.render(Object(s.jsx)(r.a.StrictMode,{children:Object(s.jsx)(m.a,{children:Object(s.jsx)(Pe,{})})}),document.getElementById("root")),Ce()}},[[60,1,2]]]);
//# sourceMappingURL=main.23527b3d.chunk.js.map